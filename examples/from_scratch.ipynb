{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import animation\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_gif(frames, path=\"./gifs/\", filename=\"gym_animation.gif\", fps=60):\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 50.0, frames[0].shape[0] / 50.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "    anim.save(path + filename, writer=\"imagemagick\", fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, visualize=False, deterministic=True):\n",
    "    trajectory = {\n",
    "        \"states\": [],\n",
    "        \"actions\": [],\n",
    "        \"rewards\": [],\n",
    "        \"terminated\": [],\n",
    "        \"truncated\": [],\n",
    "    }\n",
    "    frames = []\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action, _ = agent.predict(obs, deterministic=deterministic)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        trajectory[\"states\"].append(obs)\n",
    "        trajectory[\"actions\"].append(action)\n",
    "        trajectory[\"rewards\"].append(reward)\n",
    "        trajectory[\"terminated\"].append(terminated)\n",
    "        trajectory[\"truncated\"].append(truncated)\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "        if visualize:\n",
    "            frames.append(env.render())\n",
    "\n",
    "    if visualize:\n",
    "        print(\"saving...\")\n",
    "        save_frames_as_gif(frames)\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(env, agent, validation_n: int = 20, deterministic=False):\n",
    "    total_rewards = []\n",
    "    for _ in range(validation_n):\n",
    "        trajectory = get_trajectory(env, agent, deterministic=deterministic)\n",
    "        total_rewards.append(np.sum(trajectory[\"rewards\"]))\n",
    "\n",
    "    return np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, log_frequency=1000):\n",
    "        self.log_frequency = log_frequency\n",
    "        self.next_iteration = self.log_frequency\n",
    "\n",
    "    def log(self, steps_n, data):\n",
    "        if steps_n >= self.next_iteration:\n",
    "            self.next_iteration += self.log_frequency\n",
    "\n",
    "            dones = data[\"terminated\"] + data[\"truncated\"]\n",
    "            if sum(dones) == 0:\n",
    "                dones[-1] = True\n",
    "            dones_indeces = dones.nonzero()\n",
    "            last_done_index = dones_indeces[-1][0].item()\n",
    "            trajectory_n = sum(dones)\n",
    "            mean_trajectory_rewards = (\n",
    "                sum(data[\"rewards\"][:last_done_index]) / trajectory_n\n",
    "            )\n",
    "            mean_trajectory_length = last_done_index / trajectory_n\n",
    "            print(f\"steps_n: {steps_n}\")\n",
    "            print(f\"mean_trajectory_rewards: {mean_trajectory_rewards}\")\n",
    "            print(f\"mean_trajectory_length: {mean_trajectory_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticMlpPolicy(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.obs_dim = input_size\n",
    "        self.action_dim = int(output_size / 2)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = torch.FloatTensor(input.reshape(-1, self.obs_dim))\n",
    "        output = self.act(self.fc1(input))\n",
    "        output = self.act(self.fc2(output))\n",
    "        logits = self.fc3(output)\n",
    "        mu, log_std = logits[:, : self.action_dim], logits[:, self.action_dim :]\n",
    "        log_std = torch.clamp(log_std, min=-20, max=2)\n",
    "\n",
    "        return mu, log_std\n",
    "\n",
    "    def predict(self, input, deterministic=False):\n",
    "        mu, log_std = self.forward(input)\n",
    "        dist = Normal(mu, torch.exp(log_std))\n",
    "\n",
    "        if deterministic:\n",
    "            action = mu\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "\n",
    "        log_prob_action = dist.log_prob(action)\n",
    "        action = action.numpy().reshape(self.action_dim)\n",
    "\n",
    "        return action, log_prob_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteStochasticMlpPolicy(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.obs_dim = input_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = torch.FloatTensor(input.reshape(-1, self.obs_dim))\n",
    "        output = self.act(self.fc1(input))\n",
    "        output = self.act(self.fc2(output))\n",
    "        logits = self.fc3(output)\n",
    "        probs = self.softmax(logits)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict(self, input, deterministic=False):\n",
    "        probs = self.forward(input)\n",
    "        dist = Categorical(probs)\n",
    "\n",
    "        if deterministic:\n",
    "            action = torch.argmax(probs, dim=1)\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "\n",
    "        log_prob_action = dist.log_prob(action)\n",
    "\n",
    "        return action[0].item(), log_prob_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "discrete = True\n",
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "\n",
    "if discrete:\n",
    "    output_size = env.action_space.n\n",
    "else:\n",
    "    output_size = 2 * env.action_space.shape[0]\n",
    "\n",
    "print(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns(rewards, terminated, gamma=1):\n",
    "    rewards_n = len(rewards)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    returns[-1] = rewards[-1]\n",
    "\n",
    "    # TODO add truncated, values\n",
    "    for t in reversed(range(rewards_n - 1)):\n",
    "        returns[t] = rewards[t] + (~terminated[t]) * gamma * returns[t + 1]\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, gamma=1):\n",
    "        self.gamma = gamma\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.terminated = []\n",
    "        self.truncated = []\n",
    "\n",
    "    def add_transition(self, obs, action, log_prob, reward, terminated, truncated):\n",
    "        self.observations.append(obs)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminated.append(terminated)\n",
    "        self.truncated.append(truncated)\n",
    "\n",
    "    def get_data(self):\n",
    "        data = {\n",
    "            \"observations\": torch.tensor(self.observations, dtype=torch.float32),\n",
    "            \"actions\": torch.tensor(self.actions, dtype=torch.long),\n",
    "            \"log_probs\": torch.stack(self.log_probs).squeeze(),\n",
    "            \"rewards\": torch.tensor(self.rewards, dtype=torch.float32),\n",
    "            \"terminated\": torch.tensor(self.terminated, dtype=torch.bool),\n",
    "            \"truncated\": torch.tensor(self.truncated, dtype=torch.bool),\n",
    "        }\n",
    "\n",
    "        data[\"q_estimations\"] = get_returns(data[\"rewards\"], data[\"terminated\"])\n",
    "\n",
    "        return data\n",
    "\n",
    "    def collect_rollouts(self, env, policy, rollout_size=None, trajectories_n=None):\n",
    "        self.clear()\n",
    "        trajectories_collected = 0\n",
    "        steps_collected = 0\n",
    "\n",
    "        while True:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "            while True:\n",
    "                action, log_prob_action = policy.predict(obs)\n",
    "                next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "                self.add_transition(\n",
    "                    obs, action, log_prob_action, reward, terminated, truncated\n",
    "                )\n",
    "                obs = next_obs\n",
    "\n",
    "                steps_collected += 1\n",
    "                if rollout_size and steps_collected >= rollout_size:\n",
    "                    return\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "            trajectories_collected += 1\n",
    "\n",
    "            if trajectories_n and trajectories_collected >= trajectories_n:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_loss(data, reward_normalization=True):\n",
    "    returns = data[\"q_estimations\"]\n",
    "    log_probs = data[\"log_probs\"]\n",
    "\n",
    "    if reward_normalization:\n",
    "        mean = returns.mean()\n",
    "        std = returns.std()\n",
    "        returns = (returns - mean) / (std + 1e-8)\n",
    "\n",
    "    loss = -(log_probs * returns).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(\n",
    "    env: gym.Env, policy: DiscreteStochasticMlpPolicy, optimizer, episode_n: int = 100\n",
    "):\n",
    "    steps_n = 0\n",
    "    buffer = RolloutBuffer()\n",
    "    logger = Logger()\n",
    "\n",
    "    for _ in range(episode_n):\n",
    "        buffer.collect_rollouts(env, policy, trajectories_n=1)\n",
    "        data = buffer.get_data()\n",
    "\n",
    "        loss = reinforce_loss(data)\n",
    "\n",
    "        steps_n += data[\"observations\"].shape[0]\n",
    "        logger.log(steps_n, data)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DiscreteStochasticMlpPolicy(input_size, output_size)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce(env, policy, optimizer, episode_n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving...\n",
      "201.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trajectory = get_trajectory(env, policy, visualize=True)\n",
    "print(sum(trajectory[\"rewards\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110.15"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, policy, deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpCritic(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1_step_td_advantage(rewards, values, terminated, gamma=1):\n",
    "    targets = rewards[:-1] + gamma * (~terminated[:-1]) * values[1:]\n",
    "    advantages = targets - values[:-1]\n",
    "    return targets, advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_step_advantage(rewards, values, terminated, gamma=1):\n",
    "    returns = get_returns(rewards, terminated)\n",
    "    targets = returns[:-1]\n",
    "    advantages = targets - values[:-1]\n",
    "    return targets, advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c_loss(data, critic, advantage_fn):\n",
    "    loss = {}\n",
    "\n",
    "    log_probs = data[\"log_probs\"][:-1]\n",
    "\n",
    "    observations = data[\"observations\"]\n",
    "    values = critic(observations).squeeze()\n",
    "    targets = data[\"q_estimations\"]\n",
    "    advantages = targets[:-1].detach() - values[:-1]\n",
    "\n",
    "    loss[\"actor\"] = -(log_probs * advantages.detach()).mean()\n",
    "    loss[\"critic\"] = (advantages**2).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c(\n",
    "    env: gym.Env,\n",
    "    actor: DiscreteStochasticMlpPolicy,\n",
    "    critic: MlpCritic,\n",
    "    actor_optimizer,\n",
    "    critic_optimizer,\n",
    "    total_timesteps: int = 100_000,\n",
    "    # rollout_size: int = 200,\n",
    "    trajectories_n: int = 10,\n",
    "    advantage_fn: callable = get_max_step_advantage,\n",
    "):\n",
    "    buffer = RolloutBuffer()\n",
    "    logger = Logger()\n",
    "\n",
    "    steps_n = 0\n",
    "    while steps_n < total_timesteps:\n",
    "\n",
    "        buffer.collect_rollouts(env, actor, trajectories_n=trajectories_n)\n",
    "\n",
    "        data = buffer.get_data()\n",
    "\n",
    "        rollout_size = data[\"observations\"].shape[0]\n",
    "        steps_n += rollout_size\n",
    "\n",
    "        loss = a2c_loss(data, critic, advantage_fn)\n",
    "\n",
    "        logger.log(steps_n, data)\n",
    "\n",
    "        loss[\"actor\"].backward()\n",
    "        actor_optimizer.step()\n",
    "        actor_optimizer.zero_grad()\n",
    "\n",
    "        loss[\"critic\"].backward()\n",
    "        critic_optimizer.step()\n",
    "        critic_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = DiscreteStochasticMlpPolicy(input_size, output_size)\n",
    "critic = MlpCritic(input_size)\n",
    "\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_n: 1164\n",
      "mean_trajectory_rewards: 19.799999237060547\n",
      "mean_trajectory_length: 19.80000114440918\n",
      "steps_n: 2063\n",
      "mean_trajectory_rewards: 21.899999618530273\n",
      "mean_trajectory_length: 21.899999618530273\n",
      "steps_n: 3107\n",
      "mean_trajectory_rewards: 33.5\n",
      "mean_trajectory_length: 33.5\n",
      "steps_n: 4041\n",
      "mean_trajectory_rewards: 19.0\n",
      "mean_trajectory_length: 19.0\n",
      "steps_n: 5204\n",
      "mean_trajectory_rewards: 30.5\n",
      "mean_trajectory_length: 30.5\n",
      "steps_n: 6073\n",
      "mean_trajectory_rewards: 22.100000381469727\n",
      "mean_trajectory_length: 22.100000381469727\n",
      "steps_n: 7059\n",
      "mean_trajectory_rewards: 21.100000381469727\n",
      "mean_trajectory_length: 21.100000381469727\n",
      "steps_n: 8085\n",
      "mean_trajectory_rewards: 28.799999237060547\n",
      "mean_trajectory_length: 28.80000114440918\n",
      "steps_n: 9032\n",
      "mean_trajectory_rewards: 19.899999618530273\n",
      "mean_trajectory_length: 19.899999618530273\n",
      "steps_n: 10020\n",
      "mean_trajectory_rewards: 23.799999237060547\n",
      "mean_trajectory_length: 23.80000114440918\n",
      "steps_n: 11072\n",
      "mean_trajectory_rewards: 22.5\n",
      "mean_trajectory_length: 22.5\n",
      "steps_n: 12054\n",
      "mean_trajectory_rewards: 24.5\n",
      "mean_trajectory_length: 24.5\n",
      "steps_n: 13032\n",
      "mean_trajectory_rewards: 18.600000381469727\n",
      "mean_trajectory_length: 18.600000381469727\n",
      "steps_n: 14138\n",
      "mean_trajectory_rewards: 22.899999618530273\n",
      "mean_trajectory_length: 22.899999618530273\n",
      "steps_n: 15123\n",
      "mean_trajectory_rewards: 19.600000381469727\n",
      "mean_trajectory_length: 19.600000381469727\n",
      "steps_n: 16241\n",
      "mean_trajectory_rewards: 31.100000381469727\n",
      "mean_trajectory_length: 31.100000381469727\n",
      "steps_n: 17030\n",
      "mean_trajectory_rewards: 21.899999618530273\n",
      "mean_trajectory_length: 21.899999618530273\n",
      "steps_n: 18095\n",
      "mean_trajectory_rewards: 22.299999237060547\n",
      "mean_trajectory_length: 22.30000114440918\n",
      "steps_n: 19095\n",
      "mean_trajectory_rewards: 29.600000381469727\n",
      "mean_trajectory_length: 29.600000381469727\n",
      "steps_n: 20147\n",
      "mean_trajectory_rewards: 45.20000076293945\n",
      "mean_trajectory_length: 45.20000076293945\n",
      "steps_n: 21036\n",
      "mean_trajectory_rewards: 27.5\n",
      "mean_trajectory_length: 27.5\n",
      "steps_n: 22077\n",
      "mean_trajectory_rewards: 33.0\n",
      "mean_trajectory_length: 33.0\n",
      "steps_n: 23195\n",
      "mean_trajectory_rewards: 39.599998474121094\n",
      "mean_trajectory_length: 39.60000228881836\n",
      "steps_n: 24194\n",
      "mean_trajectory_rewards: 38.5\n",
      "mean_trajectory_length: 38.5\n",
      "steps_n: 25321\n",
      "mean_trajectory_rewards: 43.400001525878906\n",
      "mean_trajectory_length: 43.400001525878906\n",
      "steps_n: 26250\n",
      "mean_trajectory_rewards: 52.79999923706055\n",
      "mean_trajectory_length: 52.79999923706055\n",
      "steps_n: 27062\n",
      "mean_trajectory_rewards: 29.899999618530273\n",
      "mean_trajectory_length: 29.899999618530273\n",
      "steps_n: 28175\n",
      "mean_trajectory_rewards: 37.70000076293945\n",
      "mean_trajectory_length: 37.70000076293945\n",
      "steps_n: 29361\n",
      "mean_trajectory_rewards: 39.5\n",
      "mean_trajectory_length: 39.5\n",
      "steps_n: 30155\n",
      "mean_trajectory_rewards: 44.79999923706055\n",
      "mean_trajectory_length: 44.79999923706055\n",
      "steps_n: 31401\n",
      "mean_trajectory_rewards: 40.900001525878906\n",
      "mean_trajectory_length: 40.900001525878906\n",
      "steps_n: 32344\n",
      "mean_trajectory_rewards: 53.79999923706055\n",
      "mean_trajectory_length: 53.79999923706055\n",
      "steps_n: 33365\n",
      "mean_trajectory_rewards: 49.099998474121094\n",
      "mean_trajectory_length: 49.10000228881836\n",
      "steps_n: 34318\n",
      "mean_trajectory_rewards: 59.400001525878906\n",
      "mean_trajectory_length: 59.400001525878906\n",
      "steps_n: 35271\n",
      "mean_trajectory_rewards: 42.5\n",
      "mean_trajectory_length: 42.5\n",
      "steps_n: 36100\n",
      "mean_trajectory_rewards: 42.400001525878906\n",
      "mean_trajectory_length: 42.400001525878906\n",
      "steps_n: 37085\n",
      "mean_trajectory_rewards: 57.099998474121094\n",
      "mean_trajectory_length: 57.10000228881836\n",
      "steps_n: 38174\n",
      "mean_trajectory_rewards: 50.29999923706055\n",
      "mean_trajectory_length: 50.29999923706055\n",
      "steps_n: 39477\n",
      "mean_trajectory_rewards: 53.599998474121094\n",
      "mean_trajectory_length: 53.60000228881836\n",
      "steps_n: 40508\n",
      "mean_trajectory_rewards: 53.79999923706055\n",
      "mean_trajectory_length: 53.79999923706055\n",
      "steps_n: 41420\n",
      "mean_trajectory_rewards: 48.0\n",
      "mean_trajectory_length: 48.0\n",
      "steps_n: 42414\n",
      "mean_trajectory_rewards: 54.900001525878906\n",
      "mean_trajectory_length: 54.900001525878906\n",
      "steps_n: 43065\n",
      "mean_trajectory_rewards: 65.0\n",
      "mean_trajectory_length: 65.0\n",
      "steps_n: 44500\n",
      "mean_trajectory_rewards: 71.19999694824219\n",
      "mean_trajectory_length: 71.20000457763672\n",
      "steps_n: 45277\n",
      "mean_trajectory_rewards: 77.5999984741211\n",
      "mean_trajectory_length: 77.5999984741211\n",
      "steps_n: 46438\n",
      "mean_trajectory_rewards: 57.0\n",
      "mean_trajectory_length: 57.0\n",
      "steps_n: 47064\n",
      "mean_trajectory_rewards: 62.5\n",
      "mean_trajectory_length: 62.5\n",
      "steps_n: 48105\n",
      "mean_trajectory_rewards: 46.20000076293945\n",
      "mean_trajectory_length: 46.20000076293945\n",
      "steps_n: 49150\n",
      "mean_trajectory_rewards: 52.5\n",
      "mean_trajectory_length: 52.5\n",
      "steps_n: 50485\n",
      "mean_trajectory_rewards: 74.19999694824219\n",
      "mean_trajectory_length: 74.20000457763672\n"
     ]
    }
   ],
   "source": [
    "a2c(env, actor, critic, actor_optimizer, critic_optimizer, total_timesteps=50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360.35"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, actor, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.95"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, actor, deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_loss(\n",
    "    data,\n",
    "    actor,\n",
    "    critic,\n",
    "    epsilon: float = 0.3,\n",
    "    advantage_normalization: bool = True,\n",
    "    rewards_normalization: bool = True,\n",
    "):\n",
    "    loss = {}\n",
    "\n",
    "    old_log_probs = data[\"log_probs\"][:-1]\n",
    "\n",
    "    observations = data[\"observations\"]\n",
    "    _, new_log_probs = actor.predict(observations[:-1])\n",
    "\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "    ratio_clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "\n",
    "    values = critic(observations).squeeze()\n",
    "\n",
    "    targets = data[\"q_estimations\"]\n",
    "    advantages = targets[:-1].detach() - values[:-1]\n",
    "\n",
    "    actor_loss_1 = ratio * advantages.detach()\n",
    "    actor_loss_2 = ratio_clipped * advantages.detach()\n",
    "\n",
    "    loss[\"actor\"] = -(torch.min(actor_loss_1, actor_loss_2)).mean()\n",
    "    loss[\"critic\"] = (advantages**2).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(\n",
    "    env: gym.Env,\n",
    "    actor: DiscreteStochasticMlpPolicy,\n",
    "    critic: MlpCritic,\n",
    "    actor_optimizer,\n",
    "    critic_optimizer,\n",
    "    total_timesteps: int = 100_000,\n",
    "    trajectories_n: int = 10,\n",
    "    epoch_n: int = 3,\n",
    "    batch_size: int = 256,\n",
    "    advantage_fn: callable = get_max_step_advantage,\n",
    "):\n",
    "    buffer = RolloutBuffer()\n",
    "    logger = Logger()\n",
    "\n",
    "    steps_n = 0\n",
    "    while steps_n < total_timesteps:\n",
    "\n",
    "        buffer.collect_rollouts(env, actor, trajectories_n=trajectories_n)\n",
    "\n",
    "        data = buffer.get_data()\n",
    "\n",
    "        rollout_size = data[\"observations\"].shape[0]\n",
    "        steps_n += rollout_size\n",
    "\n",
    "        for _ in range(epoch_n):\n",
    "            indices = np.random.permutation(range(rollout_size))\n",
    "\n",
    "            for start in range(0, rollout_size, batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "\n",
    "                if batch_indices.size <= 1:\n",
    "                    break\n",
    "\n",
    "                batch = {key: value[batch_indices] for key, value in data.items()}\n",
    "\n",
    "                loss = ppo_loss(batch, actor, critic)\n",
    "\n",
    "                loss[\"actor\"].backward()\n",
    "                actor_optimizer.step()\n",
    "                actor_optimizer.zero_grad()\n",
    "\n",
    "                loss[\"critic\"].backward()\n",
    "                critic_optimizer.step()\n",
    "                critic_optimizer.zero_grad()\n",
    "                \n",
    "        logger.log(steps_n, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = DiscreteStochasticMlpPolicy(input_size, output_size)\n",
    "critic = MlpCritic(input_size)\n",
    "\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_n: 1019\n",
      "mean_trajectory_rewards: 23.899999618530273\n",
      "mean_trajectory_length: 23.899999618530273\n",
      "steps_n: 2229\n",
      "mean_trajectory_rewards: 25.399999618530273\n",
      "mean_trajectory_length: 25.399999618530273\n",
      "steps_n: 3092\n",
      "mean_trajectory_rewards: 20.399999618530273\n",
      "mean_trajectory_length: 20.399999618530273\n",
      "steps_n: 4122\n",
      "mean_trajectory_rewards: 22.399999618530273\n",
      "mean_trajectory_length: 22.399999618530273\n",
      "steps_n: 5204\n",
      "mean_trajectory_rewards: 22.399999618530273\n",
      "mean_trajectory_length: 22.399999618530273\n",
      "steps_n: 6017\n",
      "mean_trajectory_rewards: 25.299999237060547\n",
      "mean_trajectory_length: 25.30000114440918\n",
      "steps_n: 7091\n",
      "mean_trajectory_rewards: 22.399999618530273\n",
      "mean_trajectory_length: 22.399999618530273\n",
      "steps_n: 8026\n",
      "mean_trajectory_rewards: 20.899999618530273\n",
      "mean_trajectory_length: 20.899999618530273\n",
      "steps_n: 9077\n",
      "mean_trajectory_rewards: 28.399999618530273\n",
      "mean_trajectory_length: 28.399999618530273\n",
      "steps_n: 10074\n",
      "mean_trajectory_rewards: 26.600000381469727\n",
      "mean_trajectory_length: 26.600000381469727\n",
      "steps_n: 11177\n",
      "mean_trajectory_rewards: 18.899999618530273\n",
      "mean_trajectory_length: 18.899999618530273\n",
      "steps_n: 12013\n",
      "mean_trajectory_rewards: 17.600000381469727\n",
      "mean_trajectory_length: 17.600000381469727\n",
      "steps_n: 13034\n",
      "mean_trajectory_rewards: 17.100000381469727\n",
      "mean_trajectory_length: 17.100000381469727\n",
      "steps_n: 14226\n",
      "mean_trajectory_rewards: 23.200000762939453\n",
      "mean_trajectory_length: 23.200000762939453\n",
      "steps_n: 15071\n",
      "mean_trajectory_rewards: 28.299999237060547\n",
      "mean_trajectory_length: 28.30000114440918\n",
      "steps_n: 16056\n",
      "mean_trajectory_rewards: 19.799999237060547\n",
      "mean_trajectory_length: 19.80000114440918\n",
      "steps_n: 17190\n",
      "mean_trajectory_rewards: 20.5\n",
      "mean_trajectory_length: 20.5\n",
      "steps_n: 18075\n",
      "mean_trajectory_rewards: 17.299999237060547\n",
      "mean_trajectory_length: 17.30000114440918\n",
      "steps_n: 19133\n",
      "mean_trajectory_rewards: 22.100000381469727\n",
      "mean_trajectory_length: 22.100000381469727\n",
      "steps_n: 20002\n",
      "mean_trajectory_rewards: 16.0\n",
      "mean_trajectory_length: 16.0\n",
      "steps_n: 21103\n",
      "mean_trajectory_rewards: 21.600000381469727\n",
      "mean_trajectory_length: 21.600000381469727\n",
      "steps_n: 22042\n",
      "mean_trajectory_rewards: 18.5\n",
      "mean_trajectory_length: 18.5\n",
      "steps_n: 23121\n",
      "mean_trajectory_rewards: 19.799999237060547\n",
      "mean_trajectory_length: 19.80000114440918\n",
      "steps_n: 24053\n",
      "mean_trajectory_rewards: 17.399999618530273\n",
      "mean_trajectory_length: 17.399999618530273\n",
      "steps_n: 25178\n",
      "mean_trajectory_rewards: 22.200000762939453\n",
      "mean_trajectory_length: 22.200000762939453\n",
      "steps_n: 26148\n",
      "mean_trajectory_rewards: 27.100000381469727\n",
      "mean_trajectory_length: 27.100000381469727\n",
      "steps_n: 27187\n",
      "mean_trajectory_rewards: 22.5\n",
      "mean_trajectory_length: 22.5\n",
      "steps_n: 28036\n",
      "mean_trajectory_rewards: 33.599998474121094\n",
      "mean_trajectory_length: 33.60000228881836\n",
      "steps_n: 29056\n",
      "mean_trajectory_rewards: 17.899999618530273\n",
      "mean_trajectory_length: 17.899999618530273\n",
      "steps_n: 30069\n",
      "mean_trajectory_rewards: 16.899999618530273\n",
      "mean_trajectory_length: 16.899999618530273\n",
      "steps_n: 31106\n",
      "mean_trajectory_rewards: 23.799999237060547\n",
      "mean_trajectory_length: 23.80000114440918\n",
      "steps_n: 32283\n",
      "mean_trajectory_rewards: 28.700000762939453\n",
      "mean_trajectory_length: 28.700000762939453\n",
      "steps_n: 33073\n",
      "mean_trajectory_rewards: 16.5\n",
      "mean_trajectory_length: 16.5\n",
      "steps_n: 34149\n",
      "mean_trajectory_rewards: 21.399999618530273\n",
      "mean_trajectory_length: 21.399999618530273\n",
      "steps_n: 35008\n",
      "mean_trajectory_rewards: 21.899999618530273\n",
      "mean_trajectory_length: 21.899999618530273\n",
      "steps_n: 36154\n",
      "mean_trajectory_rewards: 22.700000762939453\n",
      "mean_trajectory_length: 22.700000762939453\n",
      "steps_n: 37081\n",
      "mean_trajectory_rewards: 21.700000762939453\n",
      "mean_trajectory_length: 21.700000762939453\n",
      "steps_n: 38148\n",
      "mean_trajectory_rewards: 16.799999237060547\n",
      "mean_trajectory_length: 16.80000114440918\n",
      "steps_n: 39240\n",
      "mean_trajectory_rewards: 25.200000762939453\n",
      "mean_trajectory_length: 25.200000762939453\n",
      "steps_n: 40078\n",
      "mean_trajectory_rewards: 20.299999237060547\n",
      "mean_trajectory_length: 20.30000114440918\n",
      "steps_n: 41196\n",
      "mean_trajectory_rewards: 24.799999237060547\n",
      "mean_trajectory_length: 24.80000114440918\n",
      "steps_n: 42084\n",
      "mean_trajectory_rewards: 15.399999618530273\n",
      "mean_trajectory_length: 15.40000057220459\n",
      "steps_n: 43174\n",
      "mean_trajectory_rewards: 19.299999237060547\n",
      "mean_trajectory_length: 19.30000114440918\n",
      "steps_n: 44197\n",
      "mean_trajectory_rewards: 25.5\n",
      "mean_trajectory_length: 25.5\n",
      "steps_n: 45038\n",
      "mean_trajectory_rewards: 27.0\n",
      "mean_trajectory_length: 27.0\n",
      "steps_n: 46146\n",
      "mean_trajectory_rewards: 23.299999237060547\n",
      "mean_trajectory_length: 23.30000114440918\n",
      "steps_n: 47104\n",
      "mean_trajectory_rewards: 26.0\n",
      "mean_trajectory_length: 26.0\n",
      "steps_n: 48034\n",
      "mean_trajectory_rewards: 15.5\n",
      "mean_trajectory_length: 15.5\n",
      "steps_n: 49265\n",
      "mean_trajectory_rewards: 32.099998474121094\n",
      "mean_trajectory_length: 32.10000228881836\n",
      "steps_n: 50122\n",
      "mean_trajectory_rewards: 23.600000381469727\n",
      "mean_trajectory_length: 23.600000381469727\n",
      "steps_n: 51174\n",
      "mean_trajectory_rewards: 17.700000762939453\n",
      "mean_trajectory_length: 17.700000762939453\n",
      "steps_n: 52013\n",
      "mean_trajectory_rewards: 16.600000381469727\n",
      "mean_trajectory_length: 16.600000381469727\n",
      "steps_n: 53116\n",
      "mean_trajectory_rewards: 18.100000381469727\n",
      "mean_trajectory_length: 18.100000381469727\n",
      "steps_n: 54029\n",
      "mean_trajectory_rewards: 18.899999618530273\n",
      "mean_trajectory_length: 18.899999618530273\n",
      "steps_n: 55062\n",
      "mean_trajectory_rewards: 24.799999237060547\n",
      "mean_trajectory_length: 24.80000114440918\n",
      "steps_n: 56209\n",
      "mean_trajectory_rewards: 28.299999237060547\n",
      "mean_trajectory_length: 28.30000114440918\n",
      "steps_n: 57147\n",
      "mean_trajectory_rewards: 27.399999618530273\n",
      "mean_trajectory_length: 27.399999618530273\n",
      "steps_n: 58089\n",
      "mean_trajectory_rewards: 15.800000190734863\n",
      "mean_trajectory_length: 15.800000190734863\n",
      "steps_n: 59158\n",
      "mean_trajectory_rewards: 32.099998474121094\n",
      "mean_trajectory_length: 32.10000228881836\n",
      "steps_n: 60259\n",
      "mean_trajectory_rewards: 26.399999618530273\n",
      "mean_trajectory_length: 26.399999618530273\n",
      "steps_n: 61144\n",
      "mean_trajectory_rewards: 18.299999237060547\n",
      "mean_trajectory_length: 18.30000114440918\n",
      "steps_n: 62182\n",
      "mean_trajectory_rewards: 21.899999618530273\n",
      "mean_trajectory_length: 21.899999618530273\n",
      "steps_n: 63186\n",
      "mean_trajectory_rewards: 26.799999237060547\n",
      "mean_trajectory_length: 26.80000114440918\n",
      "steps_n: 64086\n",
      "mean_trajectory_rewards: 28.5\n",
      "mean_trajectory_length: 28.5\n",
      "steps_n: 65172\n",
      "mean_trajectory_rewards: 28.200000762939453\n",
      "mean_trajectory_length: 28.200000762939453\n",
      "steps_n: 66098\n",
      "mean_trajectory_rewards: 22.5\n",
      "mean_trajectory_length: 22.5\n",
      "steps_n: 67106\n",
      "mean_trajectory_rewards: 20.0\n",
      "mean_trajectory_length: 20.0\n",
      "steps_n: 68129\n",
      "mean_trajectory_rewards: 27.700000762939453\n",
      "mean_trajectory_length: 27.700000762939453\n",
      "steps_n: 69119\n",
      "mean_trajectory_rewards: 22.399999618530273\n",
      "mean_trajectory_length: 22.399999618530273\n",
      "steps_n: 70174\n",
      "mean_trajectory_rewards: 22.700000762939453\n",
      "mean_trajectory_length: 22.700000762939453\n",
      "steps_n: 71151\n",
      "mean_trajectory_rewards: 16.799999237060547\n",
      "mean_trajectory_length: 16.80000114440918\n",
      "steps_n: 72132\n",
      "mean_trajectory_rewards: 23.200000762939453\n",
      "mean_trajectory_length: 23.200000762939453\n",
      "steps_n: 73212\n",
      "mean_trajectory_rewards: 25.299999237060547\n",
      "mean_trajectory_length: 25.30000114440918\n",
      "steps_n: 74165\n",
      "mean_trajectory_rewards: 27.100000381469727\n",
      "mean_trajectory_length: 27.100000381469727\n",
      "steps_n: 75040\n",
      "mean_trajectory_rewards: 21.899999618530273\n",
      "mean_trajectory_length: 21.899999618530273\n",
      "steps_n: 76125\n",
      "mean_trajectory_rewards: 21.200000762939453\n",
      "mean_trajectory_length: 21.200000762939453\n",
      "steps_n: 77021\n",
      "mean_trajectory_rewards: 24.299999237060547\n",
      "mean_trajectory_length: 24.30000114440918\n",
      "steps_n: 78158\n",
      "mean_trajectory_rewards: 19.5\n",
      "mean_trajectory_length: 19.5\n",
      "steps_n: 79223\n",
      "mean_trajectory_rewards: 24.600000381469727\n",
      "mean_trajectory_length: 24.600000381469727\n",
      "steps_n: 80149\n",
      "mean_trajectory_rewards: 19.100000381469727\n",
      "mean_trajectory_length: 19.100000381469727\n",
      "steps_n: 81041\n",
      "mean_trajectory_rewards: 25.200000762939453\n",
      "mean_trajectory_length: 25.200000762939453\n",
      "steps_n: 82004\n",
      "mean_trajectory_rewards: 21.600000381469727\n",
      "mean_trajectory_length: 21.600000381469727\n",
      "steps_n: 83100\n",
      "mean_trajectory_rewards: 15.0\n",
      "mean_trajectory_length: 15.0\n",
      "steps_n: 84060\n",
      "mean_trajectory_rewards: 23.899999618530273\n",
      "mean_trajectory_length: 23.899999618530273\n",
      "steps_n: 85136\n",
      "mean_trajectory_rewards: 18.5\n",
      "mean_trajectory_length: 18.5\n",
      "steps_n: 86192\n",
      "mean_trajectory_rewards: 24.5\n",
      "mean_trajectory_length: 24.5\n",
      "steps_n: 87028\n",
      "mean_trajectory_rewards: 22.899999618530273\n",
      "mean_trajectory_length: 22.899999618530273\n",
      "steps_n: 88178\n",
      "mean_trajectory_rewards: 23.299999237060547\n",
      "mean_trajectory_length: 23.30000114440918\n",
      "steps_n: 89068\n",
      "mean_trajectory_rewards: 23.899999618530273\n",
      "mean_trajectory_length: 23.899999618530273\n",
      "steps_n: 90024\n",
      "mean_trajectory_rewards: 16.100000381469727\n",
      "mean_trajectory_length: 16.100000381469727\n",
      "steps_n: 91107\n",
      "mean_trajectory_rewards: 18.399999618530273\n",
      "mean_trajectory_length: 18.399999618530273\n",
      "steps_n: 92038\n",
      "mean_trajectory_rewards: 19.600000381469727\n",
      "mean_trajectory_length: 19.600000381469727\n",
      "steps_n: 93152\n",
      "mean_trajectory_rewards: 21.799999237060547\n",
      "mean_trajectory_length: 21.80000114440918\n",
      "steps_n: 94086\n",
      "mean_trajectory_rewards: 18.200000762939453\n",
      "mean_trajectory_length: 18.200000762939453\n",
      "steps_n: 95138\n",
      "mean_trajectory_rewards: 25.0\n",
      "mean_trajectory_length: 25.0\n",
      "steps_n: 96120\n",
      "mean_trajectory_rewards: 24.799999237060547\n",
      "mean_trajectory_length: 24.80000114440918\n",
      "steps_n: 97021\n",
      "mean_trajectory_rewards: 22.5\n",
      "mean_trajectory_length: 22.5\n",
      "steps_n: 98200\n",
      "mean_trajectory_rewards: 22.299999237060547\n",
      "mean_trajectory_length: 22.30000114440918\n",
      "steps_n: 99103\n",
      "mean_trajectory_rewards: 17.600000381469727\n",
      "mean_trajectory_length: 17.600000381469727\n",
      "steps_n: 100029\n",
      "mean_trajectory_rewards: 23.299999237060547\n",
      "mean_trajectory_length: 23.30000114440918\n"
     ]
    }
   ],
   "source": [
    "ppo(env, actor, critic, actor_optimizer, critic_optimizer, total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  8., 11.,  3.,  7., 14.,  6., 17.,  8.,  2.,  6.,  3.,  3.,  2.,\n",
       "         5.,  9.,  9.,  5., 17.,  7.,  3.,  6., 13.,  5.,  4., 11.,  7.,  4.,\n",
       "        10., 24.,  9., 12.,  8., 11.,  8.,  4.,  7., 10., 11.,  3., 34., 15.,\n",
       "         2., 15., 17.,  5.,  8.,  9.,  4., 19., 16., 16., 21.,  3., 12., 10.,\n",
       "         6., 20.,  8., 12.,  7., 16., 20.,  4., 15.,  9.,  3., 14.,  9., 10.,\n",
       "         1.,  2., 13., 11., 17., 22.,  2., 27.,  1.,  2.,  2., 14.,  7., 35.,\n",
       "         1., 12.,  2.,  7.,  9., 20., 11., 15., 10.,  6., 11.,  2.,  3.,  9.,\n",
       "        19.,  1.,  1.,  6.,  8., 12., 19.,  6.,  3.,  5.,  8.,  9., 12.,  8.,\n",
       "        13., 14.,  5.,  8., 22.,  4., 13., 17., 30., 11., 12.,  5., 15., 16.,\n",
       "        10.,  8., 25.,  7.,  2., 14.,  5.,  5.,  5.,  7., 32., 15.,  2.,  9.,\n",
       "         3.,  1.,  7., 18., 10., 17.,  1.,  6.,  1.,  4., 10., 13.,  8., 19.,\n",
       "         8., 13.,  6., 37., 11.,  3.,  1., 21.,  3.,  2., 18.,  8., 10., 39.,\n",
       "        11., 15.,  2., 10.,  2., 12.,  6., 14.,  4., 15.,  3.,  2., 31.,  7.,\n",
       "        18., 14., 21.,  4.,  9.,  4.,  7., 20., 17.,  3.,  4.,  6., 17.,  3.,\n",
       "        10.,  3., 13., 10.,  8.,  5., 26., 36.,  8., 13., 28.,  5.,  6.,  4.,\n",
       "        13.,  7., 14., 18., 12.,  6.,  5.,  4.,  1.,  9.,  6., 14.,  2., 19.,\n",
       "        29.,  7.,  9.,  4., 13.,  6.,  7., 11., 21., 13.,  2.,  7., 18., 11.,\n",
       "         2.,  8.,  3.,  4.,  1., 13.,  9., 13., 10., 10.,  9., 10.,  1., 10.,\n",
       "         4., 20., 18.,  1.])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"q_estimations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"observations\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2666, -0.0473, -0.1145, -0.0358, -0.0603, -0.1464, -0.1577, -0.1342,\n",
       "        -0.1583, -0.2862, -0.2080, -0.1062, -0.2603, -0.0448, -0.1917, -0.0853,\n",
       "        -0.0522, -0.0389, -0.1201, -0.1180, -0.0116, -0.2338, -0.0495, -0.0491,\n",
       "        -0.1637, -0.0426, -0.0702, -0.0492, -0.1224, -0.1968, -0.1240, -0.0979,\n",
       "        -0.0500, -0.0623, -0.0545, -0.1392, -0.2119, -0.0675, -0.1242, -0.0420,\n",
       "        -0.1187, -0.0750, -0.1046, -0.1210, -0.1183, -0.0417, -0.1088, -0.0444,\n",
       "        -0.0416, -0.1194, -0.1264, -0.0805, -0.1270, -0.1964, -0.0307, -0.1429,\n",
       "        -0.0300, -0.0865, -0.0381, -0.0484, -0.0472, -0.1483, -0.0901, -0.0357,\n",
       "        -0.0684, -0.0254, -0.2776, -0.1946, -0.0658, -0.0325, -0.2860, -0.0307,\n",
       "        -0.0686, -0.0662, -0.2015, -0.1234, -0.2574, -0.1603,  0.0066, -0.0145,\n",
       "        -0.0421, -0.0443, -0.2203, -0.1493, -0.0307, -0.0687, -0.0017, -0.0988,\n",
       "        -0.2162, -0.0860, -0.2111, -0.1224, -0.0933, -0.0966, -0.0967, -0.0135,\n",
       "        -0.0046, -0.0566, -0.1214, -0.0305, -0.0620, -0.0401, -0.2428, -0.1977,\n",
       "        -0.1956, -0.2064, -0.0304, -0.0350, -0.0832, -0.0897, -0.1546, -0.1972,\n",
       "        -0.0391, -0.1320, -0.2238, -0.0668, -0.1412, -0.0361, -0.1157, -0.1284,\n",
       "        -0.1882, -0.0678, -0.0952, -0.0421, -0.1613, -0.0981, -0.0717, -0.0345,\n",
       "        -0.1659, -0.0400, -0.1865, -0.0511, -0.1191, -0.2246, -0.0577, -0.1217,\n",
       "        -0.1198, -0.0560, -0.0235, -0.1136, -0.0860, -0.0130, -0.0379, -0.1572,\n",
       "        -0.0680, -0.1241, -0.2035, -0.0461, -0.0363, -0.2512, -0.0796, -0.1272,\n",
       "        -0.2043, -0.1576, -0.0964, -0.1689, -0.1531, -0.1999, -0.0400, -0.2014,\n",
       "        -0.0374, -0.1157, -0.1958, -0.2184, -0.1252, -0.0426, -0.1589, -0.1491,\n",
       "        -0.0929, -0.1631, -0.0215, -0.1559, -0.2588, -0.0582, -0.0483, -0.1240,\n",
       "        -0.2102, -0.1390, -0.0441, -0.0501, -0.1532, -0.2204, -0.0890, -0.1926,\n",
       "        -0.0970, -0.2098, -0.1911, -0.2832, -0.1297, -0.1241, -0.0947, -0.0420,\n",
       "        -0.0211, -0.0727, -0.1597, -0.0295, -0.0611, -0.2312, -0.1650, -0.0538,\n",
       "        -0.0755, -0.0436, -0.1298, -0.1831, -0.0660, -0.1232, -0.1909, -0.1971,\n",
       "        -0.0401, -0.0296, -0.0400, -0.1915, -0.0944, -0.0834, -0.0995, -0.0513,\n",
       "        -0.0234, -0.0540, -0.0324, -0.0735, -0.0544, -0.0477, -0.1260, -0.1208,\n",
       "        -0.2045, -0.0402, -0.0707, -0.0136, -0.2072, -0.0486, -0.0730, -0.1755,\n",
       "        -0.1140, -0.0955, -0.0373, -0.0443, -0.1247, -0.1197, -0.0347, -0.1438,\n",
       "        -0.0294, -0.0491, -0.0154, -0.1234, -0.1059, -0.0836, -0.1497, -0.0975,\n",
       "        -0.1154, -0.0471, -0.2085, -0.0794, -0.0419, -0.1617, -0.1225, -0.1263],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic(batch[\"observations\"]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  8., 11.,  3.,  7., 14.,  6., 17.,  8.,  2.,  6.,  3.,  3.,  2.,\n",
       "         5.,  9.,  9.,  5., 17.,  7.,  3.,  6., 13.,  5.,  4., 11.,  7.,  4.,\n",
       "        10., 24.,  9., 12.,  8., 11.,  8.,  4.,  7., 10., 11.,  3., 34., 15.,\n",
       "         2., 15., 17.,  5.,  8.,  9.,  4., 19., 16., 16., 21.,  3., 12., 10.,\n",
       "         6., 20.,  8., 12.,  7., 16., 20.,  4., 15.,  9.,  3., 14.,  9., 10.,\n",
       "         1.,  2., 13., 11., 17., 22.,  2., 27.,  1.,  2.,  2., 14.,  7., 35.,\n",
       "         1., 12.,  2.,  7.,  9., 20., 11., 15., 10.,  6., 11.,  2.,  3.,  9.,\n",
       "        19.,  1.,  1.,  6.,  8., 12., 19.,  6.,  3.,  5.,  8.,  9., 12.,  8.,\n",
       "        13., 14.,  5.,  8., 22.,  4., 13., 17., 30., 11., 12.,  5., 15., 16.,\n",
       "        10.,  8., 25.,  7.,  2., 14.,  5.,  5.,  5.,  7., 32., 15.,  2.,  9.,\n",
       "         3.,  1.,  7., 18., 10., 17.,  1.,  6.,  1.,  4., 10., 13.,  8., 19.,\n",
       "         8., 13.,  6., 37., 11.,  3.,  1., 21.,  3.,  2., 18.,  8., 10., 39.,\n",
       "        11., 15.,  2., 10.,  2., 12.,  6., 14.,  4., 15.,  3.,  2., 31.,  7.,\n",
       "        18., 14., 21.,  4.,  9.,  4.,  7., 20., 17.,  3.,  4.,  6., 17.,  3.,\n",
       "        10.,  3., 13., 10.,  8.,  5., 26., 36.,  8., 13., 28.,  5.,  6.,  4.,\n",
       "        13.,  7., 14., 18., 12.,  6.,  5.,  4.,  1.,  9.,  6., 14.,  2., 19.,\n",
       "        29.,  7.,  9.,  4., 13.,  6.,  7., 11., 21., 13.,  2.,  7., 18., 11.,\n",
       "         2.,  8.,  3.,  4.,  1., 13.,  9., 13., 10., 10.,  9., 10.,  1., 10.,\n",
       "         4., 20., 18.,  1.])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"q_estimations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.1"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, actor, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "agent = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn(100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.15"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, actor, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.7"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, actor, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = RolloutBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.collect_rollouts(env, actor, trajectories_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "probs = actor.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0178676 , -0.03208   , -0.01955283,  0.04398148], dtype=float32)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.FloatTensor(obs.reshape(-1, actor.obs_dim))\n",
    "output = actor.act(actor.fc1(input))\n",
    "output = actor.act(actor.fc2(output))\n",
    "logits = actor.fc3(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0728,\n",
       "         0.2238, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1319, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0106,  0.0015]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4970, 0.5030]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rb.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"terminated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = {}\n",
    "epsilon = 0.1\n",
    "old_log_probs = data[\"log_probs\"][:-1]\n",
    "\n",
    "observations = data[\"observations\"]\n",
    "_, new_log_probs = actor.predict(observations[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "ratio_clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0162, 1.0000, 0.9844, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9783, 0.9881, 1.0121,\n",
       "        0.9880, 0.9880, 0.9834, 1.0223, 1.0163, 0.9890, 0.9844, 1.0000, 1.0000,\n",
       "        0.9801, 1.0000, 1.0000, 1.0000, 1.0181, 1.0118, 1.0000, 0.9885, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0244, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0162, 1.0000, 0.9844, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9783, 0.9881, 1.0121,\n",
       "        0.9880, 0.9880, 0.9834, 1.0223, 1.0163, 0.9890, 0.9844, 1.0000, 1.0000,\n",
       "        0.9801, 1.0000, 1.0000, 1.0000, 1.0181, 1.0118, 1.0000, 0.9885, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0244, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<ClampBackward1>)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = data[\"rewards\"]\n",
    "terminated = data[\"terminated\"]\n",
    "\n",
    "values = critic(observations).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = get_returns(rewards, terminated)\n",
    "advantages = returns[:-1] - values[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([44, 4]), torch.Size([43]))"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations.shape, advantages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16., 15., 14., 13., 12., 11., 10.,  9.,  8.,  7.,  6.,  5.,  4.,  3.,\n",
       "         2.,  1., 17., 16., 15., 14., 13., 12., 11., 10.,  9.,  8.,  7.,  6.,\n",
       "         5.,  4.,  3.,  2.,  1., 11., 10.,  9.,  8.,  7.,  6.,  5.,  4.,  3.,\n",
       "         2.,  1.])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -2.3278,  -3.4338,  -5.0032,  -5.4325,  -6.8733,  -7.8434,  -8.6129,\n",
       "         -9.2907, -10.4307, -11.2181, -12.2135, -12.1152, -10.9649, -12.3075,\n",
       "        -10.9311,  -9.3239,  -1.3634,  -2.3534,  -3.3723,  -4.3511,  -5.8654,\n",
       "         -7.0074,  -7.6689,  -8.3411,  -9.5152, -10.3506, -10.3001, -11.6120,\n",
       "        -10.8306, -12.1843, -10.9931, -12.2267, -13.4220,  -7.4125,  -8.3714,\n",
       "         -9.6782, -10.5685, -10.6768,  -9.6708, -11.1053, -12.4615, -11.2891,\n",
       "        -12.5376], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if advantage_normalization:\n",
    "    mean = advantages.mean()\n",
    "    std = advantages.std()\n",
    "    advantages = (advantages - mean) / (std + 1e-8)\n",
    "\n",
    "actor_loss_1 = ratio * advantages.detach()\n",
    "actor_loss_2 = ratio_clipped * advantages.detach()\n",
    "\n",
    "loss[\"actor\"] = -(torch.min(actor_loss_1, actor_loss_2)).mean()\n",
    "loss[\"critic\"] = (advantages**2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 4])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"observations\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " tensor([-0.6961, -0.6966, -0.6961, -0.6818, -0.7037, -0.6809, -0.6898, -0.6955,\n",
       "         -0.6967, -0.6954, -0.6969, -0.6909, -0.7077, -0.7125, -0.6773, -0.6961,\n",
       "         -0.7109, -0.6755, -0.6750, -0.6760, -0.6729, -0.6862, -0.6920, -0.6853,\n",
       "         -0.6922, -0.7015, -0.6924, -0.6999, -0.6940, -0.6837, -0.6938, -0.6827,\n",
       "         -0.6735, -0.7054, -0.7124, -0.7094, -0.7076, -0.6774, -0.6890, -0.6917,\n",
       "         -0.6889, -0.6802, -0.6802, -0.6802, -0.6978, -0.6922, -0.6978, -0.6802,\n",
       "         -0.6980, -0.7062, -0.6981, -0.7061, -0.6788, -0.6805, -0.6879, -0.6938,\n",
       "         -0.6810, -0.6940, -0.6880, -0.7059, -0.6877, -0.7059, -0.6990, -0.6806,\n",
       "         -0.6870, -0.6942, -0.6866, -0.7059, -0.6860, -0.6951, -0.6792, -0.6769,\n",
       "         -0.6781, -0.6883, -0.7012, -0.7014, -0.6766, -0.6779],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.predict(data[\"observations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic = True\n",
    "input = data[\"observations\"][0]\n",
    "probs = actor.forward(input)\n",
    "dist = Categorical(probs)\n",
    "\n",
    "if deterministic:\n",
    "    action = torch.argmax(probs, dim=1)\n",
    "else:\n",
    "    action = dist.sample()\n",
    "\n",
    "log_prob_action = dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5269, 0.4731]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(probs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int64)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0266, -0.1906,  0.0273,  0.2716],\n",
       "        [-0.0304,  0.0042,  0.0327, -0.0124],\n",
       "        [-0.0303,  0.1988,  0.0325, -0.2946]])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = [1, 2, 3]\n",
    "data[\"observations\"][idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1000.)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data[\"rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dones = data[\"terminated\"] + data[\"truncated\"]\n",
    "if sum(dones) == 0:\n",
    "    dones[-1] = True\n",
    "dones_indeces = dones.nonzero()\n",
    "last_done_index = dones_indeces[-1][0].item()\n",
    "trajectory_n = sum(dones)\n",
    "mean_trajectory_rewards = sum(data[\"rewards\"][:last_done_index]) / trajectory_n\n",
    "mean_trajectory_length = last_done_index / trajectory_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data[\"terminated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_n: 1001\n",
      "mean_trajectory_rewards: 35.10714340209961\n",
      "mean_trajectory_length: 35.10714340209961\n"
     ]
    }
   ],
   "source": [
    "logger.log(1001, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_n: 5\n",
      "mean_trajectory_rewards: 4.0\n",
      "mean_trajectory_length: 4.0\n"
     ]
    }
   ],
   "source": [
    "logger.log(5, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"terminated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data[\"truncated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data[\"terminated\"] + data[\"truncated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_trajectory_rewards: 35.10714340209961\n",
      "mean_trajectory_length: 35.10714340209961\n"
     ]
    }
   ],
   "source": [
    "dones = data[\"terminated\"] + data[\"truncated\"]\n",
    "dones_indeces = dones.nonzero()\n",
    "last_done_index = dones_indeces[-1][0].item()\n",
    "trajectory_n = sum(dones)\n",
    "mean_trajectory_rewards = sum(data[\"rewards\"][:last_done_index]) / trajectory_n\n",
    "mean_trajectory_length = last_done_index / trajectory_n\n",
    "\n",
    "print(f\"mean_trajectory_rewards: {mean_trajectory_rewards}\")\n",
    "print(f\"mean_trajectory_length: {mean_trajectory_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data[\"terminated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28)\n"
     ]
    }
   ],
   "source": [
    "print(trajectory_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(242.7500), tensor(242.7500))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_trajectory_length, mean_trajectory_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"observations\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
