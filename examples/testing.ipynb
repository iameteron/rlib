{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from rlib.algorithms.reinforce import reinforce\n",
    "from rlib.common.policies import DiscreteStochasticMlpPolicy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "discrete = True\n",
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "\n",
    "if discrete:\n",
    "    output_size = env.action_space.n\n",
    "else:\n",
    "    output_size = 2 * env.action_space.shape[0]\n",
    "\n",
    "print(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DiscreteStochasticMlpPolicy(input_size, output_size)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\никита\\coding\\rl\\rlib\\rlib\\common\\buffer.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  \"observations\": torch.tensor(self.observations, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_n: 1012\n",
      "mean_trajectory_rewards: 13.0\n",
      "mean_trajectory_length: 13.0\n",
      "steps_n: 2010\n",
      "mean_trajectory_rewards: 14.0\n",
      "mean_trajectory_length: 14.0\n",
      "steps_n: 3006\n",
      "mean_trajectory_rewards: 11.0\n",
      "mean_trajectory_length: 11.0\n",
      "steps_n: 4017\n",
      "mean_trajectory_rewards: 66.0\n",
      "mean_trajectory_length: 66.0\n",
      "steps_n: 5009\n",
      "mean_trajectory_rewards: 54.0\n",
      "mean_trajectory_length: 54.0\n",
      "steps_n: 6004\n",
      "mean_trajectory_rewards: 33.0\n",
      "mean_trajectory_length: 33.0\n",
      "steps_n: 7066\n",
      "mean_trajectory_rewards: 70.0\n",
      "mean_trajectory_length: 70.0\n",
      "steps_n: 8013\n",
      "mean_trajectory_rewards: 34.0\n",
      "mean_trajectory_length: 34.0\n",
      "steps_n: 9031\n",
      "mean_trajectory_rewards: 85.0\n",
      "mean_trajectory_length: 85.0\n",
      "steps_n: 10043\n",
      "mean_trajectory_rewards: 59.0\n",
      "mean_trajectory_length: 59.0\n",
      "steps_n: 11075\n",
      "mean_trajectory_rewards: 81.0\n",
      "mean_trajectory_length: 81.0\n",
      "steps_n: 12027\n",
      "mean_trajectory_rewards: 33.0\n",
      "mean_trajectory_length: 33.0\n",
      "steps_n: 13072\n",
      "mean_trajectory_rewards: 108.0\n",
      "mean_trajectory_length: 108.0\n",
      "steps_n: 14013\n",
      "mean_trajectory_rewards: 32.0\n",
      "mean_trajectory_length: 32.0\n",
      "steps_n: 15102\n",
      "mean_trajectory_rewards: 135.0\n",
      "mean_trajectory_length: 135.0\n",
      "steps_n: 16018\n",
      "mean_trajectory_rewards: 52.0\n",
      "mean_trajectory_length: 52.0\n",
      "steps_n: 17092\n",
      "mean_trajectory_rewards: 165.0\n",
      "mean_trajectory_length: 165.0\n",
      "steps_n: 18011\n",
      "mean_trajectory_rewards: 187.0\n",
      "mean_trajectory_length: 187.0\n",
      "steps_n: 19103\n",
      "mean_trajectory_rewards: 152.0\n",
      "mean_trajectory_length: 152.0\n",
      "steps_n: 20228\n",
      "mean_trajectory_rewards: 275.0\n",
      "mean_trajectory_length: 275.0\n",
      "steps_n: 21087\n",
      "mean_trajectory_rewards: 219.0\n",
      "mean_trajectory_length: 219.0\n",
      "steps_n: 22009\n",
      "mean_trajectory_rewards: 164.0\n",
      "mean_trajectory_length: 164.0\n",
      "steps_n: 23031\n",
      "mean_trajectory_rewards: 66.0\n",
      "mean_trajectory_length: 66.0\n",
      "steps_n: 24020\n",
      "mean_trajectory_rewards: 129.0\n",
      "mean_trajectory_length: 129.0\n",
      "steps_n: 25031\n",
      "mean_trajectory_rewards: 58.0\n",
      "mean_trajectory_length: 58.0\n",
      "steps_n: 26018\n",
      "mean_trajectory_rewards: 49.0\n",
      "mean_trajectory_length: 49.0\n",
      "steps_n: 27083\n",
      "mean_trajectory_rewards: 191.0\n",
      "mean_trajectory_length: 191.0\n",
      "steps_n: 28125\n",
      "mean_trajectory_rewards: 257.0\n",
      "mean_trajectory_length: 257.0\n",
      "steps_n: 29010\n",
      "mean_trajectory_rewards: 233.0\n",
      "mean_trajectory_length: 233.0\n",
      "steps_n: 30005\n",
      "mean_trajectory_rewards: 109.0\n",
      "mean_trajectory_length: 109.0\n",
      "steps_n: 31053\n",
      "mean_trajectory_rewards: 170.0\n",
      "mean_trajectory_length: 170.0\n",
      "steps_n: 32036\n",
      "mean_trajectory_rewards: 142.0\n",
      "mean_trajectory_length: 142.0\n",
      "steps_n: 33113\n",
      "mean_trajectory_rewards: 159.0\n",
      "mean_trajectory_length: 159.0\n",
      "steps_n: 34177\n",
      "mean_trajectory_rewards: 189.0\n",
      "mean_trajectory_length: 189.0\n",
      "steps_n: 35014\n",
      "mean_trajectory_rewards: 186.0\n",
      "mean_trajectory_length: 186.0\n",
      "steps_n: 36092\n",
      "mean_trajectory_rewards: 279.0\n",
      "mean_trajectory_length: 279.0\n",
      "steps_n: 37164\n",
      "mean_trajectory_rewards: 181.0\n",
      "mean_trajectory_length: 181.0\n",
      "steps_n: 38039\n",
      "mean_trajectory_rewards: 46.0\n",
      "mean_trajectory_length: 46.0\n",
      "steps_n: 39101\n",
      "mean_trajectory_rewards: 196.0\n",
      "mean_trajectory_length: 196.0\n",
      "steps_n: 40163\n",
      "mean_trajectory_rewards: 164.0\n",
      "mean_trajectory_length: 164.0\n",
      "steps_n: 41361\n",
      "mean_trajectory_rewards: 378.0\n",
      "mean_trajectory_length: 378.0\n",
      "steps_n: 42068\n",
      "mean_trajectory_rewards: 251.0\n",
      "mean_trajectory_length: 251.0\n",
      "steps_n: 43007\n",
      "mean_trajectory_rewards: 74.0\n",
      "mean_trajectory_length: 74.0\n",
      "steps_n: 44047\n",
      "mean_trajectory_rewards: 93.0\n",
      "mean_trajectory_length: 93.0\n",
      "steps_n: 45322\n",
      "mean_trajectory_rewards: 371.0\n",
      "mean_trajectory_length: 371.0\n",
      "steps_n: 46262\n",
      "mean_trajectory_rewards: 264.0\n",
      "mean_trajectory_length: 264.0\n",
      "steps_n: 47144\n",
      "mean_trajectory_rewards: 239.0\n",
      "mean_trajectory_length: 239.0\n",
      "steps_n: 48179\n",
      "mean_trajectory_rewards: 281.0\n",
      "mean_trajectory_length: 281.0\n",
      "steps_n: 49136\n",
      "mean_trajectory_rewards: 142.0\n",
      "mean_trajectory_length: 142.0\n",
      "steps_n: 50151\n",
      "mean_trajectory_rewards: 188.0\n",
      "mean_trajectory_length: 188.0\n",
      "steps_n: 51112\n",
      "mean_trajectory_rewards: 202.0\n",
      "mean_trajectory_length: 202.0\n",
      "steps_n: 52095\n",
      "mean_trajectory_rewards: 193.0\n",
      "mean_trajectory_length: 193.0\n",
      "steps_n: 53109\n",
      "mean_trajectory_rewards: 183.0\n",
      "mean_trajectory_length: 183.0\n",
      "steps_n: 54167\n",
      "mean_trajectory_rewards: 202.0\n",
      "mean_trajectory_length: 202.0\n",
      "steps_n: 55268\n",
      "mean_trajectory_rewards: 276.0\n",
      "mean_trajectory_length: 276.0\n",
      "steps_n: 56094\n",
      "mean_trajectory_rewards: 136.0\n",
      "mean_trajectory_length: 136.0\n",
      "steps_n: 57107\n",
      "mean_trajectory_rewards: 154.0\n",
      "mean_trajectory_length: 154.0\n",
      "steps_n: 58040\n",
      "mean_trajectory_rewards: 187.0\n",
      "mean_trajectory_length: 187.0\n",
      "steps_n: 59156\n",
      "mean_trajectory_rewards: 191.0\n",
      "mean_trajectory_length: 191.0\n",
      "steps_n: 60290\n",
      "mean_trajectory_rewards: 415.0\n",
      "mean_trajectory_length: 415.0\n",
      "steps_n: 61114\n",
      "mean_trajectory_rewards: 357.0\n",
      "mean_trajectory_length: 357.0\n",
      "steps_n: 62174\n",
      "mean_trajectory_rewards: 280.0\n",
      "mean_trajectory_length: 280.0\n",
      "steps_n: 63074\n",
      "mean_trajectory_rewards: 441.0\n",
      "mean_trajectory_length: 441.0\n",
      "steps_n: 64255\n",
      "mean_trajectory_rewards: 279.0\n",
      "mean_trajectory_length: 279.0\n",
      "steps_n: 65022\n",
      "mean_trajectory_rewards: 168.0\n",
      "mean_trajectory_length: 168.0\n",
      "steps_n: 66191\n",
      "mean_trajectory_rewards: 234.0\n",
      "mean_trajectory_length: 234.0\n",
      "steps_n: 67208\n",
      "mean_trajectory_rewards: 215.0\n",
      "mean_trajectory_length: 215.0\n",
      "steps_n: 68124\n",
      "mean_trajectory_rewards: 340.0\n",
      "mean_trajectory_length: 340.0\n",
      "steps_n: 69197\n",
      "mean_trajectory_rewards: 208.0\n",
      "mean_trajectory_length: 208.0\n",
      "steps_n: 70116\n",
      "mean_trajectory_rewards: 200.0\n",
      "mean_trajectory_length: 200.0\n",
      "steps_n: 71022\n",
      "mean_trajectory_rewards: 251.0\n",
      "mean_trajectory_length: 251.0\n",
      "steps_n: 72214\n",
      "mean_trajectory_rewards: 260.0\n",
      "mean_trajectory_length: 260.0\n",
      "steps_n: 73238\n",
      "mean_trajectory_rewards: 316.0\n",
      "mean_trajectory_length: 316.0\n",
      "steps_n: 74184\n",
      "mean_trajectory_rewards: 362.0\n",
      "mean_trajectory_length: 362.0\n",
      "steps_n: 75101\n",
      "mean_trajectory_rewards: 178.0\n",
      "mean_trajectory_length: 178.0\n",
      "steps_n: 76239\n",
      "mean_trajectory_rewards: 416.0\n",
      "mean_trajectory_length: 416.0\n",
      "steps_n: 77077\n",
      "mean_trajectory_rewards: 173.0\n",
      "mean_trajectory_length: 173.0\n",
      "steps_n: 78262\n",
      "mean_trajectory_rewards: 268.0\n",
      "mean_trajectory_length: 268.0\n",
      "steps_n: 79171\n",
      "mean_trajectory_rewards: 200.0\n",
      "mean_trajectory_length: 200.0\n",
      "steps_n: 80045\n",
      "mean_trajectory_rewards: 261.0\n",
      "mean_trajectory_length: 261.0\n",
      "steps_n: 81125\n",
      "mean_trajectory_rewards: 255.0\n",
      "mean_trajectory_length: 255.0\n",
      "steps_n: 82015\n",
      "mean_trajectory_rewards: 284.0\n",
      "mean_trajectory_length: 284.0\n",
      "steps_n: 83027\n",
      "mean_trajectory_rewards: 499.0\n",
      "mean_trajectory_length: 499.0\n",
      "steps_n: 84200\n",
      "mean_trajectory_rewards: 406.0\n",
      "mean_trajectory_length: 406.0\n",
      "steps_n: 85214\n",
      "mean_trajectory_rewards: 279.0\n",
      "mean_trajectory_length: 279.0\n",
      "steps_n: 86183\n",
      "mean_trajectory_rewards: 254.0\n",
      "mean_trajectory_length: 254.0\n",
      "steps_n: 87097\n",
      "mean_trajectory_rewards: 422.0\n",
      "mean_trajectory_length: 422.0\n",
      "steps_n: 88070\n",
      "mean_trajectory_rewards: 209.0\n",
      "mean_trajectory_length: 209.0\n",
      "steps_n: 89151\n",
      "mean_trajectory_rewards: 246.0\n",
      "mean_trajectory_length: 246.0\n",
      "steps_n: 90129\n",
      "mean_trajectory_rewards: 167.0\n",
      "mean_trajectory_length: 167.0\n",
      "steps_n: 91010\n",
      "mean_trajectory_rewards: 175.0\n",
      "mean_trajectory_length: 175.0\n",
      "steps_n: 92105\n",
      "mean_trajectory_rewards: 199.0\n",
      "mean_trajectory_length: 199.0\n",
      "steps_n: 93257\n",
      "mean_trajectory_rewards: 437.0\n",
      "mean_trajectory_length: 437.0\n",
      "steps_n: 94036\n",
      "mean_trajectory_rewards: 232.0\n",
      "mean_trajectory_length: 232.0\n",
      "steps_n: 95100\n",
      "mean_trajectory_rewards: 226.0\n",
      "mean_trajectory_length: 226.0\n",
      "steps_n: 96098\n",
      "mean_trajectory_rewards: 180.0\n",
      "mean_trajectory_length: 180.0\n",
      "steps_n: 97251\n",
      "mean_trajectory_rewards: 285.0\n",
      "mean_trajectory_length: 285.0\n",
      "steps_n: 98151\n",
      "mean_trajectory_rewards: 380.0\n",
      "mean_trajectory_length: 380.0\n",
      "steps_n: 99052\n",
      "mean_trajectory_rewards: 114.0\n",
      "mean_trajectory_length: 114.0\n",
      "steps_n: 100034\n",
      "mean_trajectory_rewards: 170.0\n",
      "mean_trajectory_length: 170.0\n",
      "steps_n: 101197\n",
      "mean_trajectory_rewards: 330.0\n",
      "mean_trajectory_length: 330.0\n",
      "steps_n: 102132\n",
      "mean_trajectory_rewards: 246.0\n",
      "mean_trajectory_length: 246.0\n"
     ]
    }
   ],
   "source": [
    "reinforce(env, policy, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
