{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "from torch.optim import Adam\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from rlib.algorithms.a2c import a2c\n",
    "from rlib.algorithms.ppo import ppo\n",
    "from rlib.algorithms.reinforce import reinforce\n",
    "from rlib.common.evaluation import validation\n",
    "from rlib.common.policies import (\n",
    "    DiscreteStochasticMlpPolicy,\n",
    "    MlpCritic,\n",
    "    StochasticMlpPolicy,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "min_action, max_action = -1, 1\n",
    "env = RescaleAction(env, min_action, max_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "discrete = True\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "if discrete:\n",
    "    action_dim = env.action_space.n\n",
    "else:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(obs_dim, action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "if discrete:\n",
    "    policy = DiscreteStochasticMlpPolicy(obs_dim, action_dim)\n",
    "else:\n",
    "    policy = StochasticMlpPolicy(obs_dim, action_dim)\n",
    "\n",
    "optimizer = Adam(policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_n: 1026\n",
      "mean_trajectory_rewards: 17.0\n",
      "mean_trajectory_length: 17.0\n",
      "steps_n: 2076\n",
      "mean_trajectory_rewards: 41.0\n",
      "mean_trajectory_length: 41.0\n",
      "steps_n: 3070\n",
      "mean_trajectory_rewards: 41.0\n",
      "mean_trajectory_length: 41.0\n",
      "steps_n: 4036\n",
      "mean_trajectory_rewards: 28.0\n",
      "mean_trajectory_length: 28.0\n",
      "steps_n: 5044\n",
      "mean_trajectory_rewards: 25.0\n",
      "mean_trajectory_length: 25.0\n",
      "steps_n: 6042\n",
      "mean_trajectory_rewards: 31.0\n",
      "mean_trajectory_length: 31.0\n",
      "steps_n: 7002\n",
      "mean_trajectory_rewards: 46.0\n",
      "mean_trajectory_length: 46.0\n",
      "steps_n: 8008\n",
      "mean_trajectory_rewards: 16.0\n",
      "mean_trajectory_length: 16.0\n",
      "steps_n: 9014\n",
      "mean_trajectory_rewards: 17.0\n",
      "mean_trajectory_length: 17.0\n",
      "steps_n: 10032\n",
      "mean_trajectory_rewards: 45.0\n",
      "mean_trajectory_length: 45.0\n",
      "steps_n: 11046\n",
      "mean_trajectory_rewards: 48.0\n",
      "mean_trajectory_length: 48.0\n",
      "steps_n: 12018\n",
      "mean_trajectory_rewards: 64.0\n",
      "mean_trajectory_length: 64.0\n",
      "steps_n: 13154\n",
      "mean_trajectory_rewards: 94.0\n",
      "mean_trajectory_length: 94.0\n",
      "steps_n: 14128\n",
      "mean_trajectory_rewards: 110.0\n",
      "mean_trajectory_length: 110.0\n",
      "steps_n: 15006\n",
      "mean_trajectory_rewards: 76.0\n",
      "mean_trajectory_length: 76.0\n",
      "steps_n: 16112\n",
      "mean_trajectory_rewards: 132.0\n",
      "mean_trajectory_length: 132.0\n",
      "steps_n: 17308\n",
      "mean_trajectory_rewards: 195.0\n",
      "mean_trajectory_length: 195.0\n",
      "steps_n: 18010\n",
      "mean_trajectory_rewards: 35.0\n",
      "mean_trajectory_length: 35.0\n",
      "steps_n: 19282\n",
      "mean_trajectory_rewards: 267.0\n",
      "mean_trajectory_length: 267.0\n",
      "steps_n: 20020\n",
      "mean_trajectory_rewards: 116.0\n",
      "mean_trajectory_length: 116.0\n",
      "steps_n: 21200\n",
      "mean_trajectory_rewards: 242.0\n",
      "mean_trajectory_length: 242.0\n",
      "steps_n: 22286\n",
      "mean_trajectory_rewards: 147.0\n",
      "mean_trajectory_length: 147.0\n",
      "steps_n: 23182\n",
      "mean_trajectory_rewards: 104.0\n",
      "mean_trajectory_length: 104.0\n",
      "steps_n: 24696\n",
      "mean_trajectory_rewards: 352.0\n",
      "mean_trajectory_length: 352.0\n",
      "steps_n: 25074\n",
      "mean_trajectory_rewards: 188.0\n",
      "mean_trajectory_length: 188.0\n",
      "steps_n: 26258\n",
      "mean_trajectory_rewards: 208.0\n",
      "mean_trajectory_length: 208.0\n",
      "steps_n: 27196\n",
      "mean_trajectory_rewards: 258.0\n",
      "mean_trajectory_length: 258.0\n",
      "steps_n: 28074\n",
      "mean_trajectory_rewards: 140.0\n",
      "mean_trajectory_length: 140.0\n",
      "steps_n: 29084\n",
      "mean_trajectory_rewards: 131.0\n",
      "mean_trajectory_length: 131.0\n",
      "steps_n: 30088\n",
      "mean_trajectory_rewards: 136.0\n",
      "mean_trajectory_length: 136.0\n",
      "steps_n: 31054\n",
      "mean_trajectory_rewards: 152.0\n",
      "mean_trajectory_length: 152.0\n",
      "steps_n: 32124\n",
      "mean_trajectory_rewards: 137.0\n",
      "mean_trajectory_length: 137.0\n",
      "steps_n: 33044\n",
      "mean_trajectory_rewards: 121.0\n",
      "mean_trajectory_length: 121.0\n",
      "steps_n: 34050\n",
      "mean_trajectory_rewards: 58.0\n",
      "mean_trajectory_length: 58.0\n",
      "steps_n: 35202\n",
      "mean_trajectory_rewards: 137.0\n",
      "mean_trajectory_length: 137.0\n",
      "steps_n: 36080\n",
      "mean_trajectory_rewards: 268.0\n",
      "mean_trajectory_length: 268.0\n",
      "steps_n: 37314\n",
      "mean_trajectory_rewards: 216.0\n",
      "mean_trajectory_length: 216.0\n",
      "steps_n: 38150\n",
      "mean_trajectory_rewards: 195.0\n",
      "mean_trajectory_length: 195.0\n",
      "steps_n: 39150\n",
      "mean_trajectory_rewards: 499.0\n",
      "mean_trajectory_length: 499.0\n",
      "steps_n: 40276\n",
      "mean_trajectory_rewards: 194.0\n",
      "mean_trajectory_length: 194.0\n",
      "steps_n: 41754\n",
      "mean_trajectory_rewards: 499.0\n",
      "mean_trajectory_length: 499.0\n",
      "steps_n: 42444\n",
      "mean_trajectory_rewards: 344.0\n",
      "mean_trajectory_length: 344.0\n",
      "steps_n: 43652\n",
      "mean_trajectory_rewards: 482.0\n",
      "mean_trajectory_length: 482.0\n",
      "steps_n: 44450\n",
      "mean_trajectory_rewards: 398.0\n",
      "mean_trajectory_length: 398.0\n",
      "steps_n: 45450\n",
      "mean_trajectory_rewards: 499.0\n",
      "mean_trajectory_length: 499.0\n",
      "steps_n: 46378\n",
      "mean_trajectory_rewards: 349.0\n",
      "mean_trajectory_length: 349.0\n",
      "steps_n: 47284\n",
      "mean_trajectory_rewards: 226.0\n",
      "mean_trajectory_length: 226.0\n",
      "steps_n: 48794\n",
      "mean_trajectory_rewards: 420.0\n",
      "mean_trajectory_length: 420.0\n",
      "steps_n: 49158\n",
      "mean_trajectory_rewards: 181.0\n",
      "mean_trajectory_length: 181.0\n",
      "steps_n: 50108\n",
      "mean_trajectory_rewards: 239.0\n",
      "mean_trajectory_length: 239.0\n",
      "steps_n: 51328\n",
      "mean_trajectory_rewards: 269.0\n",
      "mean_trajectory_length: 269.0\n",
      "steps_n: 52472\n",
      "mean_trajectory_rewards: 397.0\n",
      "mean_trajectory_length: 397.0\n",
      "steps_n: 53348\n",
      "mean_trajectory_rewards: 300.0\n",
      "mean_trajectory_length: 300.0\n",
      "steps_n: 54126\n",
      "mean_trajectory_rewards: 388.0\n",
      "mean_trajectory_length: 388.0\n",
      "steps_n: 55654\n",
      "mean_trajectory_rewards: 349.0\n",
      "mean_trajectory_length: 349.0\n",
      "steps_n: 56182\n",
      "mean_trajectory_rewards: 263.0\n",
      "mean_trajectory_length: 263.0\n",
      "steps_n: 57218\n",
      "mean_trajectory_rewards: 168.0\n",
      "mean_trajectory_length: 168.0\n",
      "steps_n: 58150\n",
      "mean_trajectory_rewards: 193.0\n",
      "mean_trajectory_length: 193.0\n",
      "steps_n: 59626\n",
      "mean_trajectory_rewards: 491.0\n",
      "mean_trajectory_length: 491.0\n",
      "steps_n: 60306\n",
      "mean_trajectory_rewards: 339.0\n",
      "mean_trajectory_length: 339.0\n",
      "steps_n: 61524\n",
      "mean_trajectory_rewards: 340.0\n",
      "mean_trajectory_length: 340.0\n",
      "steps_n: 62610\n",
      "mean_trajectory_rewards: 335.0\n",
      "mean_trajectory_length: 335.0\n",
      "steps_n: 63090\n",
      "mean_trajectory_rewards: 239.0\n",
      "mean_trajectory_length: 239.0\n",
      "steps_n: 64218\n",
      "mean_trajectory_rewards: 146.0\n",
      "mean_trajectory_length: 146.0\n",
      "steps_n: 65132\n",
      "mean_trajectory_rewards: 154.0\n",
      "mean_trajectory_length: 154.0\n",
      "steps_n: 66078\n",
      "mean_trajectory_rewards: 256.0\n",
      "mean_trajectory_length: 256.0\n",
      "steps_n: 67156\n",
      "mean_trajectory_rewards: 189.0\n",
      "mean_trajectory_length: 189.0\n",
      "steps_n: 68254\n",
      "mean_trajectory_rewards: 228.0\n",
      "mean_trajectory_length: 228.0\n",
      "steps_n: 69266\n",
      "mean_trajectory_rewards: 215.0\n",
      "mean_trajectory_length: 215.0\n",
      "steps_n: 70010\n",
      "mean_trajectory_rewards: 226.0\n",
      "mean_trajectory_length: 226.0\n",
      "steps_n: 71042\n",
      "mean_trajectory_rewards: 145.0\n",
      "mean_trajectory_length: 145.0\n",
      "steps_n: 72004\n",
      "mean_trajectory_rewards: 215.0\n",
      "mean_trajectory_length: 215.0\n",
      "steps_n: 73144\n",
      "mean_trajectory_rewards: 171.0\n",
      "mean_trajectory_length: 171.0\n",
      "steps_n: 74092\n",
      "mean_trajectory_rewards: 151.0\n",
      "mean_trajectory_length: 151.0\n",
      "steps_n: 75756\n",
      "mean_trajectory_rewards: 457.0\n",
      "mean_trajectory_length: 457.0\n",
      "steps_n: 76216\n",
      "mean_trajectory_rewards: 229.0\n",
      "mean_trajectory_length: 229.0\n",
      "steps_n: 77880\n",
      "mean_trajectory_rewards: 499.0\n",
      "mean_trajectory_length: 499.0\n",
      "steps_n: 78312\n",
      "mean_trajectory_rewards: 215.0\n",
      "mean_trajectory_length: 215.0\n",
      "steps_n: 79530\n",
      "mean_trajectory_rewards: 358.0\n",
      "mean_trajectory_length: 358.0\n",
      "steps_n: 80530\n",
      "mean_trajectory_rewards: 499.0\n",
      "mean_trajectory_length: 499.0\n",
      "steps_n: 81838\n",
      "mean_trajectory_rewards: 490.0\n",
      "mean_trajectory_length: 490.0\n",
      "steps_n: 82448\n",
      "mean_trajectory_rewards: 304.0\n",
      "mean_trajectory_length: 304.0\n",
      "steps_n: 83234\n",
      "mean_trajectory_rewards: 392.0\n",
      "mean_trajectory_length: 392.0\n",
      "steps_n: 84258\n",
      "mean_trajectory_rewards: 305.0\n",
      "mean_trajectory_length: 305.0\n",
      "steps_n: 85034\n",
      "mean_trajectory_rewards: 387.0\n",
      "mean_trajectory_length: 387.0\n",
      "steps_n: 86642\n",
      "mean_trajectory_rewards: 434.0\n",
      "mean_trajectory_length: 434.0\n",
      "steps_n: 87254\n",
      "mean_trajectory_rewards: 305.0\n",
      "mean_trajectory_length: 305.0\n",
      "steps_n: 88484\n",
      "mean_trajectory_rewards: 362.0\n",
      "mean_trajectory_length: 362.0\n",
      "steps_n: 89224\n",
      "mean_trajectory_rewards: 115.0\n",
      "mean_trajectory_length: 115.0\n",
      "steps_n: 90224\n",
      "mean_trajectory_rewards: 499.0\n",
      "mean_trajectory_length: 499.0\n",
      "steps_n: 91224\n",
      "mean_trajectory_rewards: 499.0\n",
      "mean_trajectory_length: 499.0\n",
      "steps_n: 92142\n",
      "mean_trajectory_rewards: 286.0\n",
      "mean_trajectory_length: 286.0\n",
      "steps_n: 93262\n",
      "mean_trajectory_rewards: 214.0\n",
      "mean_trajectory_length: 214.0\n",
      "steps_n: 94532\n",
      "mean_trajectory_rewards: 382.0\n",
      "mean_trajectory_length: 382.0\n",
      "steps_n: 95212\n",
      "mean_trajectory_rewards: 129.0\n",
      "mean_trajectory_length: 129.0\n",
      "steps_n: 96340\n",
      "mean_trajectory_rewards: 262.0\n",
      "mean_trajectory_length: 262.0\n",
      "steps_n: 97094\n",
      "mean_trajectory_rewards: 376.0\n",
      "mean_trajectory_length: 376.0\n",
      "steps_n: 98448\n",
      "mean_trajectory_rewards: 236.0\n",
      "mean_trajectory_length: 236.0\n",
      "steps_n: 99170\n",
      "mean_trajectory_rewards: 360.0\n",
      "mean_trajectory_length: 360.0\n",
      "steps_n: 100062\n",
      "mean_trajectory_rewards: 445.0\n",
      "mean_trajectory_length: 445.0\n"
     ]
    }
   ],
   "source": [
    "reinforce(env, policy, optimizer, total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(322.3)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, policy, deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "if discrete:\n",
    "    actor = DiscreteStochasticMlpPolicy(obs_dim, action_dim)\n",
    "else:\n",
    "    actor = StochasticMlpPolicy(obs_dim, action_dim)\n",
    "\n",
    "critic = MlpCritic(obs_dim)\n",
    "\n",
    "actor_optimizer = Adam(actor.parameters(), lr=3e-4)\n",
    "critic_optimizer = Adam(critic.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_n: 1202\n",
      "mean_trajectory_rewards: 31.5\n",
      "mean_trajectory_length: 31.5\n",
      "steps_n: 2145\n",
      "mean_trajectory_rewards: 21.100000381469727\n",
      "mean_trajectory_length: 21.100000381469727\n",
      "steps_n: 3176\n",
      "mean_trajectory_rewards: 26.799999237060547\n",
      "mean_trajectory_length: 26.80000114440918\n",
      "steps_n: 4079\n",
      "mean_trajectory_rewards: 22.700000762939453\n",
      "mean_trajectory_length: 22.700000762939453\n",
      "steps_n: 5129\n",
      "mean_trajectory_rewards: 27.5\n",
      "mean_trajectory_length: 27.5\n",
      "steps_n: 6215\n",
      "mean_trajectory_rewards: 25.899999618530273\n",
      "mean_trajectory_length: 25.899999618530273\n",
      "steps_n: 7160\n",
      "mean_trajectory_rewards: 23.799999237060547\n",
      "mean_trajectory_length: 23.80000114440918\n",
      "steps_n: 8231\n",
      "mean_trajectory_rewards: 30.899999618530273\n",
      "mean_trajectory_length: 30.899999618530273\n",
      "steps_n: 9009\n",
      "mean_trajectory_rewards: 32.5\n",
      "mean_trajectory_length: 32.5\n",
      "steps_n: 10220\n",
      "mean_trajectory_rewards: 25.299999237060547\n",
      "mean_trajectory_length: 25.30000114440918\n",
      "steps_n: 11313\n",
      "mean_trajectory_rewards: 32.599998474121094\n",
      "mean_trajectory_length: 32.60000228881836\n",
      "steps_n: 12262\n",
      "mean_trajectory_rewards: 27.399999618530273\n",
      "mean_trajectory_length: 27.399999618530273\n",
      "steps_n: 13080\n",
      "mean_trajectory_rewards: 31.399999618530273\n",
      "mean_trajectory_length: 31.399999618530273\n",
      "steps_n: 14166\n",
      "mean_trajectory_rewards: 33.29999923706055\n",
      "mean_trajectory_length: 33.29999923706055\n",
      "steps_n: 15179\n",
      "mean_trajectory_rewards: 26.299999237060547\n",
      "mean_trajectory_length: 26.30000114440918\n",
      "steps_n: 16102\n",
      "mean_trajectory_rewards: 24.399999618530273\n",
      "mean_trajectory_length: 24.399999618530273\n",
      "steps_n: 17135\n",
      "mean_trajectory_rewards: 19.600000381469727\n",
      "mean_trajectory_length: 19.600000381469727\n",
      "steps_n: 18049\n",
      "mean_trajectory_rewards: 20.5\n",
      "mean_trajectory_length: 20.5\n",
      "steps_n: 19068\n",
      "mean_trajectory_rewards: 21.5\n",
      "mean_trajectory_length: 21.5\n",
      "steps_n: 20039\n",
      "mean_trajectory_rewards: 28.100000381469727\n",
      "mean_trajectory_length: 28.100000381469727\n",
      "steps_n: 21086\n",
      "mean_trajectory_rewards: 31.100000381469727\n",
      "mean_trajectory_length: 31.100000381469727\n",
      "steps_n: 22113\n",
      "mean_trajectory_rewards: 27.899999618530273\n",
      "mean_trajectory_length: 27.899999618530273\n",
      "steps_n: 23302\n",
      "mean_trajectory_rewards: 34.599998474121094\n",
      "mean_trajectory_length: 34.60000228881836\n",
      "steps_n: 24183\n",
      "mean_trajectory_rewards: 37.900001525878906\n",
      "mean_trajectory_length: 37.900001525878906\n",
      "steps_n: 25054\n",
      "mean_trajectory_rewards: 29.799999237060547\n",
      "mean_trajectory_length: 29.80000114440918\n",
      "steps_n: 26027\n",
      "mean_trajectory_rewards: 34.20000076293945\n",
      "mean_trajectory_length: 34.20000076293945\n",
      "steps_n: 27204\n",
      "mean_trajectory_rewards: 28.399999618530273\n",
      "mean_trajectory_length: 28.399999618530273\n",
      "steps_n: 28080\n",
      "mean_trajectory_rewards: 37.5\n",
      "mean_trajectory_length: 37.5\n",
      "steps_n: 29239\n",
      "mean_trajectory_rewards: 27.0\n",
      "mean_trajectory_length: 27.0\n",
      "steps_n: 30126\n",
      "mean_trajectory_rewards: 24.899999618530273\n",
      "mean_trajectory_length: 24.899999618530273\n",
      "steps_n: 31052\n",
      "mean_trajectory_rewards: 33.099998474121094\n",
      "mean_trajectory_length: 33.10000228881836\n",
      "steps_n: 32238\n",
      "mean_trajectory_rewards: 28.399999618530273\n",
      "mean_trajectory_length: 28.399999618530273\n",
      "steps_n: 33194\n",
      "mean_trajectory_rewards: 31.399999618530273\n",
      "mean_trajectory_length: 31.399999618530273\n",
      "steps_n: 34207\n",
      "mean_trajectory_rewards: 31.399999618530273\n",
      "mean_trajectory_length: 31.399999618530273\n",
      "steps_n: 35178\n",
      "mean_trajectory_rewards: 26.600000381469727\n",
      "mean_trajectory_length: 26.600000381469727\n",
      "steps_n: 36258\n",
      "mean_trajectory_rewards: 36.599998474121094\n",
      "mean_trajectory_length: 36.60000228881836\n",
      "steps_n: 37286\n",
      "mean_trajectory_rewards: 36.29999923706055\n",
      "mean_trajectory_length: 36.29999923706055\n",
      "steps_n: 38050\n",
      "mean_trajectory_rewards: 38.29999923706055\n",
      "mean_trajectory_length: 38.29999923706055\n",
      "steps_n: 39234\n",
      "mean_trajectory_rewards: 26.700000762939453\n",
      "mean_trajectory_length: 26.700000762939453\n",
      "steps_n: 40352\n",
      "mean_trajectory_rewards: 42.29999923706055\n",
      "mean_trajectory_length: 42.29999923706055\n",
      "steps_n: 41243\n",
      "mean_trajectory_rewards: 34.20000076293945\n",
      "mean_trajectory_length: 34.20000076293945\n",
      "steps_n: 42317\n",
      "mean_trajectory_rewards: 34.70000076293945\n",
      "mean_trajectory_length: 34.70000076293945\n",
      "steps_n: 43159\n",
      "mean_trajectory_rewards: 43.900001525878906\n",
      "mean_trajectory_length: 43.900001525878906\n",
      "steps_n: 44300\n",
      "mean_trajectory_rewards: 43.5\n",
      "mean_trajectory_length: 43.5\n",
      "steps_n: 45102\n",
      "mean_trajectory_rewards: 41.20000076293945\n",
      "mean_trajectory_length: 41.20000076293945\n",
      "steps_n: 46004\n",
      "mean_trajectory_rewards: 30.899999618530273\n",
      "mean_trajectory_length: 30.899999618530273\n",
      "steps_n: 47184\n",
      "mean_trajectory_rewards: 37.79999923706055\n",
      "mean_trajectory_length: 37.79999923706055\n",
      "steps_n: 48348\n",
      "mean_trajectory_rewards: 53.20000076293945\n",
      "mean_trajectory_length: 53.20000076293945\n",
      "steps_n: 49123\n",
      "mean_trajectory_rewards: 37.0\n",
      "mean_trajectory_length: 37.0\n",
      "steps_n: 50207\n",
      "mean_trajectory_rewards: 35.599998474121094\n",
      "mean_trajectory_length: 35.60000228881836\n",
      "steps_n: 51282\n",
      "mean_trajectory_rewards: 40.20000076293945\n",
      "mean_trajectory_length: 40.20000076293945\n",
      "steps_n: 52036\n",
      "mean_trajectory_rewards: 26.100000381469727\n",
      "mean_trajectory_length: 26.100000381469727\n",
      "steps_n: 53267\n",
      "mean_trajectory_rewards: 47.79999923706055\n",
      "mean_trajectory_length: 47.79999923706055\n",
      "steps_n: 54058\n",
      "mean_trajectory_rewards: 41.599998474121094\n",
      "mean_trajectory_length: 41.60000228881836\n",
      "steps_n: 55362\n",
      "mean_trajectory_rewards: 37.70000076293945\n",
      "mean_trajectory_length: 37.70000076293945\n",
      "steps_n: 56136\n",
      "mean_trajectory_rewards: 40.0\n",
      "mean_trajectory_length: 40.0\n",
      "steps_n: 57334\n",
      "mean_trajectory_rewards: 35.79999923706055\n",
      "mean_trajectory_length: 35.79999923706055\n",
      "steps_n: 58234\n",
      "mean_trajectory_rewards: 38.900001525878906\n",
      "mean_trajectory_length: 38.900001525878906\n",
      "steps_n: 59046\n",
      "mean_trajectory_rewards: 48.20000076293945\n",
      "mean_trajectory_length: 48.20000076293945\n",
      "steps_n: 60364\n",
      "mean_trajectory_rewards: 49.400001525878906\n",
      "mean_trajectory_length: 49.400001525878906\n",
      "steps_n: 61328\n",
      "mean_trajectory_rewards: 53.29999923706055\n",
      "mean_trajectory_length: 53.29999923706055\n",
      "steps_n: 62245\n",
      "mean_trajectory_rewards: 45.900001525878906\n",
      "mean_trajectory_length: 45.900001525878906\n",
      "steps_n: 63122\n",
      "mean_trajectory_rewards: 42.79999923706055\n",
      "mean_trajectory_length: 42.79999923706055\n",
      "steps_n: 64153\n",
      "mean_trajectory_rewards: 43.70000076293945\n",
      "mean_trajectory_length: 43.70000076293945\n",
      "steps_n: 65212\n",
      "mean_trajectory_rewards: 52.400001525878906\n",
      "mean_trajectory_length: 52.400001525878906\n",
      "steps_n: 66015\n",
      "mean_trajectory_rewards: 40.0\n",
      "mean_trajectory_length: 40.0\n",
      "steps_n: 67024\n",
      "mean_trajectory_rewards: 49.099998474121094\n",
      "mean_trajectory_length: 49.10000228881836\n",
      "steps_n: 68324\n",
      "mean_trajectory_rewards: 43.79999923706055\n",
      "mean_trajectory_length: 43.79999923706055\n",
      "steps_n: 69368\n",
      "mean_trajectory_rewards: 62.099998474121094\n",
      "mean_trajectory_length: 62.10000228881836\n",
      "steps_n: 70195\n",
      "mean_trajectory_rewards: 40.5\n",
      "mean_trajectory_length: 40.5\n",
      "steps_n: 71174\n",
      "mean_trajectory_rewards: 44.20000076293945\n",
      "mean_trajectory_length: 44.20000076293945\n",
      "steps_n: 72274\n",
      "mean_trajectory_rewards: 59.900001525878906\n",
      "mean_trajectory_length: 59.900001525878906\n",
      "steps_n: 73417\n",
      "mean_trajectory_rewards: 57.29999923706055\n",
      "mean_trajectory_length: 57.29999923706055\n",
      "steps_n: 74393\n",
      "mean_trajectory_rewards: 41.70000076293945\n",
      "mean_trajectory_length: 41.70000076293945\n",
      "steps_n: 75453\n",
      "mean_trajectory_rewards: 58.20000076293945\n",
      "mean_trajectory_length: 58.20000076293945\n",
      "steps_n: 76101\n",
      "mean_trajectory_rewards: 64.69999694824219\n",
      "mean_trajectory_length: 64.70000457763672\n",
      "steps_n: 77201\n",
      "mean_trajectory_rewards: 52.400001525878906\n",
      "mean_trajectory_length: 52.400001525878906\n",
      "steps_n: 78250\n",
      "mean_trajectory_rewards: 38.29999923706055\n",
      "mean_trajectory_length: 38.29999923706055\n",
      "steps_n: 79181\n",
      "mean_trajectory_rewards: 42.0\n",
      "mean_trajectory_length: 42.0\n",
      "steps_n: 80316\n",
      "mean_trajectory_rewards: 52.70000076293945\n",
      "mean_trajectory_length: 52.70000076293945\n",
      "steps_n: 81599\n",
      "mean_trajectory_rewards: 65.69999694824219\n",
      "mean_trajectory_length: 65.70000457763672\n",
      "steps_n: 82082\n",
      "mean_trajectory_rewards: 48.20000076293945\n",
      "mean_trajectory_length: 48.20000076293945\n",
      "steps_n: 83013\n",
      "mean_trajectory_rewards: 39.70000076293945\n",
      "mean_trajectory_length: 39.70000076293945\n",
      "steps_n: 84122\n",
      "mean_trajectory_rewards: 49.099998474121094\n",
      "mean_trajectory_length: 49.10000228881836\n",
      "steps_n: 85291\n",
      "mean_trajectory_rewards: 55.5\n",
      "mean_trajectory_length: 55.5\n",
      "steps_n: 86560\n",
      "mean_trajectory_rewards: 70.0\n",
      "mean_trajectory_length: 70.0\n",
      "steps_n: 87131\n",
      "mean_trajectory_rewards: 57.0\n",
      "mean_trajectory_length: 57.0\n",
      "steps_n: 88270\n",
      "mean_trajectory_rewards: 62.70000076293945\n",
      "mean_trajectory_length: 62.70000076293945\n",
      "steps_n: 89381\n",
      "mean_trajectory_rewards: 60.29999923706055\n",
      "mean_trajectory_length: 60.29999923706055\n",
      "steps_n: 90116\n",
      "mean_trajectory_rewards: 73.4000015258789\n",
      "mean_trajectory_length: 73.4000015258789\n",
      "steps_n: 91235\n",
      "mean_trajectory_rewards: 52.099998474121094\n",
      "mean_trajectory_length: 52.10000228881836\n",
      "steps_n: 92620\n",
      "mean_trajectory_rewards: 78.30000305175781\n",
      "mean_trajectory_length: 78.30000305175781\n",
      "steps_n: 93092\n",
      "mean_trajectory_rewards: 47.099998474121094\n",
      "mean_trajectory_length: 47.10000228881836\n",
      "steps_n: 94124\n",
      "mean_trajectory_rewards: 36.5\n",
      "mean_trajectory_length: 36.5\n",
      "steps_n: 95520\n",
      "mean_trajectory_rewards: 59.0\n",
      "mean_trajectory_length: 59.0\n",
      "steps_n: 96113\n",
      "mean_trajectory_rewards: 59.20000076293945\n",
      "mean_trajectory_length: 59.20000076293945\n",
      "steps_n: 97285\n",
      "mean_trajectory_rewards: 62.79999923706055\n",
      "mean_trajectory_length: 62.79999923706055\n",
      "steps_n: 98076\n",
      "mean_trajectory_rewards: 79.0\n",
      "mean_trajectory_length: 79.0\n",
      "steps_n: 99192\n",
      "mean_trajectory_rewards: 43.79999923706055\n",
      "mean_trajectory_length: 43.79999923706055\n",
      "steps_n: 100509\n",
      "mean_trajectory_rewards: 73.19999694824219\n",
      "mean_trajectory_length: 73.20000457763672\n"
     ]
    }
   ],
   "source": [
    "a2c(env, actor, critic, actor_optimizer, critic_optimizer, total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(73.25)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, actor, deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "if discrete:\n",
    "    actor = DiscreteStochasticMlpPolicy(obs_dim, action_dim)\n",
    "else:\n",
    "    actor = StochasticMlpPolicy(obs_dim, action_dim)\n",
    "\n",
    "critic = MlpCritic(obs_dim)\n",
    "\n",
    "actor_optimizer = Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = Adam(critic.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_n: 1489\n",
      "mean_trajectory_rewards: 25.850000381469727\n",
      "mean_trajectory_length: 25.850000381469727\n",
      "steps_n: 2226\n",
      "mean_trajectory_rewards: 36.79999923706055\n",
      "mean_trajectory_length: 36.79999923706055\n",
      "steps_n: 3079\n",
      "mean_trajectory_rewards: 42.599998474121094\n",
      "mean_trajectory_length: 42.60000228881836\n",
      "steps_n: 4266\n",
      "mean_trajectory_rewards: 59.29999923706055\n",
      "mean_trajectory_length: 59.29999923706055\n",
      "steps_n: 6534\n",
      "mean_trajectory_rewards: 113.3499984741211\n",
      "mean_trajectory_length: 113.3499984741211\n",
      "steps_n: 10041\n",
      "mean_trajectory_rewards: 175.3000030517578\n",
      "mean_trajectory_length: 175.3000030517578\n",
      "steps_n: 14533\n",
      "mean_trajectory_rewards: 224.5500030517578\n",
      "mean_trajectory_length: 224.5500030517578\n",
      "steps_n: 19944\n",
      "mean_trajectory_rewards: 270.5\n",
      "mean_trajectory_length: 270.5\n",
      "steps_n: 25425\n",
      "mean_trajectory_rewards: 274.0\n",
      "mean_trajectory_length: 274.0\n",
      "steps_n: 32757\n",
      "mean_trajectory_rewards: 366.54998779296875\n",
      "mean_trajectory_length: 366.5500183105469\n",
      "steps_n: 38646\n",
      "mean_trajectory_rewards: 294.3999938964844\n",
      "mean_trajectory_length: 294.3999938964844\n",
      "steps_n: 46698\n",
      "mean_trajectory_rewards: 402.54998779296875\n",
      "mean_trajectory_length: 402.5500183105469\n",
      "steps_n: 55380\n",
      "mean_trajectory_rewards: 434.04998779296875\n",
      "mean_trajectory_length: 434.0500183105469\n",
      "steps_n: 63796\n",
      "mean_trajectory_rewards: 420.75\n",
      "mean_trajectory_length: 420.75\n",
      "steps_n: 73404\n",
      "mean_trajectory_rewards: 480.3500061035156\n",
      "mean_trajectory_length: 480.3500061035156\n",
      "steps_n: 82655\n",
      "mean_trajectory_rewards: 462.5\n",
      "mean_trajectory_length: 462.5\n",
      "steps_n: 92207\n",
      "mean_trajectory_rewards: 477.54998779296875\n",
      "mean_trajectory_length: 477.5500183105469\n",
      "steps_n: 100924\n",
      "mean_trajectory_rewards: 435.79998779296875\n",
      "mean_trajectory_length: 435.8000183105469\n"
     ]
    }
   ],
   "source": [
    "ppo(env, actor, critic, actor_optimizer, critic_optimizer, total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(485.6)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, actor, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1195]], grad_fn=<TanhBackward0>),\n",
       " tensor([[0.0302]], grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.forward(torch.FloatTensor(env.observation_space.sample().reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.6580007], dtype=float32),\n",
       " tensor([[-1.1629]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.predict(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlib.common.buffer import RolloutBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = RolloutBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.collect_rollouts(env, actor, rollout_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rb.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = data[\"observations\"]\n",
    "actions = data[\"actions\"]\n",
    "old_log_probs = data[\"log_probs\"]\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-31.3869],\n",
      "        [-31.8648],\n",
      "        [-32.4259],\n",
      "        [-32.6909],\n",
      "        [-32.5504],\n",
      "        [-32.0703],\n",
      "        [-30.3432],\n",
      "        [-26.3773],\n",
      "        [-20.5584],\n",
      "        [-12.6074]], grad_fn=<SubBackward0>)\n",
      "torch.Size([10, 1]) torch.Size([10, 1])\n",
      "tensor(25.7980, grad_fn=<NegBackward0>) tensor(840.6730, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = {}\n",
    "\n",
    "_, new_log_probs = actor.get_action(observations, action=actions)\n",
    "\n",
    "ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "ratio_clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "\n",
    "values = critic(observations).reshape(ratio.shape)\n",
    "\n",
    "targets = data[\"q_estimations\"].reshape(ratio.shape)\n",
    "advantages = targets.detach() - values\n",
    "# print(targets.shape, values.shape, advantages.shape)\n",
    "# print(old_log_probs.shape, new_log_probs.shape, ratio.shape)\n",
    "\n",
    "if False:\n",
    "    mean = advantages.mean()\n",
    "    std = advantages.std()\n",
    "    advantages = (advantages - mean) / (std + 1e-8)\n",
    "\n",
    "actor_loss_1 = ratio * advantages.detach()\n",
    "actor_loss_2 = ratio_clipped * advantages.detach()\n",
    "\n",
    "print(advantages)\n",
    "print(advantages.shape, ratio.shape)\n",
    "\n",
    "loss[\"actor\"] = -(torch.min(actor_loss_1, actor_loss_2)).mean()\n",
    "loss[\"critic\"] = (advantages**2).mean()\n",
    "\n",
    "print(loss[\"actor\"], loss[\"critic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-945.4736045591187"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, actor, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = torch.zeros((1, 2))\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = torch.ones((1, 2))\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Normal(mu, std)\n",
    "action = dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.log_prob(action).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9676])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.log_prob(action).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "agent = PPO(\"MlpPolicy\", env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03049863, -0.9995348 ,  0.8736682 ], dtype=float32), {})"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, _ = agent.predict(\n",
    "    env.observation_space.sample(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9694774], dtype=float32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.02193137, -0.9997595 , -0.17140453], dtype=float32),\n",
       " -2.4527108869676995,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
