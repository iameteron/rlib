{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import NormalizeObservation, NormalizeReward, RescaleAction\n",
    "from torch.optim import Adam\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from rlib.algorithms.model_free.a2c import a2c\n",
    "from rlib.algorithms.model_free.ppo import ppo\n",
    "from rlib.algorithms.model_free.reinforce import reinforce\n",
    "from rlib.common.evaluation import get_trajectory, validation\n",
    "from rlib.common.policies import (\n",
    "    DiscreteStochasticMlpPolicy,\n",
    "    MlpCritic,\n",
    "    StochasticMlpPolicy,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "discrete = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "min_action, max_action = -1, 1\n",
    "env = RescaleAction(env, min_action, max_action)\n",
    "\n",
    "discrete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1\n"
     ]
    }
   ],
   "source": [
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "if discrete:\n",
    "    action_dim = env.action_space.n\n",
    "else:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(obs_dim, action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if discrete:\n",
    "    policy = DiscreteStochasticMlpPolicy(obs_dim, action_dim)\n",
    "else:\n",
    "    policy = StochasticMlpPolicy(obs_dim, action_dim)\n",
    "\n",
    "optimizer = Adam(policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreinforce\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/rlib/algorithms/model_free/reinforce.py:25\u001b[0m, in \u001b[0;36mreinforce\u001b[0;34m(env, policy, optimizer, total_timesteps, gamma)\u001b[0m\n\u001b[1;32m     22\u001b[0m episode_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m steps_n \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrajectories_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     data \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[1;32m     28\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_estimations\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_returns(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminated\u001b[39m\u001b[38;5;124m\"\u001b[39m], gamma)\n",
      "File \u001b[0;32m/app/rlib/common/buffer.py:92\u001b[0m, in \u001b[0;36mRolloutBuffer.collect_rollouts\u001b[0;34m(self, env, policy, rollout_size, trajectories_n)\u001b[0m\n\u001b[1;32m     89\u001b[0m obs, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     action, log_prob_action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     next_obs, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_transition(\n\u001b[1;32m     96\u001b[0m         obs, action, log_prob_action, reward, terminated, truncated\n\u001b[1;32m     97\u001b[0m     )\n",
      "File \u001b[0;32m/app/rlib/common/policies.py:86\u001b[0m, in \u001b[0;36mStochasticMlpPolicy.predict\u001b[0;34m(self, observation, action, deterministic)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(observation\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_dim))\n\u001b[0;32m---> 86\u001b[0m action, log_prob_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     88\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dim,))\n",
      "File \u001b[0;32m/app/rlib/common/policies.py:50\u001b[0m, in \u001b[0;36mStochasticMlpPolicy.get_action\u001b[0;34m(self, input, action, deterministic, sample_gradients)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_action\u001b[39m(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sample_gradients\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     41\u001b[0m ):\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m        input (torch.Tensor): (B, obs_dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m        log_prob_action: (torch.Tensor): (B, 1)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     mu, log_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Normal(mu, torch\u001b[38;5;241m.\u001b[39mexp(log_std))\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/app/rlib/common/policies.py:32\u001b[0m, in \u001b[0;36mStochasticMlpPolicy.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m        input (torch.Tensor): (B, obs_dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m        log_std: (torch.Tensor): (B, action_dim)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_net\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     mu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu_layer(x))\n\u001b[1;32m     34\u001b[0m     log_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std_layer(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reinforce(env, policy, optimizer, total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1155.7180723675503"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, policy, deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if discrete:\n",
    "    actor = DiscreteStochasticMlpPolicy(obs_dim, action_dim)\n",
    "else:\n",
    "    actor = StochasticMlpPolicy(obs_dim, action_dim)\n",
    "\n",
    "critic = MlpCritic(obs_dim)\n",
    "\n",
    "actor_optimizer = Adam(actor.parameters(), lr=3e-4)\n",
    "critic_optimizer = Adam(critic.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1999, 1])\n",
      "torch.Size([1999, 1])\n",
      "torch.Size([1999, 1])\n",
      "torch.Size([1999, 1])\n",
      "torch.Size([1999, 1])\n",
      "torch.Size([1999, 1])\n",
      "torch.Size([1999, 1])\n",
      "torch.Size([1999, 1])\n",
      "torch.Size([1999, 1])\n",
      "torch.Size([1999, 1])\n",
      "torch.Size([1999, 1])\n",
      "torch.Size([1999, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma2c\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/rlib/algorithms/model_free/a2c.py:27\u001b[0m, in \u001b[0;36ma2c\u001b[0;34m(env, actor, critic, actor_optimizer, critic_optimizer, total_timesteps, trajectories_n, gamma)\u001b[0m\n\u001b[1;32m     25\u001b[0m episode_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m steps_n \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrajectories_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrajectories_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     data \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[1;32m     31\u001b[0m     values \u001b[38;5;241m=\u001b[39m critic(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/app/rlib/common/buffer.py:92\u001b[0m, in \u001b[0;36mRolloutBuffer.collect_rollouts\u001b[0;34m(self, env, policy, rollout_size, trajectories_n)\u001b[0m\n\u001b[1;32m     89\u001b[0m obs, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     action, log_prob_action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     next_obs, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_transition(\n\u001b[1;32m     96\u001b[0m         obs, action, log_prob_action, reward, terminated, truncated\n\u001b[1;32m     97\u001b[0m     )\n",
      "File \u001b[0;32m/app/rlib/common/policies.py:86\u001b[0m, in \u001b[0;36mStochasticMlpPolicy.predict\u001b[0;34m(self, observation, action, deterministic)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(observation\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_dim))\n\u001b[0;32m---> 86\u001b[0m action, log_prob_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     88\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dim,))\n",
      "File \u001b[0;32m/app/rlib/common/policies.py:50\u001b[0m, in \u001b[0;36mStochasticMlpPolicy.get_action\u001b[0;34m(self, input, action, deterministic, sample_gradients)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_action\u001b[39m(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sample_gradients\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     41\u001b[0m ):\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m        input (torch.Tensor): (B, obs_dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m        log_prob_action: (torch.Tensor): (B, 1)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     mu, log_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Normal(mu, torch\u001b[38;5;241m.\u001b[39mexp(log_std))\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/app/rlib/common/policies.py:32\u001b[0m, in \u001b[0;36mStochasticMlpPolicy.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m        input (torch.Tensor): (B, obs_dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m        log_std: (torch.Tensor): (B, action_dim)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_net\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     mu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu_layer(x))\n\u001b[1;32m     34\u001b[0m     log_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std_layer(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1743\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1739\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;66;03m# torchrec tests the code consistency with the following code\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m-> 1743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1744\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a2c(env, actor, critic, actor_optimizer, critic_optimizer, total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.2"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, actor, deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if discrete:\n",
    "    actor = DiscreteStochasticMlpPolicy(obs_dim, action_dim)\n",
    "else:\n",
    "    actor = StochasticMlpPolicy(obs_dim, action_dim)\n",
    "\n",
    "critic = MlpCritic(obs_dim)\n",
    "\n",
    "actor_optimizer = Adam(actor.parameters(), lr=3e-4)\n",
    "critic_optimizer = Adam(critic.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/rlib/algorithms/model_free/ppo.py:65\u001b[0m, in \u001b[0;36mppo\u001b[0;34m(env, actor, critic, actor_optimizer, critic_optimizer, total_timesteps, trajectories_n, epochs_per_episode, batch_size, gamma, lamb, epsilon)\u001b[0m\n\u001b[1;32m     62\u001b[0m loss \u001b[38;5;241m=\u001b[39m ppo_loss(batch, actor, epsilon)\n\u001b[1;32m     64\u001b[0m critic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 65\u001b[0m \u001b[43mloss\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcritic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m critic_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     68\u001b[0m actor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "ppo(env, actor, critic, actor_optimizer, critic_optimizer, total_timesteps=30_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.578979812574406"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, actor, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'states': [array([0.3862416 , 1.4266382 , 0.17692734], dtype=float32),\n",
       "  array([0.34003246, 1.4302872 , 0.2864614 ], dtype=float32),\n",
       "  array([0.23862796, 1.4330773 , 0.4753552 ], dtype=float32),\n",
       "  array([0.10520747, 1.4257239 , 0.58572257], dtype=float32),\n",
       "  array([-0.08905859,  1.3921188 ,  0.803689  ], dtype=float32),\n",
       "  array([-0.30531237,  1.3202944 ,  0.90883225], dtype=float32),\n",
       "  array([-0.54235303,  1.1922886 ,  1.0507729 ], dtype=float32),\n",
       "  array([-0.7781139,  0.9949062,  1.180393 ], dtype=float32),\n",
       "  array([-0.99669313,  0.7005358 ,  1.3819923 ], dtype=float32),\n",
       "  array([-1.1370034 ,  0.34924948,  1.419977  ], dtype=float32),\n",
       "  array([-1.1816864 , -0.01851028,  1.3919792 ], dtype=float32),\n",
       "  array([-1.1373404, -0.3643222,  1.3170457], dtype=float32),\n",
       "  array([-1.0079231, -0.6965706,  1.3451412], dtype=float32),\n",
       "  array([-0.8105636, -0.9765324,  1.2986019], dtype=float32),\n",
       "  array([-0.56508225, -1.1932342 ,  1.2485583 ], dtype=float32),\n",
       "  array([-0.29821622, -1.3397038 ,  1.1708567 ], dtype=float32),\n",
       "  array([-0.06270319, -1.4146732 ,  0.97476846], dtype=float32),\n",
       "  array([ 0.14657149, -1.44561   ,  0.85271376], dtype=float32),\n",
       "  array([ 0.3260995, -1.4472365,  0.7427999], dtype=float32),\n",
       "  array([ 0.44718286, -1.4354445 ,  0.54410005], dtype=float32),\n",
       "  array([ 0.4972508, -1.4274476,  0.3007906], dtype=float32),\n",
       "  array([ 0.51403457, -1.4243146 ,  0.18553665], dtype=float32),\n",
       "  array([ 0.49387166, -1.4279182 ,  0.0568483 ], dtype=float32),\n",
       "  array([ 0.40409693, -1.4405739 , -0.18369709], dtype=float32),\n",
       "  array([ 0.27033612, -1.4487767 , -0.33239523], dtype=float32),\n",
       "  array([ 0.09956104, -1.4409269 , -0.45916614], dtype=float32),\n",
       "  array([-0.11735919, -1.4006506 , -0.6296115 ], dtype=float32),\n",
       "  array([-0.36531177, -1.309024  , -0.779377  ], dtype=float32),\n",
       "  array([-0.6391878 , -1.1373022 , -0.98100436], dtype=float32),\n",
       "  array([-0.8923865, -0.8765284, -1.1176397], dtype=float32),\n",
       "  array([-1.0826503, -0.5347044, -1.2109675], dtype=float32),\n",
       "  array([-1.1752762 , -0.14496198, -1.2415634 ], dtype=float32),\n",
       "  array([-1.1560498 ,  0.26415288, -1.2718525 ], dtype=float32),\n",
       "  array([-1.0180389,  0.6614413, -1.310689 ], dtype=float32),\n",
       "  array([-0.80384636,  0.9677796 , -1.1516377 ], dtype=float32),\n",
       "  array([-0.5380971,  1.1953729, -1.0714027], dtype=float32),\n",
       "  array([-0.27655253,  1.332289  , -0.88474125], dtype=float32),\n",
       "  array([-0.02067045,  1.4072287 , -0.7872298 ], dtype=float32),\n",
       "  array([ 0.20366833,  1.4323819 , -0.6469611 ], dtype=float32),\n",
       "  array([ 0.37046555,  1.4279486 , -0.44469988], dtype=float32),\n",
       "  array([ 0.48172763,  1.4139849 , -0.25689092], dtype=float32),\n",
       "  array([ 0.55157745,  1.4005848 , -0.11624583], dtype=float32),\n",
       "  array([0.5575407 , 1.3992178 , 0.10650683], dtype=float32),\n",
       "  array([0.5094611 , 1.4089425 , 0.29553166], dtype=float32),\n",
       "  array([0.4300506, 1.4213138, 0.4028656], dtype=float32),\n",
       "  array([0.31834376, 1.4310671 , 0.51178557], dtype=float32),\n",
       "  array([0.17409399, 1.4306449 , 0.6219567 ], dtype=float32),\n",
       "  array([-0.03222929,  1.4043739 ,  0.84069186], dtype=float32),\n",
       "  array([-0.26454785,  1.3363686 ,  0.957449  ], dtype=float32),\n",
       "  array([-0.52951753,  1.2003707 ,  1.1484349 ], dtype=float32),\n",
       "  array([-0.7996816,  0.9717319,  1.340034 ], dtype=float32),\n",
       "  array([-1.0328532,  0.6317856,  1.538711 ], dtype=float32),\n",
       "  array([-1.1676668 ,  0.19417715,  1.6939565 ], dtype=float32),\n",
       "  array([-1.1571511 , -0.27477878,  1.7314973 ], dtype=float32),\n",
       "  array([-1.0054952 , -0.70103055,  1.6750807 ], dtype=float32),\n",
       "  array([-0.7370928, -1.0520216,  1.6410394], dtype=float32),\n",
       "  array([-0.39105242, -1.2966232 ,  1.5812875 ], dtype=float32),\n",
       "  array([-0.02931252, -1.421363  ,  1.4410913 ], dtype=float32),\n",
       "  array([ 0.30758435, -1.4478153 ,  1.2874197 ], dtype=float32),\n",
       "  array([ 0.6006237, -1.4050522,  1.143194 ], dtype=float32),\n",
       "  array([ 0.81589377, -1.3315554 ,  0.90651464], dtype=float32),\n",
       "  array([ 0.9932345, -1.2394849,  0.8110996], dtype=float32),\n",
       "  array([ 1.1125106 , -1.1579846 ,  0.62102777], dtype=float32),\n",
       "  array([ 1.1881618, -1.0962898,  0.4605457], dtype=float32),\n",
       "  array([ 1.2354215 , -1.0530686 ,  0.34567475], dtype=float32),\n",
       "  array([ 1.2603871 , -1.0285687 ,  0.24633288], dtype=float32),\n",
       "  array([ 1.2692934 , -1.0194852 ,  0.17029361], dtype=float32),\n",
       "  array([ 1.2637851 , -1.0250013 ,  0.10021392], dtype=float32),\n",
       "  array([ 1.2278934 , -1.0599201 , -0.04423798], dtype=float32),\n",
       "  array([ 1.1467835 , -1.1306686 , -0.24106762], dtype=float32),\n",
       "  array([ 1.0287554 , -1.2165262 , -0.37244004], dtype=float32),\n",
       "  array([ 0.87470096, -1.3039111 , -0.47955063], dtype=float32),\n",
       "  array([ 0.6745156, -1.3835042, -0.6114413], dtype=float32),\n",
       "  array([ 0.40713477, -1.4396051 , -0.8104929 ], dtype=float32),\n",
       "  array([ 0.10226759, -1.4406495 , -0.9197429 ], dtype=float32),\n",
       "  array([-0.25184557, -1.3570137 , -1.1227818 ], dtype=float32),\n",
       "  array([-0.6110617, -1.1589073, -1.2814591], dtype=float32),\n",
       "  array([-0.9370474, -0.8137315, -1.5020219], dtype=float32),\n",
       "  array([-1.1428972 , -0.34276822, -1.6345023 ], dtype=float32),\n",
       "  array([-1.1665224 ,  0.20481348, -1.7508041 ], dtype=float32),\n",
       "  array([-0.98151386,  0.72788364, -1.7753432 ], dtype=float32),\n",
       "  array([-0.6198027,  1.1370074, -1.7487975], dtype=float32),\n",
       "  array([-0.1899729,  1.3634137, -1.5430692], dtype=float32),\n",
       "  array([ 0.24892604,  1.4329648 , -1.401096  ], dtype=float32),\n",
       "  array([ 0.62165904,  1.3834623 , -1.1644341 ], dtype=float32),\n",
       "  array([ 0.94231325,  1.2531258 , -1.0603453 ], dtype=float32),\n",
       "  array([ 1.2036717 ,  1.0665265 , -0.97289866], dtype=float32),\n",
       "  array([ 1.3807913,  0.8754802, -0.7635193], dtype=float32),\n",
       "  array([ 1.512127  ,  0.67173976, -0.70057124], dtype=float32),\n",
       "  array([ 1.5955427 ,  0.48342234, -0.575318  ], dtype=float32),\n",
       "  array([ 1.6435565 ,  0.32066244, -0.4510938 ], dtype=float32),\n",
       "  array([ 1.668033  ,  0.18700717, -0.33555838], dtype=float32),\n",
       "  array([ 1.6803964 ,  0.03732428, -0.3842557 ], dtype=float32),\n",
       "  array([ 1.6782721 , -0.09693348, -0.3299305 ], dtype=float32),\n",
       "  array([ 1.659695  , -0.25503418, -0.41488567], dtype=float32),\n",
       "  array([ 1.6150194, -0.4396732, -0.5200281], dtype=float32),\n",
       "  array([ 1.5366642, -0.637782 , -0.5991638], dtype=float32),\n",
       "  array([ 1.428994  , -0.8237489 , -0.60600543], dtype=float32),\n",
       "  array([ 1.2781223, -1.0093788, -0.6899927], dtype=float32),\n",
       "  array([ 1.0647918, -1.1916966, -0.833087 ], dtype=float32),\n",
       "  array([ 0.78621525, -1.3431537 , -0.959748  ], dtype=float32),\n",
       "  array([ 0.43338755, -1.4363718 , -1.1259207 ], dtype=float32),\n",
       "  array([-0.0052439, -1.4254006, -1.3818972], dtype=float32),\n",
       "  array([-0.46296048, -1.257447  , -1.5499483 ], dtype=float32),\n",
       "  array([-0.8731029, -0.9021513, -1.7378991], dtype=float32),\n",
       "  array([-1.1306665 , -0.39176244, -1.8344191 ], dtype=float32),\n",
       "  array([-1.170391  ,  0.17928413, -1.8343086 ], dtype=float32),\n",
       "  array([-0.9907119 ,  0.71270156, -1.8021866 ], dtype=float32),\n",
       "  array([-0.6593983,  1.1058052, -1.6365664], dtype=float32),\n",
       "  array([-0.24202693,  1.3456258 , -1.52661   ], dtype=float32),\n",
       "  array([ 0.17595726,  1.431141  , -1.3388593 ], dtype=float32),\n",
       "  array([ 0.53975147,  1.4029849 , -1.1254505 ], dtype=float32),\n",
       "  array([ 0.8215418 ,  1.3127725 , -0.88686365], dtype=float32),\n",
       "  array([ 1.0283275 ,  1.2008177 , -0.67733794], dtype=float32),\n",
       "  array([ 1.1747247,  1.0912417, -0.497478 ], dtype=float32),\n",
       "  array([ 1.2928392 ,  0.97820425, -0.43072253], dtype=float32),\n",
       "  array([ 1.379974  ,  0.87547135, -0.33199   ], dtype=float32),\n",
       "  array([ 1.4509803 ,  0.77466923, -0.2927792 ], dtype=float32),\n",
       "  array([ 1.4885912 ,  0.7126759 , -0.11928271], dtype=float32),\n",
       "  array([ 1.5205847 ,  0.65366334, -0.10085604], dtype=float32),\n",
       "  array([ 1.5373065e+00,  6.1988181e-01, -4.4765670e-04], dtype=float32),\n",
       "  array([1.5398237 , 0.6144606 , 0.10768864], dtype=float32),\n",
       "  array([1.532836  , 0.6287767 , 0.18236935], dtype=float32),\n",
       "  array([1.5231801 , 0.6479835 , 0.20135054], dtype=float32),\n",
       "  array([1.5013244 , 0.6891902 , 0.28710774], dtype=float32),\n",
       "  array([1.4662533 , 0.74940157, 0.36572522], dtype=float32),\n",
       "  array([1.4244963, 0.8134235, 0.3888466], dtype=float32),\n",
       "  array([1.371194  , 0.8856964 , 0.43457648], dtype=float32),\n",
       "  array([1.2802129 , 0.99055463, 0.6022701 ], dtype=float32),\n",
       "  array([1.1610826 , 1.1017857 , 0.68526965], dtype=float32),\n",
       "  array([1.0134739, 1.2095187, 0.753391 ], dtype=float32),\n",
       "  array([0.81943136, 1.3129205 , 0.88130444], dtype=float32),\n",
       "  array([0.5431733, 1.4015939, 1.1235586], dtype=float32),\n",
       "  array([0.2047965, 1.4317495, 1.2949896], dtype=float32),\n",
       "  array([-0.20170614,  1.3592868 ,  1.5477079 ], dtype=float32),\n",
       "  array([-0.61989695,  1.1369957 ,  1.7560886 ], dtype=float32),\n",
       "  array([-0.9624051,  0.760014 ,  1.8764294], dtype=float32),\n",
       "  array([-1.1572431 ,  0.26346725,  1.9560659 ], dtype=float32),\n",
       "  array([-1.1540248 , -0.29637727,  2.0459988 ], dtype=float32),\n",
       "  array([-0.93299264, -0.82166743,  2.082651  ], dtype=float32),\n",
       "  array([-0.5558489, -1.200031 ,  1.9628785], dtype=float32),\n",
       "  array([-0.10220583, -1.4050483 ,  1.8400713 ], dtype=float32),\n",
       "  array([ 0.32047695, -1.4470874 ,  1.5881141 ], dtype=float32),\n",
       "  array([ 0.66560286, -1.3863467 ,  1.3308145 ], dtype=float32),\n",
       "  array([ 0.95816684, -1.2593068 ,  1.2213163 ], dtype=float32),\n",
       "  array([ 1.196528 , -1.0875207,  1.1335839], dtype=float32),\n",
       "  array([ 1.3703607 , -0.9028217 ,  0.99456143], dtype=float32),\n",
       "  array([ 1.4912481 , -0.72279394,  0.86776716], dtype=float32),\n",
       "  array([ 1.5702974 , -0.56056434,  0.74286485], dtype=float32),\n",
       "  array([ 1.6287719 , -0.38878435,  0.74588346], dtype=float32),\n",
       "  array([ 1.6663123 , -0.20369339,  0.77083   ], dtype=float32),\n",
       "  array([ 1.6792731 , -0.03269492,  0.71138537], dtype=float32),\n",
       "  array([1.6730261 , 0.12770699, 0.6739299 ], dtype=float32),\n",
       "  array([1.6516364, 0.2738849, 0.6303542], dtype=float32),\n",
       "  array([1.6186868 , 0.40653798, 0.5927674 ], dtype=float32),\n",
       "  array([1.5756601 , 0.5298356 , 0.57211566], dtype=float32),\n",
       "  array([1.5179814 , 0.6560043 , 0.60005754], dtype=float32),\n",
       "  array([1.4467295 , 0.7787598 , 0.61125284], dtype=float32),\n",
       "  array([1.3588887 , 0.89999396, 0.63812   ], dtype=float32),\n",
       "  array([1.2229778 , 1.04635   , 0.80965835], dtype=float32),\n",
       "  array([1.0529838, 1.1828948, 0.8730974], dtype=float32),\n",
       "  array([0.82322925, 1.3109208 , 1.0283171 ], dtype=float32),\n",
       "  array([0.5049952, 1.409014 , 1.2702463], dtype=float32),\n",
       "  array([0.12365495, 1.4269949 , 1.4392092 ], dtype=float32),\n",
       "  array([-0.30979764,  1.3185378 ,  1.6638042 ], dtype=float32),\n",
       "  array([-0.7099012,  1.0622358,  1.7595254], dtype=float32),\n",
       "  array([-1.033348  ,  0.63371515,  1.9696289 ], dtype=float32),\n",
       "  array([-1.179228  ,  0.10037585,  2.021462  ], dtype=float32),\n",
       "  array([-1.1151781 , -0.44755867,  2.0161655 ], dtype=float32),\n",
       "  array([-0.86562794, -0.91346514,  1.9384582 ], dtype=float32),\n",
       "  array([-0.5130745, -1.2285614,  1.7492915], dtype=float32),\n",
       "  array([-0.10124934, -1.4055244 ,  1.6670259 ], dtype=float32),\n",
       "  array([ 0.29040605, -1.4483634 ,  1.4805139 ], dtype=float32),\n",
       "  array([ 0.6240318, -1.3982984,  1.2847433], dtype=float32),\n",
       "  array([ 0.88548523, -1.2980247 ,  1.086465  ], dtype=float32),\n",
       "  array([ 1.073285  , -1.1851023 ,  0.87656003], dtype=float32),\n",
       "  array([ 1.2255688, -1.0602086,  0.7998808], dtype=float32),\n",
       "  array([ 1.3245064 , -0.9572072 ,  0.61422974], dtype=float32),\n",
       "  array([ 1.3950869 , -0.8692457 ,  0.51132673], dtype=float32),\n",
       "  array([ 1.4427835, -0.8006792,  0.4113198], dtype=float32),\n",
       "  array([ 1.4820296 , -0.7370574 ,  0.38130155], dtype=float32),\n",
       "  array([ 1.5073339 , -0.69164467,  0.30360374], dtype=float32),\n",
       "  array([ 1.5206158 , -0.66608787,  0.2245392 ], dtype=float32),\n",
       "  array([ 1.5314085 , -0.64432615,  0.2091508 ], dtype=float32),\n",
       "  array([ 1.5286598 , -0.64976203,  0.10560074], dtype=float32),\n",
       "  array([ 1.5161135 , -0.6744007 ,  0.03211901], dtype=float32),\n",
       "  array([ 1.4908912 , -0.7208305 , -0.05377267], dtype=float32),\n",
       "  array([ 1.4380434 , -0.8069676 , -0.21831894], dtype=float32),\n",
       "  array([ 1.374231 , -0.89575  , -0.2467463], dtype=float32),\n",
       "  array([ 1.2663801, -1.0192213, -0.4336245], dtype=float32),\n",
       "  array([ 1.1325957 , -1.1396809 , -0.48914617], dtype=float32),\n",
       "  array([ 0.92967254, -1.2741178 , -0.7071646 ], dtype=float32),\n",
       "  array([ 0.6683423, -1.3846768, -0.8467321], dtype=float32),\n",
       "  array([ 0.3530172, -1.4444892, -0.9756238], dtype=float32),\n",
       "  array([-0.00246617, -1.426285  , -1.0965904 ], dtype=float32),\n",
       "  array([-0.37906304, -1.303041  , -1.2349637 ], dtype=float32),\n",
       "  array([-0.7443637, -1.046366 , -1.40633  ], dtype=float32),\n",
       "  array([-1.0399383, -0.6370883, -1.6048834], dtype=float32),\n",
       "  array([-1.1804471 , -0.10509813, -1.758695  ], dtype=float32),\n",
       "  array([-1.1054293 ,  0.46235016, -1.8347998 ], dtype=float32)],\n",
       " 'actions': [array([-1.3377596], dtype=float32),\n",
       "  array([0.07202791], dtype=float32),\n",
       "  array([-1.3314928], dtype=float32),\n",
       "  array([0.47488698], dtype=float32),\n",
       "  array([-1.5832434], dtype=float32),\n",
       "  array([-0.3753684], dtype=float32),\n",
       "  array([-0.3206934], dtype=float32),\n",
       "  array([2.4106128], dtype=float32),\n",
       "  array([-0.7126732], dtype=float32),\n",
       "  array([-1.3258647], dtype=float32),\n",
       "  array([-1.9845697], dtype=float32),\n",
       "  array([1.7217185], dtype=float32),\n",
       "  array([0.56202185], dtype=float32),\n",
       "  array([1.3090215], dtype=float32),\n",
       "  array([3.1535738], dtype=float32),\n",
       "  array([-0.35488907], dtype=float32),\n",
       "  array([0.7812891], dtype=float32),\n",
       "  array([1.8154699], dtype=float32),\n",
       "  array([-0.20373318], dtype=float32),\n",
       "  array([-0.8303613], dtype=float32),\n",
       "  array([0.896103], dtype=float32),\n",
       "  array([0.70820236], dtype=float32),\n",
       "  array([-0.8055098], dtype=float32),\n",
       "  array([0.4646731], dtype=float32),\n",
       "  array([0.77696514], dtype=float32),\n",
       "  array([0.16993451], dtype=float32),\n",
       "  array([0.38111895], dtype=float32),\n",
       "  array([-0.4825975], dtype=float32),\n",
       "  array([0.10258162], dtype=float32),\n",
       "  array([0.23859054], dtype=float32),\n",
       "  array([0.49797732], dtype=float32),\n",
       "  array([-0.17404255], dtype=float32),\n",
       "  array([-1.3359234], dtype=float32),\n",
       "  array([1.4030755], dtype=float32),\n",
       "  array([-0.6024472], dtype=float32),\n",
       "  array([0.44902638], dtype=float32),\n",
       "  array([-1.1751837], dtype=float32),\n",
       "  array([-0.5490252], dtype=float32),\n",
       "  array([0.24969628], dtype=float32),\n",
       "  array([0.0609156], dtype=float32),\n",
       "  array([-0.5558512], dtype=float32),\n",
       "  array([0.58301896], dtype=float32),\n",
       "  array([0.12699166], dtype=float32),\n",
       "  array([-1.0857698], dtype=float32),\n",
       "  array([-1.4181913], dtype=float32),\n",
       "  array([-2.308113], dtype=float32),\n",
       "  array([0.47584292], dtype=float32),\n",
       "  array([-0.8642386], dtype=float32),\n",
       "  array([0.2624062], dtype=float32),\n",
       "  array([0.5068556], dtype=float32),\n",
       "  array([1.518332], dtype=float32),\n",
       "  array([2.42583], dtype=float32),\n",
       "  array([0.16001728], dtype=float32),\n",
       "  array([-0.30313537], dtype=float32),\n",
       "  array([0.74045086], dtype=float32),\n",
       "  array([1.292781], dtype=float32),\n",
       "  array([0.3310488], dtype=float32),\n",
       "  array([0.36424327], dtype=float32),\n",
       "  array([0.5384509], dtype=float32),\n",
       "  array([-0.79228324], dtype=float32),\n",
       "  array([1.6905614], dtype=float32),\n",
       "  array([-0.4461437], dtype=float32),\n",
       "  array([-0.1854153], dtype=float32),\n",
       "  array([0.3274205], dtype=float32),\n",
       "  array([0.46350402], dtype=float32),\n",
       "  array([0.7377095], dtype=float32),\n",
       "  array([0.8029828], dtype=float32),\n",
       "  array([-0.19804883], dtype=float32),\n",
       "  array([-0.84916365], dtype=float32),\n",
       "  array([0.1631347], dtype=float32),\n",
       "  array([0.64185834], dtype=float32),\n",
       "  array([0.45682728], dtype=float32),\n",
       "  array([-0.31763333], dtype=float32),\n",
       "  array([1.3296726], dtype=float32),\n",
       "  array([-0.27264374], dtype=float32),\n",
       "  array([0.1849314], dtype=float32),\n",
       "  array([-1.2971717], dtype=float32),\n",
       "  array([-0.40232342], dtype=float32),\n",
       "  array([-1.329421], dtype=float32),\n",
       "  array([-0.7033535], dtype=float32),\n",
       "  array([-0.9168018], dtype=float32),\n",
       "  array([0.8086331], dtype=float32),\n",
       "  array([-0.45053503], dtype=float32),\n",
       "  array([0.7158127], dtype=float32),\n",
       "  array([-1.1686935], dtype=float32),\n",
       "  array([-1.0511247], dtype=float32),\n",
       "  array([0.98101455], dtype=float32),\n",
       "  array([-0.67752874], dtype=float32),\n",
       "  array([0.5228056], dtype=float32),\n",
       "  array([0.8356332], dtype=float32),\n",
       "  array([1.3938838], dtype=float32),\n",
       "  array([-1.0277781], dtype=float32),\n",
       "  array([0.65981036], dtype=float32),\n",
       "  array([-1.550473], dtype=float32),\n",
       "  array([-2.6413627], dtype=float32),\n",
       "  array([-0.32619855], dtype=float32),\n",
       "  array([1.4035292], dtype=float32),\n",
       "  array([0.27435634], dtype=float32),\n",
       "  array([-0.20675719], dtype=float32),\n",
       "  array([0.33290625], dtype=float32),\n",
       "  array([0.05875257], dtype=float32),\n",
       "  array([-1.063343], dtype=float32),\n",
       "  array([0.17560351], dtype=float32),\n",
       "  array([-0.386546], dtype=float32),\n",
       "  array([-1.0479878], dtype=float32),\n",
       "  array([-1.071032], dtype=float32),\n",
       "  array([0.11093467], dtype=float32),\n",
       "  array([1.3277113], dtype=float32),\n",
       "  array([-0.43850195], dtype=float32),\n",
       "  array([0.20289181], dtype=float32),\n",
       "  array([0.40338573], dtype=float32),\n",
       "  array([0.79455215], dtype=float32),\n",
       "  array([0.55611825], dtype=float32),\n",
       "  array([0.34716898], dtype=float32),\n",
       "  array([-2.820949], dtype=float32),\n",
       "  array([-0.3692617], dtype=float32),\n",
       "  array([-1.0933597], dtype=float32),\n",
       "  array([1.4409552], dtype=float32),\n",
       "  array([-1.9338485], dtype=float32),\n",
       "  array([0.21658003], dtype=float32),\n",
       "  array([0.38020745], dtype=float32),\n",
       "  array([-0.06508011], dtype=float32),\n",
       "  array([-0.8469129], dtype=float32),\n",
       "  array([0.02723035], dtype=float32),\n",
       "  array([-0.14131862], dtype=float32),\n",
       "  array([-1.6678641], dtype=float32),\n",
       "  array([-0.8038787], dtype=float32),\n",
       "  array([0.7281204], dtype=float32),\n",
       "  array([-0.60477895], dtype=float32),\n",
       "  array([-1.132596], dtype=float32),\n",
       "  array([-0.37442967], dtype=float32),\n",
       "  array([1.1963565], dtype=float32),\n",
       "  array([-0.11619332], dtype=float32),\n",
       "  array([0.93635094], dtype=float32),\n",
       "  array([0.45990857], dtype=float32),\n",
       "  array([-0.3505304], dtype=float32),\n",
       "  array([-0.24932244], dtype=float32),\n",
       "  array([0.752519], dtype=float32),\n",
       "  array([1.3358245], dtype=float32),\n",
       "  array([-0.21484652], dtype=float32),\n",
       "  array([0.40018433], dtype=float32),\n",
       "  array([-1.2674795], dtype=float32),\n",
       "  array([-1.780395], dtype=float32),\n",
       "  array([0.9036562], dtype=float32),\n",
       "  array([0.9790597], dtype=float32),\n",
       "  array([-0.01632586], dtype=float32),\n",
       "  array([-0.17065978], dtype=float32),\n",
       "  array([-0.45738894], dtype=float32),\n",
       "  array([1.2196102], dtype=float32),\n",
       "  array([1.8614177], dtype=float32),\n",
       "  array([-0.46835577], dtype=float32),\n",
       "  array([-0.466191], dtype=float32),\n",
       "  array([-0.82773954], dtype=float32),\n",
       "  array([-1.3149108], dtype=float32),\n",
       "  array([-1.7882227], dtype=float32),\n",
       "  array([-0.55342746], dtype=float32),\n",
       "  array([-1.9431428], dtype=float32),\n",
       "  array([-1.3504385], dtype=float32),\n",
       "  array([0.75609785], dtype=float32),\n",
       "  array([-0.9671874], dtype=float32),\n",
       "  array([0.04342946], dtype=float32),\n",
       "  array([1.2653339], dtype=float32),\n",
       "  array([-0.16185567], dtype=float32),\n",
       "  array([0.5634021], dtype=float32),\n",
       "  array([-1.3123662], dtype=float32),\n",
       "  array([1.1326314], dtype=float32),\n",
       "  array([-0.40770403], dtype=float32),\n",
       "  array([-0.2588626], dtype=float32),\n",
       "  array([-0.2926914], dtype=float32),\n",
       "  array([-1.1257377], dtype=float32),\n",
       "  array([1.385506], dtype=float32),\n",
       "  array([-0.11050662], dtype=float32),\n",
       "  array([-0.16223931], dtype=float32),\n",
       "  array([-0.2833291], dtype=float32),\n",
       "  array([-0.615489], dtype=float32),\n",
       "  array([1.9317195], dtype=float32),\n",
       "  array([-0.6983844], dtype=float32),\n",
       "  array([0.24797653], dtype=float32),\n",
       "  array([0.13473585], dtype=float32),\n",
       "  array([0.9673608], dtype=float32),\n",
       "  array([0.20872754], dtype=float32),\n",
       "  array([0.11136286], dtype=float32),\n",
       "  array([0.93275976], dtype=float32),\n",
       "  array([-0.30358618], dtype=float32),\n",
       "  array([0.11465691], dtype=float32),\n",
       "  array([-0.01128183], dtype=float32),\n",
       "  array([-1.2152879], dtype=float32),\n",
       "  array([2.0561006], dtype=float32),\n",
       "  array([-1.2279408], dtype=float32),\n",
       "  array([2.1975126], dtype=float32),\n",
       "  array([-1.7697903], dtype=float32),\n",
       "  array([0.29976046], dtype=float32),\n",
       "  array([0.63674897], dtype=float32),\n",
       "  array([0.84827954], dtype=float32),\n",
       "  array([0.58003545], dtype=float32),\n",
       "  array([-0.08247003], dtype=float32),\n",
       "  array([-0.8977274], dtype=float32),\n",
       "  array([-1.4591098], dtype=float32),\n",
       "  array([-0.9304475], dtype=float32),\n",
       "  array([-0.83403736], dtype=float32)],\n",
       " 'rewards': [-0.022507013036999625,\n",
       "  -0.023852283989578737,\n",
       "  -0.027787336824125937,\n",
       "  -0.03240741896587846,\n",
       "  -0.04161955864533945,\n",
       "  -0.05041121753056554,\n",
       "  -0.06255730144029731,\n",
       "  -0.07700062551750961,\n",
       "  -0.09812638522763095,\n",
       "  -0.11479207709562263,\n",
       "  -0.1287018813393852,\n",
       "  -0.11049048831391407,\n",
       "  -0.09745951555202204,\n",
       "  -0.08316030052249886,\n",
       "  -0.07040322936612525,\n",
       "  -0.05840853335213058,\n",
       "  -0.04538939563179882,\n",
       "  -0.03688152178126504,\n",
       "  -0.03022833739384853,\n",
       "  -0.02415500613056144,\n",
       "  -0.02066441447788627,\n",
       "  -0.019851654752352708,\n",
       "  -0.020294245343646555,\n",
       "  -0.023714770749094234,\n",
       "  -0.028607073340674118,\n",
       "  -0.034835381172475643,\n",
       "  -0.04433521405698463,\n",
       "  -0.055928863127205304,\n",
       "  -0.07278657848894726,\n",
       "  -0.09068132173227364,\n",
       "  -0.10961414855294853,\n",
       "  -0.12774564255863263,\n",
       "  -0.12339127551522923,\n",
       "  -0.10833072765468107,\n",
       "  -0.0873150254236326,\n",
       "  -0.0724398893520321,\n",
       "  -0.05668255088084043,\n",
       "  -0.04630931036147176,\n",
       "  -0.03676758942348151,\n",
       "  -0.02838552631952702,\n",
       "  -0.02298331189864141,\n",
       "  -0.02005487659679353,\n",
       "  -0.018912342094296593,\n",
       "  -0.02039810446999002,\n",
       "  -0.02285327266383736,\n",
       "  -0.02648110389984405,\n",
       "  -0.03137795926801452,\n",
       "  -0.041010749098014505,\n",
       "  -0.050536778589892986,\n",
       "  -0.06536622014232275,\n",
       "  -0.0843001771838275,\n",
       "  -0.10828709897792406,\n",
       "  -0.13488021366758599,\n",
       "  -0.13416821063050652,\n",
       "  -0.11294251111465298,\n",
       "  -0.09536223768046219,\n",
       "  -0.07894537146172952,\n",
       "  -0.061797532836838115,\n",
       "  -0.04724618054827158,\n",
       "  -0.03578519561114009,\n",
       "  -0.02443988604364296,\n",
       "  -0.018864847929090493,\n",
       "  -0.01299490233470968,\n",
       "  -0.009472119789200067,\n",
       "  -0.007621597177526408,\n",
       "  -0.006657482358858824,\n",
       "  -0.006306141642188112,\n",
       "  -0.006348205160010963,\n",
       "  -0.0074416648401484055,\n",
       "  -0.010554814952564412,\n",
       "  -0.014505220345217713,\n",
       "  -0.019221162535186007,\n",
       "  -0.025962304386977886,\n",
       "  -0.03709386219679625,\n",
       "  -0.04766574958927768,\n",
       "  -0.06511722214786005,\n",
       "  -0.0846548978266594,\n",
       "  -0.11241414057110592,\n",
       "  -0.14034768849593915,\n",
       "  -0.15287592859831486,\n",
       "  -0.1320950379447481,\n",
       "  -0.11133122159389507,\n",
       "  -0.08439378540078014,\n",
       "  -0.06552733111454828,\n",
       "  -0.04628958201141008,\n",
       "  -0.03593972874193863,\n",
       "  -0.028097358476839945,\n",
       "  -0.018117798918962726,\n",
       "  -0.014251905858892307,\n",
       "  -0.009743245310448203,\n",
       "  -0.006324405191126091,\n",
       "  -0.003905761198923134,\n",
       "  -0.004516577874945006,\n",
       "  -0.003666861960845816,\n",
       "  -0.0053900592899043245,\n",
       "  -0.008147013610831961,\n",
       "  -0.011204586915432772,\n",
       "  -0.012964253609012507,\n",
       "  -0.017552073911883227,\n",
       "  -0.02541710170110261,\n",
       "  -0.03477585656504445,\n",
       "  -0.0484107701049768,\n",
       "  -0.07062345957154034,\n",
       "  -0.09335939981042586,\n",
       "  -0.12271388950831165,\n",
       "  -0.15109914822874396,\n",
       "  -0.1595494712337423,\n",
       "  -0.13460869390820246,\n",
       "  -0.1059974711869597,\n",
       "  -0.08500901186040591,\n",
       "  -0.0640394070506005,\n",
       "  -0.04622840319944761,\n",
       "  -0.031544315627985826,\n",
       "  -0.021346991617100637,\n",
       "  -0.014519140282123928,\n",
       "  -0.011222329720237558,\n",
       "  -0.00816502428342948,\n",
       "  -0.0064803342522030145,\n",
       "  -0.003920850713712503,\n",
       "  -0.0032490791636506353,\n",
       "  -0.0023865778231939934,\n",
       "  -0.0020658837069186735,\n",
       "  -0.0022417452763687905,\n",
       "  -0.0023969410417742955,\n",
       "  -0.003066070273617153,\n",
       "  -0.004170047666902764,\n",
       "  -0.0049891502975541,\n",
       "  -0.006251303181431353,\n",
       "  -0.009919693383440165,\n",
       "  -0.013341076311289984,\n",
       "  -0.017166899902539806,\n",
       "  -0.02365823215795443,\n",
       "  -0.0361635848515474,\n",
       "  -0.04982998965067905,\n",
       "  -0.0714357236667643,\n",
       "  -0.09640640351588343,\n",
       "  -0.12103093680178069,\n",
       "  -0.14720585410119733,\n",
       "  -0.15231570258549745,\n",
       "  -0.13218374747638956,\n",
       "  -0.10623605986659097,\n",
       "  -0.0844496291858942,\n",
       "  -0.0604841146197349,\n",
       "  -0.0416459405753885,\n",
       "  -0.03198433304748505,\n",
       "  -0.024766761831383807,\n",
       "  -0.017534153304196217,\n",
       "  -0.012193457251619326,\n",
       "  -0.008139423751720994,\n",
       "  -0.007343635423585893,\n",
       "  -0.007305526504490212,\n",
       "  -0.005865479125648469,\n",
       "  -0.005247309200423457,\n",
       "  -0.004784063423963004,\n",
       "  -0.004638347897182776,\n",
       "  -0.004918517013960183,\n",
       "  -0.0062441210240843465,\n",
       "  -0.007503937901838,\n",
       "  -0.009298622700182278,\n",
       "  -0.014982658576641727,\n",
       "  -0.019313616609958607,\n",
       "  -0.0277774767649373,\n",
       "  -0.04234401351619025,\n",
       "  -0.05788269746895428,\n",
       "  -0.08061529028432143,\n",
       "  -0.10049987732874903,\n",
       "  -0.13230947706762336,\n",
       "  -0.1585311752591689,\n",
       "  -0.14395484260152414,\n",
       "  -0.1188923015406365,\n",
       "  -0.09207310892480208,\n",
       "  -0.07477815631422052,\n",
       "  -0.05595433847545909,\n",
       "  -0.040585492876525596,\n",
       "  -0.028517169353136675,\n",
       "  -0.019124920252385928,\n",
       "  -0.014716759483023175,\n",
       "  -0.009421453475423013,\n",
       "  -0.006777356409163864,\n",
       "  -0.004932331829991604,\n",
       "  -0.004020459894555421,\n",
       "  -0.0030606957287636536,\n",
       "  -0.002522134861157226,\n",
       "  -0.002279842336630769,\n",
       "  -0.002206123751985557,\n",
       "  -0.0025370318561402315,\n",
       "  -0.0033604237697478323,\n",
       "  -0.005642511433690738,\n",
       "  -0.0069744613249176374,\n",
       "  -0.011663031500550012,\n",
       "  -0.014958868120595822,\n",
       "  -0.023804810156494664,\n",
       "  -0.03294171178581555,\n",
       "  -0.04395750437869172,\n",
       "  -0.05710448709624474,\n",
       "  -0.07401572018072972,\n",
       "  -0.09660769611715267,\n",
       "  -0.12602955379514252,\n",
       "  -0.15841484992431928,\n",
       "  -0.1473895349374905],\n",
       " 'terminated': [False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False],\n",
       " 'truncated': [False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_trajectory(env, actor, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./models/cartpole_stoc_expert\", \"wb\") as file:\n",
    "#     pickle.dump(actor, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1], requires_grad=True, dtype=torch.float32)\n",
    "b = torch.tensor([2], requires_grad=True, dtype=torch.float32)\n",
    "c = torch.tensor([3], requires_grad=True, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = a * b + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1195]], grad_fn=<TanhBackward0>),\n",
       " tensor([[0.0302]], grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.forward(torch.FloatTensor(env.observation_space.sample().reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.6580007], dtype=float32),\n",
       " tensor([[-1.1629]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.predict(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlib.common.buffer import RolloutBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = RolloutBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.collect_rollouts(env, actor, rollout_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.collect_rollouts(env, policy, trajectories_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rb.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3]) torch.Size([500, 1]) torch.Size([500, 1]) torch.Size([500, 1]) torch.Size([1]) torch.Size([500, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    data[\"observations\"].shape,\n",
    "    data[\"actions\"].shape,\n",
    "    data[\"rewards\"].shape,\n",
    "    data[\"terminated\"].shape,\n",
    "    data[\"log_probs\"][0].shape,\n",
    "    data[\"q_estimations\"].shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1]) torch.Size([500, 1]) torch.Size([500, 1])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "loss = {}\n",
    "\n",
    "returns = data[\"q_estimations\"]\n",
    "log_probs = data[\"log_probs\"]\n",
    "\n",
    "if True:\n",
    "    mean = returns.mean()\n",
    "    std = returns.std()\n",
    "    returns = (returns - mean) / (std + 1e-8)\n",
    "\n",
    "loss[\"actor\"] = -(log_probs * returns).mean()\n",
    "\n",
    "print(returns.shape, log_probs.shape, (log_probs * returns).shape)\n",
    "print(loss[\"actor\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1]) torch.Size([499, 1]) torch.Size([499, 1])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "loss = {}\n",
    "\n",
    "observations = data[\"observations\"]\n",
    "log_probs = data[\"log_probs\"]\n",
    "targets = data[\"q_estimations\"]\n",
    "\n",
    "values = critic(observations)\n",
    "advantages = targets[:-1].detach() - values[:-1]\n",
    "\n",
    "loss[\"actor\"] = -(log_probs[:-1] * advantages.detach()).mean()\n",
    "loss[\"critic\"] = (advantages**2).mean()\n",
    "\n",
    "print(values.shape, advantages.shape, (log_probs[:-1] * advantages).shape)\n",
    "print(loss[\"actor\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = data[\"observations\"]\n",
    "actions = data[\"actions\"]\n",
    "old_log_probs = data[\"log_probs\"]\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1]) torch.Size([500, 1]) torch.Size([500, 1])\n",
      "torch.Size([]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "loss = {}\n",
    "\n",
    "observations = data[\"observations\"]\n",
    "old_log_probs = data[\"log_probs\"]\n",
    "actions = data[\"actions\"]\n",
    "targets = data[\"q_estimations\"]\n",
    "\n",
    "_, new_log_probs = actor.get_action(observations, action=actions)\n",
    "\n",
    "ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "ratio_clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "\n",
    "values = critic(observations)\n",
    "\n",
    "advantages = targets.detach() - values\n",
    "\n",
    "actor_loss_1 = ratio * advantages.detach()\n",
    "actor_loss_2 = ratio_clipped * advantages.detach()\n",
    "\n",
    "loss[\"actor\"] = -(torch.min(actor_loss_1, actor_loss_2)).mean()\n",
    "loss[\"critic\"] = (advantages**2).mean()\n",
    "\n",
    "print(ratio.shape, new_log_probs.shape, torch.min(actor_loss_1, actor_loss_2).shape)\n",
    "print(loss[\"actor\"].shape, loss[\"critic\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
