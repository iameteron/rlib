{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import gymnasium as gym\n",
    "from torch.optim import Adam\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from rlib.algorithms.a2c import a2c\n",
    "from rlib.algorithms.ppo import ppo\n",
    "from rlib.algorithms.reinforce import reinforce\n",
    "from rlib.common.evaluation import get_trajectory, validation\n",
    "from rlib.common.policies import (\n",
    "    DiscreteStochasticMlpPolicy,\n",
    "    MlpCritic,\n",
    "    StochasticMlpPolicy,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.3906274e+00 -2.9158448e+38  3.0878833e-01 -2.2403954e+38] (4,)\n",
      "0 ()\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.sample(), env.observation_space.sample().shape)\n",
    "print(env.action_space.sample(), env.action_space.sample().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.915588   -0.09977326  3.9985425 ] (3,)\n",
      "[-0.05857551] (1,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.sample(), env.observation_space.sample().shape)\n",
    "print(env.action_space.sample(), env.action_space.sample().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2708755   4.943143   -4.5599866   0.6844208  -0.45849904 -2.7709544\n",
      " -2.6829946  -0.06156341  1.4338952   1.7893121  -2.1389308   1.6121464\n",
      " -3.8557954   3.723489    0.7315291  -0.6467077  -0.6370253   0.7312314\n",
      " -0.89675224  0.8066579   0.9811636  -0.01427133 -0.7791947   0.497158  ] (24,)\n",
      "[-0.16842724 -0.7188277  -0.68321896 -0.8027634 ] (4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.sample(), env.observation_space.sample().shape)\n",
    "print(env.action_space.sample(), env.action_space.sample().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9118681],\n",
       "       [-0.150479 ],\n",
       "       [ 0.7591273],\n",
       "       [ 0.9307303]], dtype=float32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample().reshape(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1\n"
     ]
    }
   ],
   "source": [
    "discrete = False\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "if discrete:\n",
    "    action_dim = env.action_space.n\n",
    "else:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(obs_dim, action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = DiscreteStochasticMlpPolicy(input_size, output_size)\n",
    "policy = StochasticMlpPolicy(input_size, output_size, action_scale=2)\n",
    "optimizer = Adam(policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = get_trajectory(env, policy, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce(env, policy, optimizer, total_timesteps=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1113, grad_fn=<UnbindBackward0>),\n",
       " tensor(-0.9518, grad_fn=<UnbindBackward0>))"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.forward(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1697.8311858462926"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, policy, deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor = DiscreteStochasticMlpPolicy(input_size, output_size)\n",
    "actor = StochasticMlpPolicy(input_size, output_size, action_scale=2)\n",
    "critic = MlpCritic(input_size)\n",
    "\n",
    "actor_optimizer = Adam(actor.parameters(), lr=3e-4)\n",
    "critic_optimizer = Adam(critic.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_n: 2000\n",
      "mean_trajectory_rewards: -1217.060302734375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 4000\n",
      "mean_trajectory_rewards: -1151.2532958984375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 6000\n",
      "mean_trajectory_rewards: -1255.719482421875\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 8000\n",
      "mean_trajectory_rewards: -991.3551025390625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 10000\n",
      "mean_trajectory_rewards: -1137.3951416015625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 12000\n",
      "mean_trajectory_rewards: -1250.375244140625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 14000\n",
      "mean_trajectory_rewards: -1279.54296875\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 16000\n",
      "mean_trajectory_rewards: -1271.0977783203125\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 18000\n",
      "mean_trajectory_rewards: -1371.1456298828125\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 20000\n",
      "mean_trajectory_rewards: -1120.009033203125\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 22000\n",
      "mean_trajectory_rewards: -1090.566162109375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 24000\n",
      "mean_trajectory_rewards: -1227.2841796875\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 26000\n",
      "mean_trajectory_rewards: -1220.4176025390625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 28000\n",
      "mean_trajectory_rewards: -1245.642578125\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 30000\n",
      "mean_trajectory_rewards: -1290.2310791015625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 32000\n",
      "mean_trajectory_rewards: -1316.8909912109375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 34000\n",
      "mean_trajectory_rewards: -1198.275390625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 36000\n",
      "mean_trajectory_rewards: -1322.880126953125\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 38000\n",
      "mean_trajectory_rewards: -1325.662353515625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 40000\n",
      "mean_trajectory_rewards: -1188.651123046875\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 42000\n",
      "mean_trajectory_rewards: -1265.839599609375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 44000\n",
      "mean_trajectory_rewards: -1293.421875\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 46000\n",
      "mean_trajectory_rewards: -1266.294921875\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 48000\n",
      "mean_trajectory_rewards: -1237.419677734375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 50000\n",
      "mean_trajectory_rewards: -1295.2279052734375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 52000\n",
      "mean_trajectory_rewards: -1231.1156005859375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 54000\n",
      "mean_trajectory_rewards: -1277.49658203125\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 56000\n",
      "mean_trajectory_rewards: -1255.4990234375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 58000\n",
      "mean_trajectory_rewards: -1237.3846435546875\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 60000\n",
      "mean_trajectory_rewards: -1349.526123046875\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 62000\n",
      "mean_trajectory_rewards: -1225.7425537109375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 64000\n",
      "mean_trajectory_rewards: -1177.554931640625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 66000\n",
      "mean_trajectory_rewards: -1345.055908203125\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 68000\n",
      "mean_trajectory_rewards: -1227.5626220703125\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 70000\n",
      "mean_trajectory_rewards: -1211.6392822265625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 72000\n",
      "mean_trajectory_rewards: -1210.746337890625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 74000\n",
      "mean_trajectory_rewards: -1285.099853515625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 76000\n",
      "mean_trajectory_rewards: -1319.9727783203125\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 78000\n",
      "mean_trajectory_rewards: -1113.958984375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 80000\n",
      "mean_trajectory_rewards: -1191.1107177734375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 82000\n",
      "mean_trajectory_rewards: -1272.0518798828125\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 84000\n",
      "mean_trajectory_rewards: -1036.675537109375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 86000\n",
      "mean_trajectory_rewards: -1179.3885498046875\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 88000\n",
      "mean_trajectory_rewards: -1284.9451904296875\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 90000\n",
      "mean_trajectory_rewards: -1301.1676025390625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 92000\n",
      "mean_trajectory_rewards: -1188.765869140625\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 94000\n",
      "mean_trajectory_rewards: -1278.2042236328125\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 96000\n",
      "mean_trajectory_rewards: -1130.3740234375\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 98000\n",
      "mean_trajectory_rewards: -1186.491455078125\n",
      "mean_trajectory_length: 199.90000915527344\n",
      "steps_n: 100000\n",
      "mean_trajectory_rewards: -1358.4803466796875\n",
      "mean_trajectory_length: 199.90000915527344\n"
     ]
    }
   ],
   "source": [
    "a2c(env, actor, critic, actor_optimizer, critic_optimizer, total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1205.740195305502"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, actor, deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "if discrete:\n",
    "    actor = DiscreteStochasticMlpPolicy(obs_dim, action_dim)\n",
    "else:\n",
    "    actor = StochasticMlpPolicy(obs_dim, action_dim)\n",
    "\n",
    "critic = MlpCritic(obs_dim)\n",
    "\n",
    "actor_optimizer = Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = Adam(critic.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo(env, actor, critic, actor_optimizer, critic_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.91028386], dtype=float32),\n",
       " tensor([[-1.9151]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.predict(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(\n",
    "    [\n",
    "        actor.predict(env.observation_space.sample())[1],\n",
    "        actor.predict(env.observation_space.sample())[1],\n",
    "    ], dim=0\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlib.common.buffer import RolloutBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = RolloutBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb.collect_rollouts(env, actor, rollout_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rb.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"observations\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"actions\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.forward(data[\"observations\"])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = data[\"observations\"]\n",
    "actions = data[\"actions\"]\n",
    "old_log_probs = data[\"log_probs\"]\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1]) torch.Size([10, 1]) torch.Size([10, 1])\n",
      "torch.Size([10, 1]) torch.Size([10, 1]) torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "loss = {}\n",
    "\n",
    "_, new_log_probs = actor.get_action(observations, action=actions)\n",
    "\n",
    "ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "ratio_clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "\n",
    "values = critic(observations)\n",
    "\n",
    "targets = data[\"q_estimations\"].reshape(-1, 1)\n",
    "advantages = targets.detach() - values\n",
    "print(targets.shape, values.shape, advantages.shape)\n",
    "print(old_log_probs.shape, new_log_probs.shape, ratio.shape)\n",
    "\n",
    "actor_loss_1 = ratio * advantages.detach()\n",
    "actor_loss_2 = ratio_clipped * advantages.detach()\n",
    "\n",
    "loss[\"actor\"] = -(torch.min(actor_loss_1, actor_loss_2)).mean()\n",
    "loss[\"critic\"] = (advantages**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-945.4736045591187"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(env, actor, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = torch.zeros((1, 2))\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = torch.ones((1, 2))\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Normal(mu, std)\n",
    "action = dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.log_prob(action).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9676])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.log_prob(action).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "agent = PPO(\"MlpPolicy\", env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03049863, -0.9995348 ,  0.8736682 ], dtype=float32), {})"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, _ = agent.predict(\n",
    "    env.observation_space.sample(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9694774], dtype=float32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.02193137, -0.9997595 , -0.17140453], dtype=float32),\n",
       " -2.4527108869676995,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
