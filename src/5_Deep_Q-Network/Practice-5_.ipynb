{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network\n",
    "\n",
    "Задаем структуру аппроксимации $Q^\\theta$, начальные вектор параметров $\\theta$, вероятность исследования среды $\\varepsilon = 1$.\n",
    "\n",
    "Для каждого эпизода $k$ делаем:\n",
    "\n",
    "Пока эпизод не закончен делаем:\n",
    "\n",
    "- Находясь в состоянии $S_t$ совершаем действие $A_t \\sim \\pi(\\cdot|S_t)$, где $\\pi = \\varepsilon\\text{-greedy}(Q^\\theta)$, получаем награду $R_t$  переходим в состояние $S_{t+1}$. Сохраняем $(S_t,A_t,R_t,S_{t+1}) \\rightarrow Memory$\n",
    "\n",
    "\n",
    "- Берем $\\{(s_i,a_i,r_i,s'_i)\\}_{i=1}^{n} \\leftarrow Memory$, определяем целевые значения\n",
    "\n",
    "$$\n",
    "y_i =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "r_i, &\\text{ если } s'_i\\text{ -терминальное},\\\\[0.0cm]\n",
    " r_i + \\gamma \\max\\limits_{a'} Q^\\theta(s'_i,a'), &\\text{ иначе}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "функцию потерь $Loss(\\theta) = \\frac{1}{n}\\sum\\limits_{i=1}^n \\big(y_i - Q^\\theta(s_i,a_i)\\big)^2$\n",
    "и обновляем вектор параметров\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta Loss(\\theta)\n",
    "$$\n",
    "\n",
    "- Уменьшаем $\\varepsilon$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T12:05:45.405043Z",
     "start_time": "2020-11-26T12:05:45.381352Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Qfunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(state_dim, 64)\n",
    "        self.linear_2 = nn.Linear(64, 64)\n",
    "        self.linear_3 = nn.Linear(64, action_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, states):\n",
    "        hidden = self.linear_1(states)\n",
    "        hidden = self.activation(hidden)\n",
    "        hidden = self.linear_2(hidden)\n",
    "        hidden = self.activation(hidden)\n",
    "        actions = self.linear_3(hidden)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T12:07:05.971803Z",
     "start_time": "2020-11-26T12:05:45.804040Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-3, batch_size=64, epsilon_decrease=0.01, epilon_min=0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.q_function = Qfunction(self.state_dim, self.action_dim)\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decrease = epsilon_decrease\n",
    "        self.epilon_min = epilon_min\n",
    "        self.memory = []\n",
    "        self.optimzaer = torch.optim.Adam(self.q_function.parameters(), lr=lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        q_values = self.q_function(torch.FloatTensor(state))\n",
    "        argmax_action = torch.argmax(q_values)\n",
    "        probs = self.epsilon * np.ones(self.action_dim) / self.action_dim\n",
    "        probs[argmax_action] += 1 - self.epsilon\n",
    "        action = np.random.choice(np.arange(self.action_dim), p=probs)\n",
    "        return action\n",
    "    \n",
    "    def fit(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, int(done), next_state])\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))\n",
    "    \n",
    "            targets = rewards + self.gamma * (1 - dones) * torch.max(self.q_function(next_states), dim=1).values\n",
    "            q_values = self.q_function(states)[torch.arange(self.batch_size), actions]\n",
    "            \n",
    "            loss = torch.mean((q_values - targets.detach()) ** 2)\n",
    "            loss.backward()\n",
    "            self.optimzaer.step()\n",
    "            self.optimzaer.zero_grad()\n",
    "            \n",
    "            if self.epsilon > self.epilon_min:\n",
    "                self.epsilon -= self.epsilon_decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total_reward: 21.0\n",
      "episode: 1, total_reward: 42.0\n",
      "episode: 2, total_reward: 13.0\n",
      "episode: 3, total_reward: 12.0\n",
      "episode: 4, total_reward: 17.0\n",
      "episode: 5, total_reward: 13.0\n",
      "episode: 6, total_reward: 8.0\n",
      "episode: 7, total_reward: 11.0\n",
      "episode: 8, total_reward: 10.0\n",
      "episode: 9, total_reward: 9.0\n",
      "episode: 10, total_reward: 12.0\n",
      "episode: 11, total_reward: 10.0\n",
      "episode: 12, total_reward: 10.0\n",
      "episode: 13, total_reward: 10.0\n",
      "episode: 14, total_reward: 9.0\n",
      "episode: 15, total_reward: 9.0\n",
      "episode: 16, total_reward: 8.0\n",
      "episode: 17, total_reward: 9.0\n",
      "episode: 18, total_reward: 9.0\n",
      "episode: 19, total_reward: 8.0\n",
      "episode: 20, total_reward: 8.0\n",
      "episode: 21, total_reward: 8.0\n",
      "episode: 22, total_reward: 10.0\n",
      "episode: 23, total_reward: 9.0\n",
      "episode: 24, total_reward: 9.0\n",
      "episode: 25, total_reward: 10.0\n",
      "episode: 26, total_reward: 10.0\n",
      "episode: 27, total_reward: 11.0\n",
      "episode: 28, total_reward: 10.0\n",
      "episode: 29, total_reward: 23.0\n",
      "episode: 30, total_reward: 10.0\n",
      "episode: 31, total_reward: 28.0\n",
      "episode: 32, total_reward: 63.0\n",
      "episode: 33, total_reward: 87.0\n",
      "episode: 34, total_reward: 36.0\n",
      "episode: 35, total_reward: 90.0\n",
      "episode: 36, total_reward: 62.0\n",
      "episode: 37, total_reward: 44.0\n",
      "episode: 38, total_reward: 38.0\n",
      "episode: 39, total_reward: 52.0\n",
      "episode: 40, total_reward: 50.0\n",
      "episode: 41, total_reward: 100.0\n",
      "episode: 42, total_reward: 32.0\n",
      "episode: 43, total_reward: 59.0\n",
      "episode: 44, total_reward: 35.0\n",
      "episode: 45, total_reward: 101.0\n",
      "episode: 46, total_reward: 97.0\n",
      "episode: 47, total_reward: 53.0\n",
      "episode: 48, total_reward: 142.0\n",
      "episode: 49, total_reward: 41.0\n",
      "episode: 50, total_reward: 48.0\n",
      "episode: 51, total_reward: 64.0\n",
      "episode: 52, total_reward: 46.0\n",
      "episode: 53, total_reward: 81.0\n",
      "episode: 54, total_reward: 104.0\n",
      "episode: 55, total_reward: 107.0\n",
      "episode: 56, total_reward: 123.0\n",
      "episode: 57, total_reward: 95.0\n",
      "episode: 58, total_reward: 169.0\n",
      "episode: 59, total_reward: 109.0\n",
      "episode: 60, total_reward: 110.0\n",
      "episode: 61, total_reward: 123.0\n",
      "episode: 62, total_reward: 101.0\n",
      "episode: 63, total_reward: 133.0\n",
      "episode: 64, total_reward: 125.0\n",
      "episode: 65, total_reward: 139.0\n",
      "episode: 66, total_reward: 120.0\n",
      "episode: 67, total_reward: 143.0\n",
      "episode: 68, total_reward: 125.0\n",
      "episode: 69, total_reward: 162.0\n",
      "episode: 70, total_reward: 143.0\n",
      "episode: 71, total_reward: 121.0\n",
      "episode: 72, total_reward: 305.0\n",
      "episode: 73, total_reward: 226.0\n",
      "episode: 74, total_reward: 220.0\n",
      "episode: 75, total_reward: 283.0\n",
      "episode: 76, total_reward: 500.0\n",
      "episode: 77, total_reward: 340.0\n",
      "episode: 78, total_reward: 307.0\n",
      "episode: 79, total_reward: 310.0\n",
      "episode: 80, total_reward: 500.0\n",
      "episode: 81, total_reward: 500.0\n",
      "episode: 82, total_reward: 435.0\n",
      "episode: 83, total_reward: 500.0\n",
      "episode: 84, total_reward: 500.0\n",
      "episode: 85, total_reward: 304.0\n",
      "episode: 86, total_reward: 353.0\n",
      "episode: 87, total_reward: 500.0\n",
      "episode: 88, total_reward: 251.0\n",
      "episode: 89, total_reward: 472.0\n",
      "episode: 90, total_reward: 456.0\n",
      "episode: 91, total_reward: 466.0\n",
      "episode: 92, total_reward: 500.0\n",
      "episode: 93, total_reward: 500.0\n",
      "episode: 94, total_reward: 493.0\n",
      "episode: 95, total_reward: 471.0\n",
      "episode: 96, total_reward: 183.0\n",
      "episode: 97, total_reward: 443.0\n",
      "episode: 98, total_reward: 226.0\n",
      "episode: 99, total_reward: 154.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQN(state_dim, action_dim)\n",
    "\n",
    "episode_n = 100\n",
    "t_max = 500\n",
    "\n",
    "for episode in range(episode_n):\n",
    "    total_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    for t in range(t_max):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        \n",
    "        agent.fit(state, action, reward, done, next_state)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f'episode: {episode}, total_reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804.247719318987"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pi * 16**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276404.87737713364"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.32 * 804.247719318987 * 0.8194 * 0.5 * (3.2 * 16)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276360.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28200 * 9.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3199480444743992"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "276360.0 / (804.247719318987 * 0.8194 * 0.5 * (3.2 * 16)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ T = C_T \\cdot F \\cdot \\frac{\\rho \\cdot (\\omega R)^2}{2} \\approx 276360 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ C_T = 0.32 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
