{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL Course 2023 Домашнее задание 7 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отчет по выполнению домашнего задания, Nikita Sorokin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif', fps=60):\n",
    "\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 50.0, frames[0].shape[0] / 50.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение алгоритмов для среды Pendulum-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $ \\pi^{\\theta}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m $ - нейронная сеть.\n",
    "\n",
    "В цикле по $ n $ для $ n \\in \\overline{1, N} $:\n",
    "\n",
    "* **(Policy evaluation)** В соответствии с политикой\n",
    "\n",
    "$$\n",
    "\\pi_n (s) = [\\pi^{\\theta_n} + Noise(\\varepsilon)]_A,\n",
    "$$\n",
    "\n",
    "получим $ K $ траекторий $ \\theta_k $ и награду $ G(\\tau_k) $. Оценим матожидание как:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\pi_n} [G] \\approx V_n :=  \\frac{1}{K} \\sum_{k=1}^{K} G(\\tau_k)\n",
    "$$\n",
    "\n",
    "\n",
    "* **(Policy improvement)** Выбираем элитные тракектории, как\n",
    "\n",
    "$$  \n",
    "T_n = \\{\\tau_k: k \\in \\overline{1, K}: G(\\tau_k) > \\gamma_q \\}, \\quad\n",
    "\\text{где $ \\gamma_q $ - квантиль уровня $ q $}. \n",
    "$$\n",
    "\n",
    "Определяем лосс:\n",
    "\n",
    "$$\n",
    "Loss(\\theta) = \\frac{1}{|T_n|} \\sum_{(a|s) \\in T_n} || \\pi^{\\theta_n} (s) - a||^2\n",
    "$$\n",
    "\n",
    "Обновляем $ \\theta $ градиентным спуском и уменьшаем $ \\varepsilon $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEM_continuous(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, min_action, max_action):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.min_action = min_action\n",
    "        self.max_action = max_action\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, self.action_dim)\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-1, betas=(0.9, 0.999))\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, _input):\n",
    "        return self.network(_input)\n",
    "\n",
    "    def get_action(self, state, eps=0, discrete_action=False):\n",
    "        state = torch.FloatTensor(state)\n",
    "        self.min_action = torch.FloatTensor([self.min_action])\n",
    "        self.max_action = torch.FloatTensor([self.max_action])\n",
    "\n",
    "        noise = torch.randn(self.action_dim)\n",
    "        noise = 2 if noise > 0 else -2\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = torch.clamp(self.forward(state) + eps * noise,\n",
    "                                 min=self.min_action, max=self.max_action)\n",
    "        action = action.detach().numpy()\n",
    "\n",
    "        if discrete_action == True:\n",
    "            action = list([2 if action > 0 else -2])\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_policy(self, elite_trajectories):\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for trajectory in elite_trajectories:\n",
    "            elite_states.extend(trajectory['states'])\n",
    "            elite_actions.extend(trajectory['actions'])\n",
    "        elite_states = torch.FloatTensor(elite_states)\n",
    "        elite_actions = torch.FloatTensor(elite_actions)\n",
    "\n",
    "        loss = self.loss(self.forward(elite_states), elite_actions)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, trajectory_len, eps=0, visualize=False, filename='gym_animation.gif', discrete_action=False):\n",
    "    trajectory = {'states':[], 'actions': [], 'total_reward': 0}\n",
    "\n",
    "    state = env.reset()\n",
    "    trajectory['states'].append(state)\n",
    "\n",
    "    frames = []\n",
    "    for i in range(trajectory_len):\n",
    "\n",
    "        action = agent.get_action(state, eps, discrete_action)\n",
    "        trajectory['actions'].append(action)\n",
    "\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        trajectory['total_reward'] += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        if visualize:\n",
    "            frames.append(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "        if i != trajectory_len - 1:\n",
    "            trajectory['states'].append(state)\n",
    "\n",
    "    if visualize:\n",
    "        save_frames_as_gif(frames, filename=filename)\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "def get_elite_trajectories(trajectories, q_param):\n",
    "    total_rewards = [trajectory['total_reward'] for trajectory in trajectories]\n",
    "    quantile = np.quantile(total_rewards, q=q_param)\n",
    "    return [trajectory for trajectory in trajectories if trajectory['total_reward'] > quantile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, episode_n, trajectory_n, trajectory_len, q_param, noise=False, autosave=False, discrete_action=False):\n",
    "    mean_rewards = []\n",
    "    all_rewards = []\n",
    "    max_reward = -5000\n",
    "    for episode in range(episode_n):\n",
    "\n",
    "        eps = 0\n",
    "        if noise == True:\n",
    "            eps = 1 / np.sqrt(episode + 1)\n",
    "\n",
    "        trajectories = [get_trajectory(env, agent, trajectory_len, eps, discrete_action=discrete_action) for _ in range(trajectory_n)]\n",
    "        mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "\n",
    "        if autosave == True:\n",
    "            if mean_total_reward < max_reward:\n",
    "                continue\n",
    "        max_reward = mean_total_reward\n",
    "\n",
    "        all_rewards.extend([trajectory['total_reward'] for trajectory in trajectories])\n",
    "        mean_rewards.append(mean_total_reward)\n",
    "        print(f'episode: {episode}, mean_total_reward = {mean_total_reward}')\n",
    "\n",
    "        elite_trajectories = get_elite_trajectories(trajectories, q_param)\n",
    "        if len(elite_trajectories) > 0:\n",
    "            agent.update_policy(elite_trajectories)\n",
    "\n",
    "    return all_rewards, mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(env, agent, validation_n=100, trajectory_len=200):\n",
    "    rewards = []\n",
    "    for i in range(validation_n):\n",
    "        trajectory = get_trajectory(env, agent, trajectory_len, visualize=False, discrete_action=True)\n",
    "        rewards.append(trajectory['total_reward'])\n",
    "\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение: (30 минут)\n",
    "\n",
    "Подобранные гиперпараметры:\n",
    "\n",
    "\n",
    "```python\n",
    "episode_n = 125\n",
    "trajectory_n = 100\n",
    "trajectory_len = 200\n",
    "q_param = 0.8\n",
    "\n",
    "agent.optimizer = torch.optim.Adam(agent.parameters(), lr=1e-1)\n",
    "```\n",
    "\n",
    "При обучении разрешаем всего 2 действия 2 и -2 из всего отрезка действий [-2, 2]. Это ограничение позволяет маятнику научиться уверенно раскачиваться. За эту функцию отвечает условие discrete_action = True.\n",
    "\n",
    "**Замечание:** При выполнении дз 2 обучение проходило в 3 этапа:\n",
    "\n",
    "1. Учимся с большим количеством шума, причем разрешаем всего 2 действия 2 и -2 из всего отрезка действий [-2, 2]. Это ограничение позволяет маятнику научиться уверенно раскачиваться. За эту функцию отвечает условие discrete_action = True.\n",
    "\n",
    "2. Оставляем шум, возвращаем возможность выполнять все действия в отрезке [-2, 2]. Этот этап позволяет научиться маятнику выбирать действия, когда он проходит положение неустойчивого равновесия наверху.\n",
    "\n",
    "3. Убираем шум, разрешаем действия 2 и -2. К тому же теперь используем условие autosave = True, которое не позволяет модели учиться если mean_total_reward полученный в текущем эпизоде меньше предыдущего. Этот этап позволяет уверенно управлять маятником, когда тот находится наверху и пытается устоять.\n",
    "\n",
    "Но для честности сравнения алгоритмов обучение проводится в 1 этап: \n",
    "\n",
    "1. Действия разрешается всего 2 штуки: 2 и -2. Размеренное использование шума $ \\varepsilon(n) = 1 / \\sqrt{n + 1} $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_action, max_action = -2, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CEM_continuous(state_dim, action_dim, min_action, max_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_n = 125\n",
    "trajectory_n = 100\n",
    "trajectory_len = 200\n",
    "q_param = 0.8\n",
    "\n",
    "agent.optimizer = torch.optim.Adam(agent.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "mean_rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_history, new_mean_rewards_history = train(env, agent, episode_n, trajectory_n, trajectory_len, q_param, discrete_action=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.extend(new_history)\n",
    "mean_rewards_history.extend(new_mean_rewards_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "График обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.load('cem_pendulum_mean_rewards_history.npy')\n",
    "history = history[:125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('CEM, Pendulum-v1')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Mean reward')\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Валидация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score = validation(env, agent)\n",
    "print(f'mean total reward on 100 validation trajectories: {val_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за длительности обучения и нестабильности ограничимся только этим результатом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использую экспоненциальное сглаживание, преобразуем график истории обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = np.load('cem_pendulum_history.npy')\n",
    "h2 = np.load('cem_pendulum_history2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = h1[:h2.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plots(h1):\n",
    "\n",
    "    h_smoothed1 = np.zeros_like(h1)\n",
    "\n",
    "    alpha = 0.1\n",
    "    h_smoothed1[0] = h1[0]\n",
    "\n",
    "    for i in range(1, h_smoothed1.size):\n",
    "        h_smoothed1[i] = alpha * h1[i] + (1 - alpha) * h_smoothed1[i - 1]\n",
    "\n",
    "    h_smoothed1 = h_smoothed1.reshape(-1, 1)\n",
    "\n",
    "    plt.plot(np.arange(h_smoothed1.size), h_smoothed1)\n",
    "\n",
    "    plt.title('CEM, Pendulum-v1')\n",
    "    plt.xlabel('Trajectory number')\n",
    "    plt.ylabel('Trajectory reward')\n",
    "    plt.legend()\n",
    "\n",
    "    return h_smoothed1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cem_history_smoothed = get_plots(h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При меньшем коэффиценте сглаживания награда в конце становится еще меньше. Поэтому ограничимся таким уровнем сглаживания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Задаем структуру аппроксимации $Q^\\theta$, начальные вектор параметров $\\theta$, вероятность исследования среды $\\varepsilon = 1$.\n",
    "\n",
    "Для каждого эпизода $k$ делаем:\n",
    "\n",
    "Пока эпизод не закончен делаем:\n",
    "\n",
    "- Находясь в состоянии $S_t$ совершаем действие $A_t \\sim \\pi(\\cdot|S_t)$, где $\\pi = \\varepsilon\\text{-greedy}(Q^\\theta)$, получаем награду $R_t$  переходим в состояние $S_{t+1}$. Сохраняем $(S_t,A_t,R_t,S_{t+1}) \\rightarrow Memory$\n",
    "\n",
    "\n",
    "- Берем $\\{(s_i,a_i,r_i,s'_i)\\}_{i=1}^{n} \\leftarrow Memory$, определяем целевые значения\n",
    "\n",
    "$$\n",
    "y_i =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "r_i, &\\text{ если } s'_i\\text{ -терминальное},\\\\[0.0cm]\n",
    " r_i + \\gamma \\max\\limits_{a'} Q^\\theta(s'_i,a'), &\\text{ иначе}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "функцию потерь $Loss(\\theta) = \\frac{1}{n}\\sum\\limits_{i=1}^n \\big(y_i - Q^\\theta(s_i,a_i)\\big)^2$\n",
    "и обновляем вектор параметров\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta Loss(\\theta)\n",
    "$$\n",
    "\n",
    "- Уменьшаем $\\varepsilon$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для использования этого алгоритма в средах с непрерывным пространством действий воспользуемся дискретизацией. Договоримся, что агент может выполнять только 2 действия: -2 и 2. Функция get_action() агента будет выдавать номер действия (0 или 1). При создании траектории после использования функции get_action() будем преобразовывать ее вывод в действие -2 или 2 (0 переходит в -2, 1 в 2).\n",
    "\n",
    "Реализация:\n",
    "\n",
    "```python\n",
    "...\n",
    "actual_action = np.array([int(-2 + 4 * action)])\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация алгоритма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qfunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_n):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(state_dim, 512)\n",
    "        self.linear_3 = nn.Linear(512, action_n)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, states):\n",
    "        hidden = self.linear_1(states)\n",
    "        hidden = self.activation(hidden)\n",
    "        actions = self.linear_3(hidden)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, state_dim, action_n, gamma=0.99, lr=1e-3, batch_size=64, epsilon_decrease=0.01, epilon_min=0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_n = action_n\n",
    "        self.q_function = Qfunction(self.state_dim, self.action_n)\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decrease = epsilon_decrease\n",
    "        self.epilon_min = epilon_min\n",
    "        self.memory = []\n",
    "        self.lr = lr\n",
    "        self.optimizer = torch.optim.Adam(self.q_function.parameters(), lr=self.lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        q_values = self.q_function(torch.FloatTensor(state))\n",
    "        argmax_action = torch.argmax(q_values)\n",
    "        probs = self.epsilon * np.ones(self.action_n) / self.action_n\n",
    "        probs[argmax_action] += 1 - self.epsilon\n",
    "        action = np.random.choice(np.arange(self.action_n), p=probs)\n",
    "        return action\n",
    "    \n",
    "    def fit(self, state, action, reward, done, next_state, algo='default', hard_n=3, tau=0.9):\n",
    "        self.memory.append([state, action, reward, int(done), next_state])\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "    \n",
    "            if algo == 'default':\n",
    "\n",
    "                batch = random.sample(self.memory, self.batch_size)\n",
    "                states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))\n",
    "\n",
    "                targets = rewards + self.gamma * (1 - dones) * torch.max(self.q_function(next_states), dim=1).values\n",
    "                q_values = self.q_function(states)[torch.arange(self.batch_size), actions]\n",
    "                loss = torch.mean((q_values - targets.detach()) ** 2)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            if algo == 'hard':\n",
    "\n",
    "                theta_updating = self.q_function.state_dict()\n",
    "                q_function_updating = Qfunction(self.state_dim, self.action_n)\n",
    "                q_function_updating.load_state_dict(theta_updating, strict=False)\n",
    "                \n",
    "                optimizer_updating = torch.optim.Adam(q_function_updating.parameters(), lr=self.lr)\n",
    "\n",
    "                for _ in range(hard_n):\n",
    "\n",
    "                    batch = random.sample(self.memory, self.batch_size)\n",
    "                    states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))\n",
    "\n",
    "                    targets = rewards + self.gamma * (1 - dones) * torch.max(self.q_function(next_states), dim=1).values\n",
    "                    q_values = q_function_updating(states)[torch.arange(self.batch_size), actions]\n",
    "                    loss = torch.mean((q_values - targets.detach()) ** 2)\n",
    "                    loss.backward()\n",
    "                    optimizer_updating.step()\n",
    "                    optimizer_updating.zero_grad()\n",
    "\n",
    "                theta_new = q_function_updating.state_dict()\n",
    "                self.q_function.load_state_dict(theta_new)\n",
    "\n",
    "            if algo == 'soft':\n",
    "\n",
    "                q_function_updating = Qfunction(self.state_dim, self.action_n)\n",
    "                q_function_updating.load_state_dict(self.q_function.state_dict())\n",
    "                optimizer_updating = torch.optim.Adam(q_function_updating.parameters(), lr=self.lr)\n",
    "\n",
    "                batch = random.sample(self.memory, self.batch_size)\n",
    "                states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))\n",
    "\n",
    "                targets = rewards + self.gamma * (1 - dones) * torch.max(self.q_function(next_states), dim=1).values\n",
    "                q_values = q_function_updating(states)[torch.arange(self.batch_size), actions]\n",
    "                loss = torch.mean((q_values - targets.detach()) ** 2)\n",
    "                loss.backward()\n",
    "                optimizer_updating.step()\n",
    "                optimizer_updating.zero_grad()\n",
    "\n",
    "                theta_new = q_function_updating.state_dict()\n",
    "                self.q_function.load_state_dict(theta_new)\n",
    "\n",
    "                for param1, param2 in zip(self.q_function.parameters(), q_function_updating.parameters()):\n",
    "                    param1.data = tau * param2.data + (1 - tau) * param1.data\n",
    "\n",
    "            if algo == 'double':\n",
    "\n",
    "                q_function_updating = Qfunction(self.state_dim, self.action_n)\n",
    "                q_function_updating.load_state_dict(self.q_function.state_dict())\n",
    "                optimizer_updating = torch.optim.Adam(q_function_updating.parameters(), lr=self.lr)\n",
    "\n",
    "                batch = random.sample(self.memory, self.batch_size)\n",
    "                states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))\n",
    "\n",
    "                argument = torch.argmax(self.q_function(next_states), dim=1)\n",
    "                argument = torch.LongTensor(argument)\n",
    "                \n",
    "                q_function_updating_values = torch.tensor(\n",
    "                    [q_function_updating(next_states)[i, argument[i]] for i in range(len(batch))]\n",
    "                )\n",
    "\n",
    "                targets = rewards + self.gamma * (1 - dones) * q_function_updating_values\n",
    "                q_values = q_function_updating(states)[torch.arange(self.batch_size), actions]\n",
    "                loss = torch.mean((q_values - targets.detach()) ** 2)\n",
    "                loss.backward()\n",
    "                optimizer_updating.step()\n",
    "                optimizer_updating.zero_grad()\n",
    "\n",
    "                theta_new = q_function_updating.state_dict()\n",
    "                self.q_function.load_state_dict(theta_new)\n",
    "\n",
    "                for param1, param2 in zip(self.q_function.parameters(), q_function_updating.parameters()):\n",
    "                    param1.data = tau * param2.data + (1 - tau) * param1.data\n",
    "            \n",
    "            if self.epsilon > self.epilon_min:\n",
    "                self.epsilon -= self.epsilon_decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, trajectory_len, fit=False, algo='default', visualize=False, filename='gym_animation.gif'):\n",
    "    trajectory = {'states':[], 'actions': [], 'total_reward': 0}\n",
    "    \n",
    "    state = env.reset()\n",
    "    trajectory['states'].append(state)\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(trajectory_len):\n",
    "\n",
    "        action = agent.get_action(state)\n",
    "        trajectory['actions'].append(action)\n",
    "\n",
    "        actual_action = np.array([int(-2 + 4 * action)])\n",
    "\n",
    "        next_state, reward, done, _ = env.step(actual_action)\n",
    "        trajectory['total_reward'] += reward\n",
    "\n",
    "        if fit == True:\n",
    "            agent.fit(state, action, reward, done, next_state, algo)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        if visualize:\n",
    "            frames.append(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "    if visualize:\n",
    "        save_frames_as_gif(frames, filename=filename)\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, episode_n, trajectory_len, algo='default'):\n",
    "    rewards = []\n",
    "    for episode in range(episode_n):\n",
    "        trajectory = get_trajectory(env, agent, trajectory_len, fit=True, algo=algo)\n",
    "        print(f\"episode: {episode}, total_reward: {trajectory['total_reward']}\")\n",
    "        rewards.append(trajectory['total_reward'])\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(env, agent, trajectory_len, validation_n):\n",
    "    total_rewards = []\n",
    "    for _ in range(validation_n):\n",
    "        trajectory = get_trajectory(env, agent, trajectory_len)\n",
    "        total_rewards.append(trajectory['total_reward'])\n",
    "\n",
    "    return np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение: (5 мин)\n",
    "\n",
    "Используется стандартная версия алгоритма (без Soft/Hard Target модификаций). Проблем связанных с автокорелляцией не возникает, поскольку функция наград непрерывна по состоянию для среду Pendulum-v1:\n",
    "\n",
    "$$\n",
    "r = - \\left( \\theta^2 + 0.1 \\cdot \\frac{\\partial^2 \\theta}{\\partial t^2} + 0.001 \\cdot u \\right)\n",
    "$$\n",
    "\n",
    "Выбранные гиперпараметры:\n",
    "\n",
    "```python\n",
    "episode_n = 500\n",
    "trajectory_len = 200\n",
    "\n",
    "dqn_agent.lr = 1e-4\n",
    "dqn_agent.epsilon_decrease = 0.005\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQN(state_dim, action_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_n = 500\n",
    "trajectory_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.lr = 1e-4\n",
    "dqn_agent.epsilon_decrease = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_rewards = train(env, dqn_agent, episode_n, trajectory_len, algo='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('DQN, Pendulum-v1')\n",
    "plt.xlabel('Trajectory Number')\n",
    "plt.ylabel('Trajectory Reward')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.plot(dqn_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Валидация:\n",
    "\n",
    "**DQN validation_score: -149.3915106810389**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_n = 100\n",
    "validation_score = validation(env, dqn_agent, validation_n=validation_n, trajectory_len=200)\n",
    "print(f'DQN validation_score: {validation_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторим еще 2 раза и построим сглаженный график:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = np.load('dqn_history1.npy')\n",
    "h2 = np.load('dqn_history2.npy')\n",
    "h3 = np.load('dqn_history3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plots(h1, h2, h3):\n",
    "\n",
    "    h_smoothed1 = np.zeros_like(h1)\n",
    "    h_smoothed2 = np.zeros_like(h1)\n",
    "    h_smoothed3 = np.zeros_like(h1)\n",
    "\n",
    "    alpha = 0.1\n",
    "    h_smoothed1[0] = h1[0]\n",
    "    h_smoothed2[0] = h2[0]\n",
    "    h_smoothed3[0] = h3[0]\n",
    "\n",
    "    for i in range(1, h_smoothed1.size):\n",
    "        h_smoothed1[i] = alpha * h1[i] + (1 - alpha) * h_smoothed1[i - 1]\n",
    "        h_smoothed2[i] = alpha * h2[i] + (1 - alpha) * h_smoothed2[i - 1]\n",
    "        h_smoothed3[i] = alpha * h3[i] + (1 - alpha) * h_smoothed3[i - 1]\n",
    "\n",
    "\n",
    "    h_smoothed1 = h_smoothed1.reshape(-1, 1)\n",
    "    h_smoothed2 = h_smoothed2.reshape(-1, 1)\n",
    "    h_smoothed3 = h_smoothed3.reshape(-1, 1)\n",
    "\n",
    "    h_smoothed = np.concatenate((h_smoothed1, h_smoothed2, h_smoothed3), axis=1)\n",
    "    h_smoothed_mean = np.mean(h_smoothed, axis=1)\n",
    "    stds = np.std(h_smoothed, axis=1)\n",
    "\n",
    "    plt.plot(np.arange(h_smoothed_mean.size), h_smoothed_mean)\n",
    "    plt.fill_between(np.arange(h_smoothed_mean.size), h_smoothed_mean - stds, h_smoothed_mean + stds, alpha=0.5)\n",
    "\n",
    "    plt.title('PPO, default advantage, Pendulum-v1')\n",
    "    plt.xlabel('Trajectory number')\n",
    "    plt.ylabel('Trajectory reward')\n",
    "    plt.legend()\n",
    "\n",
    "    return h_smoothed_mean, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_history_smoothed, dqn_std_smoothed = get_plots(h1, h2, h3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение проходит очень быстро, а результат получается на удивление хорошим!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Задаем структуру аппроксимации $Q^\\theta$, начальные вектор параметров $\\theta$, вероятность исследования среды $\\varepsilon = 1$.\n",
    "\n",
    "Для каждого эпизода $k$ делаем:\n",
    "\n",
    "Пока эпизод не закончен делаем:\n",
    "\n",
    "- Находясь в состоянии $S_t$ совершаем действие $A_t \\sim \\pi(\\cdot|S_t)$, где $\\pi = \\varepsilon\\text{-greedy}(Q^\\theta)$, получаем награду $R_t$  переходим в состояние $S_{t+1}$. Сохраняем $(S_t,A_t,R_t,S_{t+1}) \\rightarrow Memory$\n",
    "\n",
    "\n",
    "- Берем $\\{(s_i,a_i,r_i,s'_i)\\}_{i=1}^{n} \\leftarrow Memory$, определяем целевые значения\n",
    "\n",
    "$$\n",
    "y_i =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "r_i, &\\text{ если } s'_i\\text{ -терминальное},\\\\[0.0cm]\n",
    " r_i + \\gamma \\max\\limits_{a'} Q^\\theta(s'_i,a'), &\\text{ иначе}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "функцию потерь $Loss(\\theta) = \\frac{1}{n}\\sum\\limits_{i=1}^n \\big(y_i - Q^\\theta(s_i,a_i)\\big)^2$\n",
    "и обновляем вектор параметров\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta Loss(\\theta)\n",
    "$$\n",
    "\n",
    "- Уменьшаем $\\varepsilon$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.9, batch_size=128, \n",
    "                 epsilon=0.2, epoch_n=30, pi_lr=1e-4, v_lr=5e-4):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.pi_model = nn.Sequential(nn.Linear(self.state_dim, 128), nn.ReLU(),\n",
    "                                      nn.Linear(128, 128), nn.ReLU(),\n",
    "                                      nn.Linear(128, 2 * self.action_dim), nn.Tanh())\n",
    "        \n",
    "        self.v_model = nn.Sequential(nn.Linear(self.state_dim, 128), nn.ReLU(),\n",
    "                                     nn.Linear(128, 128), nn.ReLU(),\n",
    "                                     nn.Linear(128, 1))\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epoch_n = epoch_n\n",
    "        self.pi_optimizer = torch.optim.Adam(self.pi_model.parameters(), lr=pi_lr)\n",
    "        self.v_optimizer = torch.optim.Adam(self.v_model.parameters(), lr=v_lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        mean, log_std = self.pi_model(torch.FloatTensor(state))\n",
    "        dist = Normal(mean, torch.exp(log_std))\n",
    "        action = dist.sample()\n",
    "        return action.numpy().reshape(1)\n",
    "\n",
    "    def fit(self, states, actions, rewards, dones, advantage='default'):\n",
    "        \n",
    "        states, actions, rewards, dones = map(np.array, [states, actions, rewards, dones])\n",
    "        rewards, dones = rewards.reshape(-1, 1), dones.reshape(-1, 1)\n",
    "\n",
    "        next_states = np.zeros_like(states)\n",
    "        next_states[:-1] = states[1:]\n",
    "\n",
    "        returns = np.zeros(rewards.shape)\n",
    "        returns[-1] = rewards[-1]\n",
    "        for t in range(returns.shape[0] - 2, -1, -1):\n",
    "            returns[t] = rewards[t] + (1 - dones[t]) * self.gamma * returns[t + 1]\n",
    "\n",
    "        states, next_states, actions, rewards, returns, dones = map(torch.FloatTensor, [states, next_states, actions, rewards, returns, dones])\n",
    "\n",
    "        mean, log_std = self.pi_model(states).T\n",
    "        mean, log_std = mean.unsqueeze(1), log_std.unsqueeze(1)\n",
    "        dist = Normal(mean, torch.exp(log_std))\n",
    "        old_log_probs = dist.log_prob(actions).detach()\n",
    "\n",
    "        for epoch in range(self.epoch_n):\n",
    "            \n",
    "            idxs = np.random.permutation(returns.shape[0])\n",
    "            for i in range(0, returns.shape[0], self.batch_size):\n",
    "                b_idxs = idxs[i: i + self.batch_size]\n",
    "                b_states = states[b_idxs]\n",
    "                b_next_states = next_states[b_idxs]\n",
    "                b_dones = dones[b_idxs]\n",
    "                b_actions = actions[b_idxs]\n",
    "                b_rewards = rewards[b_idxs]\n",
    "                b_returns = returns[b_idxs]\n",
    "                b_old_log_probs = old_log_probs[b_idxs]\n",
    "\n",
    "                if advantage == 'default':\n",
    "                    b_advantage = b_returns.detach() - self.v_model(b_states)\n",
    "\n",
    "                if advantage == 'bellman':\n",
    "                    b_advantage = b_rewards.detach() + (1 - b_dones.detach()) * self.gamma * self.v_model(b_next_states.detach()) - self.v_model(b_states) \n",
    "\n",
    "                if advantage == 'test':\n",
    "                    b_advantage = b_rewards.detach() + (1 - b_dones.detach()) * self.gamma * self.v_model(b_next_states) - self.v_model(b_states) \n",
    "\n",
    "                b_mean, b_log_std = self.pi_model(b_states).T\n",
    "                b_mean, b_log_std = b_mean.unsqueeze(1), b_log_std.unsqueeze(1)\n",
    "                b_dist = Normal(b_mean, torch.exp(b_log_std))\n",
    "                b_new_log_probs = b_dist.log_prob(b_actions)\n",
    "    \n",
    "                b_ratio = torch.exp(b_new_log_probs - b_old_log_probs)\n",
    "                pi_loss_1 = b_ratio * b_advantage.detach()\n",
    "                pi_loss_2 = torch.clamp(b_ratio, 1. - self.epsilon,  1. + self.epsilon) * b_advantage.detach()\n",
    "                pi_loss = - torch.mean(torch.min(pi_loss_1, pi_loss_2))\n",
    "                \n",
    "                pi_loss.backward()\n",
    "                self.pi_optimizer.step()\n",
    "                self.pi_optimizer.zero_grad()\n",
    "                \n",
    "                v_loss = torch.mean(b_advantage ** 2)\n",
    "    \n",
    "                v_loss.backward()\n",
    "                self.v_optimizer.step()\n",
    "                self.v_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, trajectory_len=200, visualize=False, filename='gym_animation.gif'):\n",
    "    trajectory = {'states':[], 'actions': [], 'rewards': [], 'dones': []}\n",
    "    \n",
    "    state = env.reset()\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(trajectory_len):\n",
    "\n",
    "        trajectory['states'].append(state)\n",
    "\n",
    "        action = agent.get_action(state)\n",
    "        trajectory['actions'].append(action)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(2 * action)\n",
    "        trajectory['rewards'].append(reward)\n",
    "        trajectory['dones'].append(done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        if visualize:\n",
    "            frames.append(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "    if visualize:\n",
    "        save_frames_as_gif(frames, filename=filename)\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, episode_n=50, trajectory_n=20, trajectory_len=200, advantage='default'):\n",
    "    total_rewards = []\n",
    "    for episode in range(episode_n):\n",
    "\n",
    "        states, actions, rewards, dones = [], [], [], []\n",
    "\n",
    "        for i in range(trajectory_n):\n",
    "\n",
    "            trajectory = get_trajectory(env, agent, trajectory_len)\n",
    "\n",
    "            states.extend(trajectory['states'])\n",
    "            actions.extend(trajectory['actions'])\n",
    "            rewards.extend(trajectory['rewards'])\n",
    "            dones.extend(trajectory['dones'])\n",
    "\n",
    "            total_rewards.append(np.sum(trajectory['rewards']))\n",
    "\n",
    "        print(f\"{episode}: mean reward = {np.mean(total_rewards[-trajectory_n:])}\")\n",
    "\n",
    "        agent.fit(states, actions, rewards, dones, advantage)\n",
    "\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(env, agent, validation_n, trajectory_len=200):\n",
    "    total_rewards = []\n",
    "    for _ in range(validation_n):\n",
    "        trajectory = get_trajectory(env, agent, trajectory_len)\n",
    "        total_rewards.append(np.sum(trajectory['rewards']))\n",
    "\n",
    "    return np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение: (4 мин)\n",
    "\n",
    "Используется стандартная версия алгоритма для непрерывного одномерного пространства действий.\n",
    "\n",
    "Для обучения были подобраны следующие гиперпараметры:\n",
    "\n",
    "```python\n",
    "episode_n = 25\n",
    "trajectory_n = 20\n",
    "\n",
    "gamma = 0.9\n",
    "batch_size = 128 \n",
    "epsilon = 0.2\n",
    "epoch_n = 30 \n",
    "pi_lr = 1e-4 \n",
    "v_lr = 5e-4\n",
    "```\n",
    "\n",
    "Архитектуры нейронных сетей:\n",
    "\n",
    "```python\n",
    "\n",
    "self.pi_model = nn.Sequential(nn.Linear(self.state_dim, 128), nn.ReLU(),\n",
    "                              nn.Linear(128, 128), nn.ReLU(),\n",
    "                              nn.Linear(128, 2 * self.action_dim), nn.Tanh())\n",
    "\n",
    "self.v_model = nn.Sequential(nn.Linear(self.state_dim, 128), nn.ReLU(),\n",
    "                             nn.Linear(128, 128), nn.ReLU(),\n",
    "                             nn.Linear(128, 1))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_default_agent = PPO(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_n = 25\n",
    "trajectory_n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_default_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_default_rewards = train(env, ppo_default_agent, episode_n, trajectory_n, advantage='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_default_history.extend(ppo_default_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "График обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('PPO, default advantage, Pendulum-v1')\n",
    "plt.xlabel('Trajectory number')\n",
    "plt.ylabel('Trajectory reward')\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(ppo_default_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_n = 100\n",
    "\n",
    "validation_score = validation(env, ppo_default_agent, validation_n=validation_n, trajectory_len=200)\n",
    "print(f'PPO, default advantage, validation_score: {validation_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторим еще 2 раза и построим сглаженный график:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = np.load('ppo_default_history1.npy')\n",
    "h2 = np.load('ppo_default_history2.npy')\n",
    "h3 = np.load('ppo_default_history3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plots(h1, h2, h3):\n",
    "\n",
    "    h_smoothed1 = np.zeros_like(h1)\n",
    "    h_smoothed2 = np.zeros_like(h1)\n",
    "    h_smoothed3 = np.zeros_like(h1)\n",
    "\n",
    "    alpha = 0.1\n",
    "    h_smoothed1[0] = h1[0]\n",
    "    h_smoothed2[0] = h2[0]\n",
    "    h_smoothed3[0] = h3[0]\n",
    "\n",
    "    for i in range(1, h_smoothed1.size):\n",
    "        h_smoothed1[i] = alpha * h1[i] + (1 - alpha) * h_smoothed1[i - 1]\n",
    "        h_smoothed2[i] = alpha * h2[i] + (1 - alpha) * h_smoothed2[i - 1]\n",
    "        h_smoothed3[i] = alpha * h3[i] + (1 - alpha) * h_smoothed3[i - 1]\n",
    "\n",
    "\n",
    "    h_smoothed1 = h_smoothed1.reshape(-1, 1)\n",
    "    h_smoothed2 = h_smoothed2.reshape(-1, 1)\n",
    "    h_smoothed3 = h_smoothed3.reshape(-1, 1)\n",
    "\n",
    "    h_smoothed = np.concatenate((h_smoothed1, h_smoothed2, h_smoothed3), axis=1)\n",
    "    h_smoothed_mean = np.mean(h_smoothed, axis=1)\n",
    "    stds = np.std(h_smoothed, axis=1)\n",
    "\n",
    "    plt.plot(np.arange(h_smoothed_mean.size), h_smoothed_mean)\n",
    "    plt.fill_between(np.arange(h_smoothed_mean.size), h_smoothed_mean - stds, h_smoothed_mean + stds, alpha=0.5)\n",
    "\n",
    "    plt.title('PPO, default advantage, Pendulum-v1')\n",
    "    plt.xlabel('Trajectory number')\n",
    "    plt.ylabel('Trajectory reward')\n",
    "    plt.legend()\n",
    "\n",
    "    return h_smoothed_mean, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_history_smoothed, ppo_std_smoothed = get_plots(h1, h2, h3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, alpha=1e-3, tau=1e-2, \n",
    "                 batch_size=64, pi_lr=1e-3, q_lr=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi_model = nn.Sequential(nn.Linear(state_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 2 * action_dim), nn.Tanh())\n",
    "\n",
    "        self.q1_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 1))\n",
    "\n",
    "        self.q2_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 1))\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        self.pi_optimizer = torch.optim.Adam(self.pi_model.parameters(), pi_lr)\n",
    "        self.q1_optimizer = torch.optim.Adam(self.q1_model.parameters(), q_lr)\n",
    "        self.q2_optimizer = torch.optim.Adam(self.q2_model.parameters(), q_lr)\n",
    "\n",
    "        self.q1_target_model = deepcopy(self.q1_model)\n",
    "        self.q2_target_model = deepcopy(self.q2_model)\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action, _ = self.predict_actions(state)\n",
    "        return action.squeeze(1).detach().numpy()\n",
    "\n",
    "\n",
    "    def fit(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, done, next_state])\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, dones, next_states = map(torch.FloatTensor, zip(*batch))\n",
    "            rewards, dones = rewards.unsqueeze(1), dones.unsqueeze(1)\n",
    "\n",
    "            next_actions, next_log_probs = self.predict_actions(next_states)\n",
    "            \n",
    "            next_states_and_actions = torch.concatenate((next_states, next_actions), dim=1)\n",
    "            next_q1_values = self.q1_target_model(next_states_and_actions)\n",
    "            next_q2_values = self.q2_target_model(next_states_and_actions)\n",
    "            next_min_q_values = torch.min(next_q1_values, next_q2_values)\n",
    "            targets = rewards + self.gamma * (1 - dones) * (next_min_q_values - self.alpha * next_log_probs)\n",
    "\n",
    "            states_and_actions = torch.concatenate((states, actions), dim=1)\n",
    "            q1_loss = torch.mean((self.q1_model(states_and_actions) - targets.detach()) ** 2)\n",
    "            q2_loss = torch.mean((self.q2_model(states_and_actions) - targets.detach()) ** 2)\n",
    "            self.update_model(q1_loss, self.q1_optimizer, self.q1_model, self.q1_target_model)\n",
    "            self.update_model(q2_loss, self.q2_optimizer, self.q2_model, self.q2_target_model)\n",
    "\n",
    "            pred_actions, log_probs = self.predict_actions(states)\n",
    "            states_and_pred_actions = torch.concatenate((states, pred_actions), dim=1)\n",
    "            q1_values = self.q1_model(states_and_pred_actions)\n",
    "            q2_values = self.q2_model(states_and_pred_actions)\n",
    "            min_q_values = torch.min(q1_values, q2_values)\n",
    "            pi_loss = - torch.mean(min_q_values - self.alpha * log_probs)\n",
    "            self.update_model(pi_loss, self.pi_optimizer)\n",
    "            \n",
    "    def update_model(self, loss, optimizer, model=None, target_model=None):\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if model != None and target_model != None:\n",
    "            for param, terget_param in zip(model.parameters(), target_model.parameters()):\n",
    "                new_terget_param = (1 - self.tau) * terget_param + self.tau * param\n",
    "                terget_param.data.copy_(new_terget_param)\n",
    "\n",
    "    def predict_actions(self, states):\n",
    "        means, log_stds = self.pi_model(states).T\n",
    "        means, log_stds = means.unsqueeze(1), log_stds.unsqueeze(1)\n",
    "        dists = Normal(means, torch.exp(log_stds))\n",
    "        actions = dists.rsample()\n",
    "        log_probs = dists.log_prob(actions)\n",
    "\n",
    "        return actions, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, trajectory_len=200, visualize=False, filename='gym_animation.gif'):\n",
    "    trajectory = {'states':[], 'actions': [], 'rewards': [], 'dones': []}\n",
    "    \n",
    "    state = env.reset()\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(trajectory_len):\n",
    "\n",
    "        trajectory['states'].append(state)\n",
    "\n",
    "        action = agent.get_action(state)\n",
    "        trajectory['actions'].append(action)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        trajectory['rewards'].append(reward)\n",
    "        trajectory['dones'].append(done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        if visualize:\n",
    "            frames.append(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "    if visualize:\n",
    "        save_frames_as_gif(frames, filename=filename)\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(env, agent, validation_n, trajectory_len=200):\n",
    "    total_rewards = []\n",
    "    for _ in range(validation_n):\n",
    "        trajectory = get_trajectory(env, agent, trajectory_len)\n",
    "        total_rewards.append(np.sum(trajectory['rewards']))\n",
    "\n",
    "    return np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, episode_n, trajectory_len=200):\n",
    "    total_rewards = []\n",
    "    for episode in range(episode_n):\n",
    "\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        for t in range(trajectory_len):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "            agent.fit(state, action, reward, done, next_state)\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение: (6 мин)\n",
    "\n",
    "Выбранные гиперпараметры:\n",
    "\n",
    "```python\n",
    "episode_n = 200\n",
    "\n",
    "gamma=0.99 \n",
    "alpha=1e-3\n",
    "tau=1e-2\n",
    "batch_size=64\n",
    "pi_lr=1e-3\n",
    "q_lr=1e-3\n",
    "```\n",
    "\n",
    "Архитектуры нейронный сетей:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "self.pi_model = nn.Sequential(nn.Linear(state_dim, 128), nn.ReLU(), \n",
    "                              nn.Linear(128, 128), nn.ReLU(), \n",
    "                              nn.Linear(128, 2 * action_dim), nn.Tanh())\n",
    "\n",
    "self.q1_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(), \n",
    "                              nn.Linear(128, 128), nn.ReLU(), \n",
    "                              nn.Linear(128, 1))\n",
    "\n",
    "self.q2_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(), \n",
    "                              nn.Linear(128, 128), nn.ReLU(), \n",
    "                              nn.Linear(128, 1))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SAC(state_dim, action_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_n = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_history = train(env, agent, episode_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "График обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('SAC, Pendulum-v1')\n",
    "plt.xlabel('Trajectory number')\n",
    "plt.ylabel('Trajectory reward')\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(sac_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Валидация:\n",
    "\n",
    "**SAC, validation_score: -235.55826693948117**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_n = 100\n",
    "\n",
    "validation_score = validation(env, agent, validation_n=validation_n, trajectory_len=200)\n",
    "print(f'SAC, default advantage, validation_score: {validation_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторим еще 2 раза и построим сглаженный график:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = np.load('sac_history1.npy')\n",
    "h2 = np.load('sac_history2.npy')\n",
    "h3 = np.load('sac_history3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plots(h1, h2, h3):\n",
    "\n",
    "    h_smoothed1 = np.zeros_like(h1)\n",
    "    h_smoothed2 = np.zeros_like(h1)\n",
    "    h_smoothed3 = np.zeros_like(h1)\n",
    "\n",
    "    alpha = 1\n",
    "    h_smoothed1[0] = h1[0]\n",
    "    h_smoothed2[0] = h2[0]\n",
    "    h_smoothed3[0] = h3[0]\n",
    "\n",
    "    for i in range(1, h_smoothed1.size):\n",
    "        h_smoothed1[i] = alpha * h1[i] + (1 - alpha) * h_smoothed1[i - 1]\n",
    "        h_smoothed2[i] = alpha * h2[i] + (1 - alpha) * h_smoothed2[i - 1]\n",
    "        h_smoothed3[i] = alpha * h3[i] + (1 - alpha) * h_smoothed3[i - 1]\n",
    "\n",
    "    h_smoothed1 = h_smoothed1.reshape(-1, 1)\n",
    "    h_smoothed2 = h_smoothed2.reshape(-1, 1)\n",
    "    h_smoothed3 = h_smoothed3.reshape(-1, 1)\n",
    "\n",
    "    h_smoothed = np.concatenate((h_smoothed1, h_smoothed2, h_smoothed3), axis=1)\n",
    "    h_smoothed_mean = np.mean(h_smoothed, axis=1)\n",
    "    stds = np.std(h_smoothed, axis=1)\n",
    "\n",
    "    plt.plot(np.arange(h_smoothed_mean.size), h_smoothed_mean)\n",
    "    plt.fill_between(np.arange(h_smoothed_mean.size), h_smoothed_mean - stds, h_smoothed_mean + stds, alpha=0.5)\n",
    "\n",
    "    plt.title('SAC, Pendulum-v1')\n",
    "    plt.xlabel('Trajectory number')\n",
    "    plt.ylabel('Trajectory reward')\n",
    "    plt.legend()\n",
    "\n",
    "    return h_smoothed_mean, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_history_smoothed, sac_std_smoothed = get_plots(h1, h2, h3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полученные результаты:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже понятно, что CEM использует сильно больше траекторий. Изобразим кривые обучения DQN, PPO и SAC на одном графике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results():\n",
    "\n",
    "    plt.plot(np.arange(dqn_history_smoothed.size), dqn_history_smoothed, label='DQN')\n",
    "    plt.fill_between(np.arange(dqn_history_smoothed.size),\n",
    "                     dqn_history_smoothed - dqn_std_smoothed, \n",
    "                     dqn_history_smoothed + dqn_std_smoothed, alpha=0.4)\n",
    "\n",
    "    plt.plot(np.arange(ppo_history_smoothed.size), ppo_history_smoothed, label='PPO')\n",
    "    plt.fill_between(np.arange(ppo_history_smoothed.size),\n",
    "                     ppo_history_smoothed - ppo_std_smoothed, \n",
    "                     ppo_history_smoothed + ppo_std_smoothed, alpha=0.4)\n",
    "\n",
    "    plt.plot(np.arange(sac_history_smoothed.size), sac_history_smoothed, label='SAC')\n",
    "    plt.fill_between(np.arange(sac_history_smoothed.size),\n",
    "                     sac_history_smoothed - sac_std_smoothed, \n",
    "                     sac_history_smoothed + sac_std_smoothed, alpha=0.4)\n",
    "\n",
    "\n",
    "    plt.title('Algorithm comparison, Pendulum-v1')\n",
    "    plt.xlabel('Trajectory number')\n",
    "    plt.ylabel('Trajectory reward')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Вывод:**\n",
    "\n",
    "Наилучшим алгоритмом по результату на валидации оказался стандартный DQN, использующий 2 действия: -2 и 2 из отрезка [-2, 2]. Результат на валидации алгоритма DQN оказался -150.\n",
    "\n",
    "Алгоритмы PPO и SAC показывают значение на валидации не больше -200 независимо от количества эпизодов обучения. Возможно для улучшения этого результата следует аналогично сделать пространство действий дискретным, оставив 2 значения (-2 и 2). В свою очередь моя реализация CEM использует эту идею дискретизации действий, однако обучается хуже и получает значение на валидации не выше -400.\n",
    "\n",
    "По времени обучения лидируют алгоритмы DQN и PPO, обучаясь по 5 минут. Однако SAC не сильно отстает и обучается за 6-7 минут. Обучение CEM для получения указанного выше результата занимает 30 минут.\n",
    "\n",
    "Алгоритму SAC требуется наименьшее количество сгенерированных траекторий для успешного обучения - всего 200 штук. DQN и PPO используют по 500 траекторий. CEM потребовал 12500 траекторий, однако это связано со спецификой алгоритма. CEM является эволюционным алгоритмом и не задействует всего того теоретического аппарата $ Q $-функций и Policy Gradient теорем, которые задействуют другие алгоритмы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
